args2: backbone=, config_file=configs/trainers/CoOp/rn101.yaml, dataset_config_file=configs/datasets/caltech101.yaml, eval_only=False, head=, load_epoch=None, model_dir=, no_train=False,  opts: ['TRAINER.COOP.N_CTX', '16', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '1'], output_dir=output/Caltech/1, resume=, root=/home/brandnerkasper/Uni/MP/MP_CustomCoOp/data, seed=1, source_domains=None, target_domains=None, trainer=CoOp, transforms=None
Setting fixed seed: 1
***************
** Arguments **
***************
config_file: configs/trainers/CoOp/rn101.yaml
csc: False
ctp: end
dataset_config_file: configs/datasets/caltech101.yaml
n_ctx: 16
opts: ['TRAINER.COOP.N_CTX', '16', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '1']
output_dir: output/Caltech/1
root: /home/brandnerkasper/Uni/MP/MP_CustomCoOp/data
seed: 1
shots: 1
trainer: CoOp
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 4
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: Caltech101
  NUM_LABELED: -1
  NUM_SHOTS: 1
  ROOT: /home/brandnerkasper/Uni/MP/MP_CustomCoOp/data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN101
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 200
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/Caltech/1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: 
    N_CTX: 16
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: 
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: CoOp
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 2.0.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 22.04.3 LTS (x86_64)
GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.35

Python version: 3.10.12 (main, Jul  5 2023, 18:54:27) [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-6.2.0-32-generic-x86_64-with-glibc2.35
Is CUDA available: True
CUDA runtime version: 11.5.119
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA GeForce GTX 970
Nvidia driver version: 525.125.06
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Architecture:                       x86_64
CPU op-mode(s):                     32-bit, 64-bit
Address sizes:                      39 bits physical, 48 bits virtual
Byte Order:                         Little Endian
CPU(s):                             4
On-line CPU(s) list:                0-3
Vendor ID:                          GenuineIntel
Model name:                         Intel(R) Xeon(R) CPU E3-1225 v3 @ 3.20GHz
CPU family:                         6
Model:                              60
Thread(s) per core:                 1
Core(s) per socket:                 4
Socket(s):                          1
Stepping:                           3
CPU max MHz:                        3600,0000
CPU min MHz:                        800,0000
BogoMIPS:                           6398.04
Flags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm cpuid_fault invpcid_single pti ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid xsaveopt dtherm ida arat pln pts md_clear flush_l1d
Virtualization:                     VT-x
L1d cache:                          128 KiB (4 instances)
L1i cache:                          128 KiB (4 instances)
L2 cache:                           1 MiB (4 instances)
L3 cache:                           8 MiB (1 instance)
NUMA node(s):                       1
NUMA node0 CPU(s):                  0-3
Vulnerability Gather data sampling: Not affected
Vulnerability Itlb multihit:        KVM: Mitigation: VMX disabled
Vulnerability L1tf:                 Mitigation; PTE Inversion; VMX conditional cache flushes, SMT disabled
Vulnerability Mds:                  Mitigation; Clear CPU buffers; SMT disabled
Vulnerability Meltdown:             Mitigation; PTI
Vulnerability Mmio stale data:      Unknown: No mitigations
Vulnerability Retbleed:             Not affected
Vulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl
Vulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization
Vulnerability Spectre v2:           Mitigation; Retpolines, IBPB conditional, IBRS_FW, STIBP disabled, RSB filling, PBRSB-eIBRS Not affected
Vulnerability Srbds:                Mitigation; Microcode
Vulnerability Tsx async abort:      Not affected

Versions of relevant libraries:
[pip3] numpy==1.25.2
[pip3] open-clip-torch==2.20.0
[pip3] torch==2.0.1
[pip3] torchaudio==2.0.2
[pip3] torchvision==0.15.2
[conda] blas                      1.0                         mkl  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] mkl                       2023.1.0         h213fc3f_46343  
[conda] mkl-service               2.4.0           py310h5eee18b_1  
[conda] mkl_fft                   1.3.6           py310h1128e8f_1  
[conda] mkl_random                1.2.2           py310h1128e8f_1  
[conda] numpy                     1.25.2          py310h5f9d8c6_0  
[conda] numpy-base                1.25.2          py310hb5e798b_0  
[conda] open-clip-torch           2.20.0                   pypi_0    pypi
[conda] pytorch                   2.0.1           py3.10_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchaudio                2.0.2               py310_cu117    pytorch
[conda] torchtriton               2.0.0                     py310    pytorch
[conda] torchvision               0.15.2              py310_cu117    pytorch
        Pillow (9.4.0)

Loading trainer: CoOp
Loading dataset: Caltech101
Reading split from /home/brandnerkasper/Uni/MP/MP_CustomCoOp/data/caltech-101/split_zhou_Caltech101.json
Creating a 1-shot dataset
Creating a 1-shot dataset
Saving preprocessed few-shot data to /home/brandnerkasper/Uni/MP/MP_CustomCoOp/data/caltech-101/split_fewshot/shot_1-seed_1.pkl
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ----------
Dataset    Caltech101
# classes  100
# train_x  100
# val      100
# test     2,465
---------  ----------
Loading CLIP (backbone: RN101)
CLIP(
  (visual): ModifiedResNet(
    (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (act1): ReLU(inplace=True)
    (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (act2): ReLU(inplace=True)
    (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (act3): ReLU(inplace=True)
    (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (layer1): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
        (downsample): Sequential(
          (-1): AvgPool2d(kernel_size=1, stride=1, padding=0)
          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
      )
    )
    (layer2): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
        (downsample): Sequential(
          (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
      )
    )
    (layer3): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
        (downsample): Sequential(
          (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
      )
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
      )
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
      )
      (6): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
      )
      (7): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
      )
      (8): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
      )
      (9): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
      )
      (10): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
      )
      (11): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
      )
      (12): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
      )
      (13): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
      )
      (14): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
      )
      (15): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
      )
      (16): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
      )
      (17): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
      )
      (18): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
      )
      (19): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
      )
      (20): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
      )
      (21): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
      )
      (22): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
      )
    )
    (layer4): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
        (downsample): Sequential(
          (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
      )
    )
    (attnpool): AttentionPool2d(
      (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
      (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
      (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
      (c_proj): Linear(in_features=2048, out_features=512, bias=True)
    )
  )
  (transformer): Transformer(
    (resblocks): ModuleList(
      (0-11): 12 x ResidualAttentionBlock(
        (ln_1): LayerNormFp32((512,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNormFp32((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ls_2): Identity()
      )
    )
  )
  (token_embedding): Embedding(49408, 512)
  (ln_final): LayerNormFp32((512,), eps=1e-05, elementwise_affine=True)
)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
Turning off gradients in both the image and the text encoder
Loading evaluator: Classification
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/Caltech/1/tensorboard)
epoch [1/200] batch [1/3] time 5.472 (5.472) data 0.297 (0.297) loss 4.5703 (4.5703) acc 3.1250 (3.1250) lr 1.0000e-05 eta 0:54:37
epoch [1/200] batch [2/3] time 2.065 (3.768) data 0.000 (0.148) loss 4.6094 (4.5898) acc 3.1250 (3.1250) lr 1.0000e-05 eta 0:37:33
epoch [1/200] batch [3/3] time 2.067 (3.201) data 0.000 (0.099) loss 4.5117 (4.5638) acc 3.1250 (3.1250) lr 2.0000e-03 eta 0:31:51
epoch [2/200] batch [1/3] time 2.300 (2.300) data 0.230 (0.230) loss 4.6172 (4.6172) acc 0.0000 (0.0000) lr 2.0000e-03 eta 0:22:50
epoch [2/200] batch [2/3] time 2.068 (2.184) data 0.000 (0.115) loss 4.5469 (4.5820) acc 3.1250 (1.5625) lr 2.0000e-03 eta 0:21:39
epoch [2/200] batch [3/3] time 2.074 (2.147) data 0.000 (0.077) loss 4.5078 (4.5573) acc 6.2500 (3.1250) lr 1.9999e-03 eta 0:21:15
epoch [3/200] batch [1/3] time 2.288 (2.288) data 0.219 (0.219) loss 4.5547 (4.5547) acc 0.0000 (0.0000) lr 1.9999e-03 eta 0:22:36
epoch [3/200] batch [2/3] time 2.075 (2.182) data 0.000 (0.110) loss 4.4922 (4.5234) acc 3.1250 (1.5625) lr 1.9999e-03 eta 0:21:31
epoch [3/200] batch [3/3] time 2.078 (2.147) data 0.000 (0.073) loss 4.5312 (4.5260) acc 3.1250 (2.0833) lr 1.9995e-03 eta 0:21:08
epoch [4/200] batch [1/3] time 2.303 (2.303) data 0.224 (0.224) loss 4.5547 (4.5547) acc 3.1250 (3.1250) lr 1.9995e-03 eta 0:22:38
epoch [4/200] batch [2/3] time 2.080 (2.191) data 0.000 (0.112) loss 4.5156 (4.5352) acc 9.3750 (6.2500) lr 1.9995e-03 eta 0:21:30
epoch [4/200] batch [3/3] time 2.077 (2.153) data 0.000 (0.075) loss 4.3164 (4.4622) acc 15.6250 (9.3750) lr 1.9989e-03 eta 0:21:06
epoch [5/200] batch [1/3] time 2.324 (2.324) data 0.237 (0.237) loss 4.4258 (4.4258) acc 3.1250 (3.1250) lr 1.9989e-03 eta 0:22:43
epoch [5/200] batch [2/3] time 2.080 (2.202) data 0.000 (0.119) loss 4.3242 (4.3750) acc 9.3750 (6.2500) lr 1.9989e-03 eta 0:21:30
epoch [5/200] batch [3/3] time 2.086 (2.163) data 0.000 (0.079) loss 4.4531 (4.4010) acc 3.1250 (5.2083) lr 1.9980e-03 eta 0:21:05
epoch [6/200] batch [1/3] time 2.300 (2.300) data 0.218 (0.218) loss 4.2617 (4.2617) acc 15.6250 (15.6250) lr 1.9980e-03 eta 0:22:23
epoch [6/200] batch [2/3] time 2.085 (2.192) data 0.000 (0.109) loss 4.2461 (4.2539) acc 3.1250 (9.3750) lr 1.9980e-03 eta 0:21:18
epoch [6/200] batch [3/3] time 2.080 (2.155) data 0.000 (0.073) loss 4.1836 (4.2305) acc 9.3750 (9.3750) lr 1.9969e-03 eta 0:20:54
epoch [7/200] batch [1/3] time 2.310 (2.310) data 0.234 (0.234) loss 4.1797 (4.1797) acc 9.3750 (9.3750) lr 1.9969e-03 eta 0:22:22
epoch [7/200] batch [2/3] time 2.080 (2.195) data 0.000 (0.117) loss 3.8848 (4.0322) acc 21.8750 (15.6250) lr 1.9969e-03 eta 0:21:13
epoch [7/200] batch [3/3] time 2.084 (2.158) data 0.000 (0.078) loss 4.1289 (4.0645) acc 3.1250 (11.4583) lr 1.9956e-03 eta 0:20:49
epoch [8/200] batch [1/3] time 2.319 (2.319) data 0.236 (0.236) loss 3.8770 (3.8770) acc 25.0000 (25.0000) lr 1.9956e-03 eta 0:22:20
epoch [8/200] batch [2/3] time 2.073 (2.196) data 0.000 (0.118) loss 3.3809 (3.6289) acc 25.0000 (25.0000) lr 1.9956e-03 eta 0:21:07
epoch [8/200] batch [3/3] time 2.074 (2.156) data 0.000 (0.079) loss 3.7988 (3.6855) acc 9.3750 (19.7917) lr 1.9940e-03 eta 0:20:41
epoch [9/200] batch [1/3] time 2.323 (2.323) data 0.237 (0.237) loss 2.9062 (2.9062) acc 50.0000 (50.0000) lr 1.9940e-03 eta 0:22:15
epoch [9/200] batch [2/3] time 2.071 (2.197) data 0.000 (0.119) loss 3.1680 (3.0371) acc 28.1250 (39.0625) lr 1.9940e-03 eta 0:21:01
epoch [9/200] batch [3/3] time 2.086 (2.160) data 0.000 (0.079) loss 3.5098 (3.1947) acc 15.6250 (31.2500) lr 1.9921e-03 eta 0:20:37
epoch [10/200] batch [1/3] time 2.316 (2.316) data 0.232 (0.232) loss 2.9219 (2.9219) acc 34.3750 (34.3750) lr 1.9921e-03 eta 0:22:04
epoch [10/200] batch [2/3] time 2.077 (2.196) data 0.000 (0.116) loss 2.5488 (2.7354) acc 40.6250 (37.5000) lr 1.9921e-03 eta 0:20:54
epoch [10/200] batch [3/3] time 2.080 (2.157) data 0.000 (0.078) loss 2.6797 (2.7168) acc 25.0000 (33.3333) lr 1.9900e-03 eta 0:20:29
epoch [11/200] batch [1/3] time 2.315 (2.315) data 0.223 (0.223) loss 2.5566 (2.5566) acc 40.6250 (40.6250) lr 1.9900e-03 eta 0:21:56
epoch [11/200] batch [2/3] time 2.075 (2.195) data 0.000 (0.112) loss 2.2305 (2.3936) acc 43.7500 (42.1875) lr 1.9900e-03 eta 0:20:46
epoch [11/200] batch [3/3] time 2.083 (2.158) data 0.000 (0.075) loss 2.6934 (2.4935) acc 37.5000 (40.6250) lr 1.9877e-03 eta 0:20:23
epoch [12/200] batch [1/3] time 2.334 (2.334) data 0.230 (0.230) loss 1.9756 (1.9756) acc 50.0000 (50.0000) lr 1.9877e-03 eta 0:22:01
epoch [12/200] batch [2/3] time 2.107 (2.220) data 0.000 (0.115) loss 2.8184 (2.3970) acc 37.5000 (43.7500) lr 1.9877e-03 eta 0:20:54
epoch [12/200] batch [3/3] time 2.078 (2.173) data 0.000 (0.077) loss 1.8828 (2.2256) acc 59.3750 (48.9583) lr 1.9851e-03 eta 0:20:25
epoch [13/200] batch [1/3] time 2.304 (2.304) data 0.220 (0.220) loss 2.5664 (2.5664) acc 37.5000 (37.5000) lr 1.9851e-03 eta 0:21:37
epoch [13/200] batch [2/3] time 2.077 (2.191) data 0.000 (0.110) loss 2.2207 (2.3936) acc 46.8750 (42.1875) lr 1.9851e-03 eta 0:20:31
epoch [13/200] batch [3/3] time 2.080 (2.154) data 0.000 (0.073) loss 2.1426 (2.3099) acc 53.1250 (45.8333) lr 1.9823e-03 eta 0:20:08
epoch [14/200] batch [1/3] time 2.313 (2.313) data 0.230 (0.230) loss 1.5410 (1.5410) acc 50.0000 (50.0000) lr 1.9823e-03 eta 0:21:35
epoch [14/200] batch [2/3] time 2.096 (2.205) data 0.000 (0.115) loss 1.8828 (1.7119) acc 65.6250 (57.8125) lr 1.9823e-03 eta 0:20:32
epoch [14/200] batch [3/3] time 2.081 (2.164) data 0.000 (0.077) loss 1.6602 (1.6947) acc 53.1250 (56.2500) lr 1.9792e-03 eta 0:20:07
epoch [15/200] batch [1/3] time 2.326 (2.326) data 0.235 (0.235) loss 1.8691 (1.8691) acc 65.6250 (65.6250) lr 1.9792e-03 eta 0:21:35
epoch [15/200] batch [2/3] time 2.088 (2.207) data 0.000 (0.118) loss 1.7578 (1.8135) acc 53.1250 (59.3750) lr 1.9792e-03 eta 0:20:27
epoch [15/200] batch [3/3] time 2.092 (2.169) data 0.000 (0.078) loss 1.3027 (1.6432) acc 71.8750 (63.5417) lr 1.9759e-03 eta 0:20:03
epoch [16/200] batch [1/3] time 2.320 (2.320) data 0.231 (0.231) loss 1.5312 (1.5312) acc 56.2500 (56.2500) lr 1.9759e-03 eta 0:21:25
epoch [16/200] batch [2/3] time 2.075 (2.198) data 0.000 (0.115) loss 1.4072 (1.4692) acc 65.6250 (60.9375) lr 1.9759e-03 eta 0:20:15
epoch [16/200] batch [3/3] time 2.081 (2.159) data 0.000 (0.077) loss 1.5322 (1.4902) acc 62.5000 (61.4583) lr 1.9724e-03 eta 0:19:51
epoch [17/200] batch [1/3] time 2.315 (2.315) data 0.224 (0.224) loss 1.6943 (1.6943) acc 59.3750 (59.3750) lr 1.9724e-03 eta 0:21:15
epoch [17/200] batch [2/3] time 2.086 (2.200) data 0.000 (0.112) loss 1.0488 (1.3716) acc 75.0000 (67.1875) lr 1.9724e-03 eta 0:20:10
epoch [17/200] batch [3/3] time 2.081 (2.161) data 0.000 (0.075) loss 1.4141 (1.3857) acc 65.6250 (66.6667) lr 1.9686e-03 eta 0:19:46
epoch [18/200] batch [1/3] time 2.333 (2.333) data 0.246 (0.246) loss 1.2441 (1.2441) acc 71.8750 (71.8750) lr 1.9686e-03 eta 0:21:18
epoch [18/200] batch [2/3] time 2.091 (2.212) data 0.000 (0.123) loss 1.2900 (1.2671) acc 65.6250 (68.7500) lr 1.9686e-03 eta 0:20:09
epoch [18/200] batch [3/3] time 2.078 (2.167) data 0.000 (0.082) loss 1.1045 (1.2129) acc 75.0000 (70.8333) lr 1.9646e-03 eta 0:19:43
epoch [19/200] batch [1/3] time 2.331 (2.331) data 0.237 (0.237) loss 1.1455 (1.1455) acc 65.6250 (65.6250) lr 1.9646e-03 eta 0:21:10
epoch [19/200] batch [2/3] time 2.081 (2.206) data 0.000 (0.119) loss 0.9424 (1.0439) acc 68.7500 (67.1875) lr 1.9646e-03 eta 0:19:59
epoch [19/200] batch [3/3] time 2.094 (2.168) data 0.000 (0.079) loss 1.4980 (1.1953) acc 59.3750 (64.5833) lr 1.9603e-03 eta 0:19:37
epoch [20/200] batch [1/3] time 2.315 (2.315) data 0.221 (0.221) loss 1.3506 (1.3506) acc 62.5000 (62.5000) lr 1.9603e-03 eta 0:20:54
epoch [20/200] batch [2/3] time 2.100 (2.207) data 0.000 (0.110) loss 1.0977 (1.2241) acc 68.7500 (65.6250) lr 1.9603e-03 eta 0:19:54
epoch [20/200] batch [3/3] time 2.096 (2.170) data 0.000 (0.074) loss 1.3555 (1.2679) acc 75.0000 (68.7500) lr 1.9558e-03 eta 0:19:31
epoch [21/200] batch [1/3] time 2.320 (2.320) data 0.231 (0.231) loss 0.8179 (0.8179) acc 81.2500 (81.2500) lr 1.9558e-03 eta 0:20:50
epoch [21/200] batch [2/3] time 2.099 (2.209) data 0.000 (0.115) loss 1.2803 (1.0491) acc 68.7500 (75.0000) lr 1.9558e-03 eta 0:19:48
epoch [21/200] batch [3/3] time 2.097 (2.172) data 0.000 (0.077) loss 0.9526 (1.0169) acc 75.0000 (75.0000) lr 1.9511e-03 eta 0:19:26
epoch [22/200] batch [1/3] time 2.327 (2.327) data 0.231 (0.231) loss 0.9600 (0.9600) acc 78.1250 (78.1250) lr 1.9511e-03 eta 0:20:47
epoch [22/200] batch [2/3] time 2.104 (2.215) data 0.000 (0.115) loss 1.1455 (1.0527) acc 62.5000 (70.3125) lr 1.9511e-03 eta 0:19:45
epoch [22/200] batch [3/3] time 2.092 (2.174) data 0.000 (0.077) loss 1.2227 (1.1094) acc 71.8750 (70.8333) lr 1.9461e-03 eta 0:19:21
epoch [23/200] batch [1/3] time 2.329 (2.329) data 0.239 (0.239) loss 0.8149 (0.8149) acc 87.5000 (87.5000) lr 1.9461e-03 eta 0:20:41
epoch [23/200] batch [2/3] time 2.088 (2.209) data 0.000 (0.119) loss 1.1123 (0.9636) acc 71.8750 (79.6875) lr 1.9461e-03 eta 0:19:34
epoch [23/200] batch [3/3] time 2.104 (2.174) data 0.000 (0.080) loss 0.8779 (0.9351) acc 90.6250 (83.3333) lr 1.9409e-03 eta 0:19:14
epoch [24/200] batch [1/3] time 2.335 (2.335) data 0.240 (0.240) loss 0.9053 (0.9053) acc 78.1250 (78.1250) lr 1.9409e-03 eta 0:20:37
epoch [24/200] batch [2/3] time 2.082 (2.209) data 0.000 (0.120) loss 0.8438 (0.8745) acc 87.5000 (82.8125) lr 1.9409e-03 eta 0:19:28
epoch [24/200] batch [3/3] time 2.082 (2.166) data 0.000 (0.080) loss 0.8804 (0.8765) acc 90.6250 (85.4167) lr 1.9354e-03 eta 0:19:03
epoch [25/200] batch [1/3] time 2.319 (2.319) data 0.220 (0.220) loss 1.1826 (1.1826) acc 75.0000 (75.0000) lr 1.9354e-03 eta 0:20:22
epoch [25/200] batch [2/3] time 2.095 (2.207) data 0.000 (0.110) loss 0.9282 (1.0554) acc 78.1250 (76.5625) lr 1.9354e-03 eta 0:19:20
epoch [25/200] batch [3/3] time 2.097 (2.170) data 0.000 (0.073) loss 1.0381 (1.0496) acc 75.0000 (76.0417) lr 1.9298e-03 eta 0:18:59
epoch [26/200] batch [1/3] time 2.323 (2.323) data 0.229 (0.229) loss 0.5952 (0.5952) acc 81.2500 (81.2500) lr 1.9298e-03 eta 0:20:17
epoch [26/200] batch [2/3] time 2.094 (2.208) data 0.000 (0.115) loss 1.0771 (0.8362) acc 68.7500 (75.0000) lr 1.9298e-03 eta 0:19:14
epoch [26/200] batch [3/3] time 2.100 (2.172) data 0.000 (0.077) loss 0.8115 (0.8280) acc 84.3750 (78.1250) lr 1.9239e-03 eta 0:18:53
epoch [27/200] batch [1/3] time 2.322 (2.322) data 0.222 (0.222) loss 0.9434 (0.9434) acc 75.0000 (75.0000) lr 1.9239e-03 eta 0:20:09
epoch [27/200] batch [2/3] time 2.096 (2.209) data 0.000 (0.111) loss 0.8721 (0.9077) acc 78.1250 (76.5625) lr 1.9239e-03 eta 0:19:08
epoch [27/200] batch [3/3] time 2.093 (2.170) data 0.000 (0.074) loss 0.8770 (0.8975) acc 78.1250 (77.0833) lr 1.9178e-03 eta 0:18:46
epoch [28/200] batch [1/3] time 2.319 (2.319) data 0.223 (0.223) loss 1.3047 (1.3047) acc 75.0000 (75.0000) lr 1.9178e-03 eta 0:20:01
epoch [28/200] batch [2/3] time 2.103 (2.211) data 0.000 (0.112) loss 1.0977 (1.2012) acc 78.1250 (76.5625) lr 1.9178e-03 eta 0:19:03
epoch [28/200] batch [3/3] time 2.087 (2.170) data 0.000 (0.074) loss 0.4771 (0.9598) acc 87.5000 (80.2083) lr 1.9114e-03 eta 0:18:39
epoch [29/200] batch [1/3] time 2.329 (2.329) data 0.239 (0.239) loss 0.6992 (0.6992) acc 78.1250 (78.1250) lr 1.9114e-03 eta 0:19:59
epoch [29/200] batch [2/3] time 2.096 (2.212) data 0.000 (0.119) loss 0.8867 (0.7930) acc 81.2500 (79.6875) lr 1.9114e-03 eta 0:18:57
epoch [29/200] batch [3/3] time 2.098 (2.174) data 0.000 (0.080) loss 0.4717 (0.6859) acc 96.8750 (85.4167) lr 1.9048e-03 eta 0:18:35
epoch [30/200] batch [1/3] time 2.330 (2.330) data 0.238 (0.238) loss 0.6646 (0.6646) acc 84.3750 (84.3750) lr 1.9048e-03 eta 0:19:52
epoch [30/200] batch [2/3] time 2.098 (2.214) data 0.000 (0.119) loss 0.6201 (0.6423) acc 87.5000 (85.9375) lr 1.9048e-03 eta 0:18:51
epoch [30/200] batch [3/3] time 2.094 (2.174) data 0.000 (0.079) loss 1.2832 (0.8560) acc 81.2500 (84.3750) lr 1.8980e-03 eta 0:18:28
epoch [31/200] batch [1/3] time 2.317 (2.317) data 0.222 (0.222) loss 0.9351 (0.9351) acc 81.2500 (81.2500) lr 1.8980e-03 eta 0:19:39
epoch [31/200] batch [2/3] time 2.096 (2.207) data 0.000 (0.111) loss 0.8418 (0.8884) acc 81.2500 (81.2500) lr 1.8980e-03 eta 0:18:41
epoch [31/200] batch [3/3] time 2.104 (2.172) data 0.000 (0.074) loss 1.1797 (0.9855) acc 68.7500 (77.0833) lr 1.8910e-03 eta 0:18:21
epoch [32/200] batch [1/3] time 2.315 (2.315) data 0.223 (0.223) loss 0.5928 (0.5928) acc 87.5000 (87.5000) lr 1.8910e-03 eta 0:19:31
epoch [32/200] batch [2/3] time 2.104 (2.210) data 0.000 (0.111) loss 1.0752 (0.8340) acc 81.2500 (84.3750) lr 1.8910e-03 eta 0:18:35
epoch [32/200] batch [3/3] time 2.098 (2.173) data 0.000 (0.074) loss 0.4622 (0.7100) acc 84.3750 (84.3750) lr 1.8838e-03 eta 0:18:14
epoch [33/200] batch [1/3] time 2.317 (2.317) data 0.222 (0.222) loss 0.7368 (0.7368) acc 78.1250 (78.1250) lr 1.8838e-03 eta 0:19:25
epoch [33/200] batch [2/3] time 2.087 (2.202) data 0.000 (0.111) loss 0.7524 (0.7446) acc 84.3750 (81.2500) lr 1.8838e-03 eta 0:18:25
epoch [33/200] batch [3/3] time 2.089 (2.165) data 0.000 (0.074) loss 0.7402 (0.7432) acc 81.2500 (81.2500) lr 1.8763e-03 eta 0:18:04
epoch [34/200] batch [1/3] time 2.322 (2.322) data 0.232 (0.232) loss 0.4189 (0.4189) acc 90.6250 (90.6250) lr 1.8763e-03 eta 0:19:21
epoch [34/200] batch [2/3] time 2.089 (2.206) data 0.000 (0.116) loss 0.4949 (0.4569) acc 93.7500 (92.1875) lr 1.8763e-03 eta 0:18:20
epoch [34/200] batch [3/3] time 2.097 (2.170) data 0.000 (0.078) loss 0.9199 (0.6112) acc 81.2500 (88.5417) lr 1.8686e-03 eta 0:18:00
epoch [35/200] batch [1/3] time 2.328 (2.328) data 0.233 (0.233) loss 0.4932 (0.4932) acc 90.6250 (90.6250) lr 1.8686e-03 eta 0:19:17
epoch [35/200] batch [2/3] time 2.100 (2.214) data 0.000 (0.116) loss 0.5557 (0.5244) acc 87.5000 (89.0625) lr 1.8686e-03 eta 0:18:18
epoch [35/200] batch [3/3] time 2.104 (2.177) data 0.000 (0.078) loss 0.9692 (0.6727) acc 75.0000 (84.3750) lr 1.8607e-03 eta 0:17:57
epoch [36/200] batch [1/3] time 2.316 (2.316) data 0.223 (0.223) loss 0.4807 (0.4807) acc 90.6250 (90.6250) lr 1.8607e-03 eta 0:19:03
epoch [36/200] batch [2/3] time 2.097 (2.206) data 0.000 (0.112) loss 0.9834 (0.7321) acc 78.1250 (84.3750) lr 1.8607e-03 eta 0:18:07
epoch [36/200] batch [3/3] time 2.093 (2.168) data 0.000 (0.074) loss 0.6792 (0.7144) acc 84.3750 (84.3750) lr 1.8526e-03 eta 0:17:46
epoch [37/200] batch [1/3] time 2.343 (2.343) data 0.229 (0.229) loss 0.5859 (0.5859) acc 87.5000 (87.5000) lr 1.8526e-03 eta 0:19:10
epoch [37/200] batch [2/3] time 2.108 (2.225) data 0.000 (0.115) loss 0.6875 (0.6367) acc 87.5000 (87.5000) lr 1.8526e-03 eta 0:18:10
epoch [37/200] batch [3/3] time 2.103 (2.184) data 0.000 (0.076) loss 0.6113 (0.6283) acc 84.3750 (86.4583) lr 1.8443e-03 eta 0:17:48
epoch [38/200] batch [1/3] time 2.332 (2.332) data 0.240 (0.240) loss 0.4695 (0.4695) acc 96.8750 (96.8750) lr 1.8443e-03 eta 0:18:58
epoch [38/200] batch [2/3] time 2.099 (2.215) data 0.000 (0.120) loss 1.0986 (0.7841) acc 75.0000 (85.9375) lr 1.8443e-03 eta 0:17:58
epoch [38/200] batch [3/3] time 2.101 (2.177) data 0.000 (0.080) loss 0.8770 (0.8150) acc 84.3750 (85.4167) lr 1.8358e-03 eta 0:17:38
epoch [39/200] batch [1/3] time 2.318 (2.318) data 0.223 (0.223) loss 0.8076 (0.8076) acc 81.2500 (81.2500) lr 1.8358e-03 eta 0:18:44
epoch [39/200] batch [2/3] time 2.093 (2.205) data 0.000 (0.112) loss 0.8149 (0.8113) acc 87.5000 (84.3750) lr 1.8358e-03 eta 0:17:47
epoch [39/200] batch [3/3] time 2.112 (2.174) data 0.000 (0.074) loss 1.0195 (0.8807) acc 78.1250 (82.2917) lr 1.8271e-03 eta 0:17:30
epoch [40/200] batch [1/3] time 2.316 (2.316) data 0.217 (0.217) loss 0.6841 (0.6841) acc 78.1250 (78.1250) lr 1.8271e-03 eta 0:18:36
epoch [40/200] batch [2/3] time 2.105 (2.211) data 0.000 (0.109) loss 0.4216 (0.5529) acc 90.6250 (84.3750) lr 1.8271e-03 eta 0:17:43
epoch [40/200] batch [3/3] time 2.102 (2.174) data 0.000 (0.073) loss 0.9829 (0.6962) acc 81.2500 (83.3333) lr 1.8181e-03 eta 0:17:23
epoch [41/200] batch [1/3] time 2.308 (2.308) data 0.222 (0.222) loss 0.3984 (0.3984) acc 93.7500 (93.7500) lr 1.8181e-03 eta 0:18:25
epoch [41/200] batch [2/3] time 2.101 (2.205) data 0.000 (0.111) loss 0.6753 (0.5369) acc 81.2500 (87.5000) lr 1.8181e-03 eta 0:17:33
epoch [41/200] batch [3/3] time 2.108 (2.172) data 0.000 (0.074) loss 0.6714 (0.5817) acc 87.5000 (87.5000) lr 1.8090e-03 eta 0:17:16
epoch [42/200] batch [1/3] time 2.336 (2.336) data 0.232 (0.232) loss 0.9194 (0.9194) acc 71.8750 (71.8750) lr 1.8090e-03 eta 0:18:32
epoch [42/200] batch [2/3] time 2.102 (2.219) data 0.000 (0.116) loss 1.1787 (1.0491) acc 65.6250 (68.7500) lr 1.8090e-03 eta 0:17:34
epoch [42/200] batch [3/3] time 2.098 (2.179) data 0.000 (0.077) loss 0.3313 (0.8098) acc 93.7500 (77.0833) lr 1.7997e-03 eta 0:17:12
epoch [43/200] batch [1/3] time 2.334 (2.334) data 0.237 (0.237) loss 0.4509 (0.4509) acc 87.5000 (87.5000) lr 1.7997e-03 eta 0:18:24
epoch [43/200] batch [2/3] time 2.100 (2.217) data 0.000 (0.119) loss 0.4604 (0.4557) acc 90.6250 (89.0625) lr 1.7997e-03 eta 0:17:26
epoch [43/200] batch [3/3] time 2.105 (2.180) data 0.000 (0.079) loss 1.2002 (0.7039) acc 75.0000 (84.3750) lr 1.7902e-03 eta 0:17:06
epoch [44/200] batch [1/3] time 2.330 (2.330) data 0.224 (0.224) loss 0.7949 (0.7949) acc 84.3750 (84.3750) lr 1.7902e-03 eta 0:18:15
epoch [44/200] batch [2/3] time 2.111 (2.221) data 0.000 (0.112) loss 0.7021 (0.7485) acc 84.3750 (84.3750) lr 1.7902e-03 eta 0:17:21
epoch [44/200] batch [3/3] time 2.095 (2.179) data 0.000 (0.075) loss 0.5923 (0.6965) acc 84.3750 (84.3750) lr 1.7804e-03 eta 0:16:59
epoch [45/200] batch [1/3] time 2.327 (2.327) data 0.230 (0.230) loss 0.4172 (0.4172) acc 90.6250 (90.6250) lr 1.7804e-03 eta 0:18:06
epoch [45/200] batch [2/3] time 2.104 (2.215) data 0.000 (0.115) loss 0.5303 (0.4738) acc 84.3750 (87.5000) lr 1.7804e-03 eta 0:17:12
epoch [45/200] batch [3/3] time 2.093 (2.175) data 0.000 (0.077) loss 0.5195 (0.4890) acc 84.3750 (86.4583) lr 1.7705e-03 eta 0:16:51
epoch [46/200] batch [1/3] time 2.328 (2.328) data 0.238 (0.238) loss 0.3440 (0.3440) acc 90.6250 (90.6250) lr 1.7705e-03 eta 0:18:00
epoch [46/200] batch [2/3] time 2.104 (2.216) data 0.000 (0.119) loss 0.6333 (0.4886) acc 87.5000 (89.0625) lr 1.7705e-03 eta 0:17:06
epoch [46/200] batch [3/3] time 2.111 (2.181) data 0.000 (0.079) loss 0.7695 (0.5823) acc 84.3750 (87.5000) lr 1.7604e-03 eta 0:16:47
epoch [47/200] batch [1/3] time 2.324 (2.324) data 0.226 (0.226) loss 0.3901 (0.3901) acc 93.7500 (93.7500) lr 1.7604e-03 eta 0:17:51
epoch [47/200] batch [2/3] time 2.104 (2.214) data 0.000 (0.113) loss 0.7329 (0.5615) acc 81.2500 (87.5000) lr 1.7604e-03 eta 0:16:58
epoch [47/200] batch [3/3] time 2.105 (2.178) data 0.000 (0.075) loss 0.5234 (0.5488) acc 87.5000 (87.5000) lr 1.7501e-03 eta 0:16:39
epoch [48/200] batch [1/3] time 2.322 (2.322) data 0.229 (0.229) loss 0.7319 (0.7319) acc 84.3750 (84.3750) lr 1.7501e-03 eta 0:17:43
epoch [48/200] batch [2/3] time 2.106 (2.214) data 0.000 (0.114) loss 0.4336 (0.5828) acc 90.6250 (87.5000) lr 1.7501e-03 eta 0:16:51
epoch [48/200] batch [3/3] time 2.103 (2.177) data 0.000 (0.076) loss 0.5127 (0.5594) acc 87.5000 (87.5000) lr 1.7396e-03 eta 0:16:32
epoch [49/200] batch [1/3] time 2.311 (2.311) data 0.219 (0.219) loss 0.3262 (0.3262) acc 90.6250 (90.6250) lr 1.7396e-03 eta 0:17:31
epoch [49/200] batch [2/3] time 2.101 (2.206) data 0.000 (0.110) loss 0.4414 (0.3838) acc 87.5000 (89.0625) lr 1.7396e-03 eta 0:16:41
epoch [49/200] batch [3/3] time 2.098 (2.170) data 0.000 (0.073) loss 0.4915 (0.4197) acc 87.5000 (88.5417) lr 1.7290e-03 eta 0:16:22
epoch [50/200] batch [1/3] time 2.332 (2.332) data 0.234 (0.234) loss 0.6479 (0.6479) acc 90.6250 (90.6250) lr 1.7290e-03 eta 0:17:34
epoch [50/200] batch [2/3] time 2.094 (2.213) data 0.000 (0.117) loss 0.6567 (0.6523) acc 81.2500 (85.9375) lr 1.7290e-03 eta 0:16:37
epoch [50/200] batch [3/3] time 2.108 (2.178) data 0.000 (0.078) loss 0.7803 (0.6950) acc 78.1250 (83.3333) lr 1.7181e-03 eta 0:16:19
epoch [51/200] batch [1/3] time 2.327 (2.327) data 0.231 (0.231) loss 0.5518 (0.5518) acc 84.3750 (84.3750) lr 1.7181e-03 eta 0:17:24
epoch [51/200] batch [2/3] time 2.108 (2.217) data 0.000 (0.116) loss 0.5156 (0.5337) acc 81.2500 (82.8125) lr 1.7181e-03 eta 0:16:33
epoch [51/200] batch [3/3] time 2.104 (2.180) data 0.000 (0.077) loss 0.3955 (0.4876) acc 93.7500 (86.4583) lr 1.7071e-03 eta 0:16:14
epoch [52/200] batch [1/3] time 2.323 (2.323) data 0.231 (0.231) loss 0.4714 (0.4714) acc 87.5000 (87.5000) lr 1.7071e-03 eta 0:17:16
epoch [52/200] batch [2/3] time 2.100 (2.212) data 0.000 (0.115) loss 0.6577 (0.5646) acc 84.3750 (85.9375) lr 1.7071e-03 eta 0:16:24
epoch [52/200] batch [3/3] time 2.091 (2.171) data 0.000 (0.077) loss 0.4360 (0.5217) acc 90.6250 (87.5000) lr 1.6959e-03 eta 0:16:04
epoch [53/200] batch [1/3] time 2.312 (2.312) data 0.229 (0.229) loss 0.4097 (0.4097) acc 93.7500 (93.7500) lr 1.6959e-03 eta 0:17:04
epoch [53/200] batch [2/3] time 2.099 (2.206) data 0.000 (0.115) loss 0.4397 (0.4247) acc 84.3750 (89.0625) lr 1.6959e-03 eta 0:16:15
epoch [53/200] batch [3/3] time 2.096 (2.169) data 0.000 (0.076) loss 0.3718 (0.4071) acc 96.8750 (91.6667) lr 1.6845e-03 eta 0:15:56
epoch [54/200] batch [1/3] time 2.324 (2.324) data 0.230 (0.230) loss 0.6143 (0.6143) acc 81.2500 (81.2500) lr 1.6845e-03 eta 0:17:02
epoch [54/200] batch [2/3] time 2.104 (2.214) data 0.000 (0.115) loss 1.0029 (0.8086) acc 75.0000 (78.1250) lr 1.6845e-03 eta 0:16:11
epoch [54/200] batch [3/3] time 2.097 (2.175) data 0.000 (0.077) loss 0.5015 (0.7062) acc 87.5000 (81.2500) lr 1.6730e-03 eta 0:15:52
epoch [55/200] batch [1/3] time 2.318 (2.318) data 0.218 (0.218) loss 1.3643 (1.3643) acc 68.7500 (68.7500) lr 1.6730e-03 eta 0:16:52
epoch [55/200] batch [2/3] time 2.107 (2.212) data 0.000 (0.109) loss 0.3142 (0.8392) acc 100.0000 (84.3750) lr 1.6730e-03 eta 0:16:04
epoch [55/200] batch [3/3] time 2.109 (2.178) data 0.000 (0.073) loss 0.6582 (0.7789) acc 87.5000 (85.4167) lr 1.6613e-03 eta 0:15:47
epoch [56/200] batch [1/3] time 2.314 (2.314) data 0.219 (0.219) loss 0.4214 (0.4214) acc 87.5000 (87.5000) lr 1.6613e-03 eta 0:16:44
epoch [56/200] batch [2/3] time 2.102 (2.208) data 0.000 (0.109) loss 0.5166 (0.4690) acc 90.6250 (89.0625) lr 1.6613e-03 eta 0:15:56
epoch [56/200] batch [3/3] time 2.094 (2.170) data 0.000 (0.073) loss 0.4927 (0.4769) acc 93.7500 (90.6250) lr 1.6494e-03 eta 0:15:37
epoch [57/200] batch [1/3] time 2.327 (2.327) data 0.221 (0.221) loss 0.6855 (0.6855) acc 81.2500 (81.2500) lr 1.6494e-03 eta 0:16:43
epoch [57/200] batch [2/3] time 2.109 (2.218) data 0.000 (0.111) loss 1.0898 (0.8877) acc 71.8750 (76.5625) lr 1.6494e-03 eta 0:15:53
epoch [57/200] batch [3/3] time 2.104 (2.180) data 0.000 (0.074) loss 0.3862 (0.7205) acc 93.7500 (82.2917) lr 1.6374e-03 eta 0:15:35
epoch [58/200] batch [1/3] time 2.325 (2.325) data 0.222 (0.222) loss 0.6270 (0.6270) acc 87.5000 (87.5000) lr 1.6374e-03 eta 0:16:35
epoch [58/200] batch [2/3] time 2.104 (2.214) data 0.000 (0.111) loss 0.8677 (0.7473) acc 78.1250 (82.8125) lr 1.6374e-03 eta 0:15:45
epoch [58/200] batch [3/3] time 2.110 (2.180) data 0.000 (0.074) loss 0.5742 (0.6896) acc 87.5000 (84.3750) lr 1.6252e-03 eta 0:15:28
epoch [59/200] batch [1/3] time 2.340 (2.340) data 0.240 (0.240) loss 0.7808 (0.7808) acc 84.3750 (84.3750) lr 1.6252e-03 eta 0:16:34
epoch [59/200] batch [2/3] time 2.108 (2.224) data 0.000 (0.120) loss 0.6299 (0.7053) acc 87.5000 (85.9375) lr 1.6252e-03 eta 0:15:42
epoch [59/200] batch [3/3] time 2.151 (2.200) data 0.000 (0.080) loss 0.3950 (0.6019) acc 90.6250 (87.5000) lr 1.6129e-03 eta 0:15:30
epoch [60/200] batch [1/3] time 2.426 (2.426) data 0.233 (0.233) loss 0.4177 (0.4177) acc 90.6250 (90.6250) lr 1.6129e-03 eta 0:17:03
epoch [60/200] batch [2/3] time 2.158 (2.292) data 0.000 (0.116) loss 0.4446 (0.4312) acc 90.6250 (90.6250) lr 1.6129e-03 eta 0:16:04
epoch [60/200] batch [3/3] time 2.149 (2.244) data 0.000 (0.078) loss 0.6245 (0.4956) acc 81.2500 (87.5000) lr 1.6004e-03 eta 0:15:42
epoch [61/200] batch [1/3] time 2.322 (2.322) data 0.228 (0.228) loss 0.4304 (0.4304) acc 93.7500 (93.7500) lr 1.6004e-03 eta 0:16:12
epoch [61/200] batch [2/3] time 2.105 (2.213) data 0.000 (0.114) loss 0.3369 (0.3837) acc 96.8750 (95.3125) lr 1.6004e-03 eta 0:15:25
epoch [61/200] batch [3/3] time 2.062 (2.163) data 0.000 (0.076) loss 0.3535 (0.3736) acc 93.7500 (94.7917) lr 1.5878e-03 eta 0:15:01
epoch [62/200] batch [1/3] time 2.296 (2.296) data 0.220 (0.220) loss 0.4419 (0.4419) acc 93.7500 (93.7500) lr 1.5878e-03 eta 0:15:55
epoch [62/200] batch [2/3] time 2.073 (2.184) data 0.000 (0.110) loss 0.2808 (0.3613) acc 93.7500 (93.7500) lr 1.5878e-03 eta 0:15:06
epoch [62/200] batch [3/3] time 2.075 (2.148) data 0.000 (0.073) loss 0.4346 (0.3857) acc 87.5000 (91.6667) lr 1.5750e-03 eta 0:14:49
epoch [63/200] batch [1/3] time 2.288 (2.288) data 0.221 (0.221) loss 0.6714 (0.6714) acc 87.5000 (87.5000) lr 1.5750e-03 eta 0:15:45
epoch [63/200] batch [2/3] time 2.070 (2.179) data 0.000 (0.111) loss 0.4375 (0.5544) acc 96.8750 (92.1875) lr 1.5750e-03 eta 0:14:57
epoch [63/200] batch [3/3] time 2.055 (2.138) data 0.000 (0.074) loss 0.2313 (0.4467) acc 93.7500 (92.7083) lr 1.5621e-03 eta 0:14:38
epoch [64/200] batch [1/3] time 2.303 (2.303) data 0.229 (0.229) loss 0.3752 (0.3752) acc 96.8750 (96.8750) lr 1.5621e-03 eta 0:15:44
epoch [64/200] batch [2/3] time 2.069 (2.186) data 0.000 (0.114) loss 0.5459 (0.4606) acc 90.6250 (93.7500) lr 1.5621e-03 eta 0:14:54
epoch [64/200] batch [3/3] time 2.078 (2.150) data 0.000 (0.076) loss 0.7744 (0.5652) acc 81.2500 (89.5833) lr 1.5490e-03 eta 0:14:37
epoch [65/200] batch [1/3] time 2.306 (2.306) data 0.229 (0.229) loss 0.5811 (0.5811) acc 84.3750 (84.3750) lr 1.5490e-03 eta 0:15:38
epoch [65/200] batch [2/3] time 2.077 (2.192) data 0.000 (0.115) loss 0.6089 (0.5950) acc 87.5000 (85.9375) lr 1.5490e-03 eta 0:14:49
epoch [65/200] batch [3/3] time 2.072 (2.152) data 0.000 (0.076) loss 0.5542 (0.5814) acc 90.6250 (87.5000) lr 1.5358e-03 eta 0:14:31
epoch [66/200] batch [1/3] time 2.300 (2.300) data 0.226 (0.226) loss 0.6797 (0.6797) acc 81.2500 (81.2500) lr 1.5358e-03 eta 0:15:29
epoch [66/200] batch [2/3] time 2.061 (2.181) data 0.000 (0.113) loss 0.3875 (0.5336) acc 90.6250 (85.9375) lr 1.5358e-03 eta 0:14:38
epoch [66/200] batch [3/3] time 2.073 (2.145) data 0.000 (0.075) loss 0.6201 (0.5624) acc 81.2500 (84.3750) lr 1.5225e-03 eta 0:14:22
epoch [67/200] batch [1/3] time 2.314 (2.314) data 0.236 (0.236) loss 0.6040 (0.6040) acc 84.3750 (84.3750) lr 1.5225e-03 eta 0:15:27
epoch [67/200] batch [2/3] time 2.078 (2.196) data 0.000 (0.118) loss 0.5508 (0.5774) acc 84.3750 (84.3750) lr 1.5225e-03 eta 0:14:38
epoch [67/200] batch [3/3] time 2.057 (2.150) data 0.000 (0.079) loss 0.4348 (0.5299) acc 90.6250 (86.4583) lr 1.5090e-03 eta 0:14:17
epoch [68/200] batch [1/3] time 2.313 (2.313) data 0.238 (0.238) loss 0.4885 (0.4885) acc 93.7500 (93.7500) lr 1.5090e-03 eta 0:15:20
epoch [68/200] batch [2/3] time 2.071 (2.192) data 0.000 (0.119) loss 0.4277 (0.4581) acc 87.5000 (90.6250) lr 1.5090e-03 eta 0:14:30
epoch [68/200] batch [3/3] time 2.078 (2.154) data 0.000 (0.079) loss 0.7285 (0.5483) acc 84.3750 (88.5417) lr 1.4955e-03 eta 0:14:12
epoch [69/200] batch [1/3] time 2.296 (2.296) data 0.230 (0.230) loss 0.6431 (0.6431) acc 87.5000 (87.5000) lr 1.4955e-03 eta 0:15:06
epoch [69/200] batch [2/3] time 2.064 (2.180) data 0.000 (0.115) loss 0.3259 (0.4845) acc 90.6250 (89.0625) lr 1.4955e-03 eta 0:14:18
epoch [69/200] batch [3/3] time 2.062 (2.140) data 0.000 (0.077) loss 0.1580 (0.3757) acc 100.0000 (92.7083) lr 1.4818e-03 eta 0:14:01
epoch [70/200] batch [1/3] time 2.296 (2.296) data 0.223 (0.223) loss 0.5068 (0.5068) acc 90.6250 (90.6250) lr 1.4818e-03 eta 0:14:59
epoch [70/200] batch [2/3] time 2.083 (2.189) data 0.000 (0.112) loss 0.6660 (0.5864) acc 87.5000 (89.0625) lr 1.4818e-03 eta 0:14:16
epoch [70/200] batch [3/3] time 2.061 (2.147) data 0.000 (0.074) loss 0.2090 (0.4606) acc 96.8750 (91.6667) lr 1.4679e-03 eta 0:13:57
epoch [71/200] batch [1/3] time 2.283 (2.283) data 0.221 (0.221) loss 0.4993 (0.4993) acc 93.7500 (93.7500) lr 1.4679e-03 eta 0:14:48
epoch [71/200] batch [2/3] time 2.058 (2.171) data 0.000 (0.111) loss 0.1095 (0.3044) acc 100.0000 (96.8750) lr 1.4679e-03 eta 0:14:02
epoch [71/200] batch [3/3] time 2.079 (2.140) data 0.000 (0.074) loss 0.8901 (0.4996) acc 78.1250 (90.6250) lr 1.4540e-03 eta 0:13:48
epoch [72/200] batch [1/3] time 2.275 (2.275) data 0.219 (0.219) loss 0.2302 (0.2302) acc 90.6250 (90.6250) lr 1.4540e-03 eta 0:14:38
epoch [72/200] batch [2/3] time 2.074 (2.174) data 0.000 (0.110) loss 0.7505 (0.4904) acc 81.2500 (85.9375) lr 1.4540e-03 eta 0:13:57
epoch [72/200] batch [3/3] time 2.066 (2.138) data 0.000 (0.073) loss 0.2014 (0.3940) acc 96.8750 (89.5833) lr 1.4399e-03 eta 0:13:41
epoch [73/200] batch [1/3] time 2.297 (2.297) data 0.229 (0.229) loss 0.2390 (0.2390) acc 100.0000 (100.0000) lr 1.4399e-03 eta 0:14:39
epoch [73/200] batch [2/3] time 2.064 (2.180) data 0.000 (0.115) loss 0.3472 (0.2931) acc 93.7500 (96.8750) lr 1.4399e-03 eta 0:13:52
epoch [73/200] batch [3/3] time 2.073 (2.145) data 0.000 (0.077) loss 0.3403 (0.3088) acc 93.7500 (95.8333) lr 1.4258e-03 eta 0:13:37
epoch [74/200] batch [1/3] time 2.296 (2.296) data 0.229 (0.229) loss 0.3730 (0.3730) acc 90.6250 (90.6250) lr 1.4258e-03 eta 0:14:32
epoch [74/200] batch [2/3] time 2.077 (2.186) data 0.000 (0.114) loss 0.5356 (0.4543) acc 84.3750 (87.5000) lr 1.4258e-03 eta 0:13:48
epoch [74/200] batch [3/3] time 2.073 (2.148) data 0.000 (0.076) loss 0.3369 (0.4152) acc 87.5000 (87.5000) lr 1.4115e-03 eta 0:13:32
epoch [75/200] batch [1/3] time 2.302 (2.302) data 0.225 (0.225) loss 0.8467 (0.8467) acc 78.1250 (78.1250) lr 1.4115e-03 eta 0:14:27
epoch [75/200] batch [2/3] time 2.074 (2.188) data 0.000 (0.113) loss 0.3726 (0.6096) acc 96.8750 (87.5000) lr 1.4115e-03 eta 0:13:42
epoch [75/200] batch [3/3] time 2.054 (2.143) data 0.000 (0.075) loss 0.1738 (0.4644) acc 96.8750 (90.6250) lr 1.3971e-03 eta 0:13:23
epoch [76/200] batch [1/3] time 2.298 (2.298) data 0.233 (0.233) loss 0.4041 (0.4041) acc 93.7500 (93.7500) lr 1.3971e-03 eta 0:14:19
epoch [76/200] batch [2/3] time 2.075 (2.186) data 0.000 (0.116) loss 0.5278 (0.4659) acc 90.6250 (92.1875) lr 1.3971e-03 eta 0:13:35
epoch [76/200] batch [3/3] time 2.076 (2.149) data 0.000 (0.078) loss 0.5098 (0.4806) acc 87.5000 (90.6250) lr 1.3827e-03 eta 0:13:19
epoch [77/200] batch [1/3] time 2.295 (2.295) data 0.217 (0.217) loss 0.4292 (0.4292) acc 93.7500 (93.7500) lr 1.3827e-03 eta 0:14:11
epoch [77/200] batch [2/3] time 2.075 (2.185) data 0.000 (0.109) loss 0.4478 (0.4385) acc 90.6250 (92.1875) lr 1.3827e-03 eta 0:13:28
epoch [77/200] batch [3/3] time 2.063 (2.145) data 0.000 (0.072) loss 0.3052 (0.3940) acc 90.6250 (91.6667) lr 1.3681e-03 eta 0:13:11
epoch [78/200] batch [1/3] time 2.294 (2.294) data 0.222 (0.222) loss 0.4419 (0.4419) acc 87.5000 (87.5000) lr 1.3681e-03 eta 0:14:04
epoch [78/200] batch [2/3] time 2.060 (2.177) data 0.000 (0.111) loss 0.3550 (0.3984) acc 90.6250 (89.0625) lr 1.3681e-03 eta 0:13:18
epoch [78/200] batch [3/3] time 2.071 (2.142) data 0.000 (0.074) loss 0.2429 (0.3466) acc 96.8750 (91.6667) lr 1.3535e-03 eta 0:13:03
epoch [79/200] batch [1/3] time 2.272 (2.272) data 0.216 (0.216) loss 0.2065 (0.2065) acc 93.7500 (93.7500) lr 1.3535e-03 eta 0:13:49
epoch [79/200] batch [2/3] time 2.066 (2.169) data 0.000 (0.108) loss 0.4993 (0.3529) acc 87.5000 (90.6250) lr 1.3535e-03 eta 0:13:09
epoch [79/200] batch [3/3] time 2.060 (2.133) data 0.000 (0.072) loss 0.2627 (0.3228) acc 96.8750 (92.7083) lr 1.3387e-03 eta 0:12:54
epoch [80/200] batch [1/3] time 2.275 (2.275) data 0.219 (0.219) loss 0.1714 (0.1714) acc 96.8750 (96.8750) lr 1.3387e-03 eta 0:13:43
epoch [80/200] batch [2/3] time 2.066 (2.171) data 0.000 (0.110) loss 0.4124 (0.2919) acc 93.7500 (95.3125) lr 1.3387e-03 eta 0:13:03
epoch [80/200] batch [3/3] time 2.045 (2.129) data 0.000 (0.073) loss 0.0851 (0.2229) acc 100.0000 (96.8750) lr 1.3239e-03 eta 0:12:46
epoch [81/200] batch [1/3] time 2.302 (2.302) data 0.225 (0.225) loss 0.7969 (0.7969) acc 81.2500 (81.2500) lr 1.3239e-03 eta 0:13:46
epoch [81/200] batch [2/3] time 2.076 (2.189) data 0.000 (0.112) loss 0.5498 (0.6733) acc 87.5000 (84.3750) lr 1.3239e-03 eta 0:13:03
epoch [81/200] batch [3/3] time 2.067 (2.148) data 0.000 (0.075) loss 0.2595 (0.5354) acc 96.8750 (88.5417) lr 1.3090e-03 eta 0:12:46
epoch [82/200] batch [1/3] time 2.292 (2.292) data 0.221 (0.221) loss 0.4978 (0.4978) acc 84.3750 (84.3750) lr 1.3090e-03 eta 0:13:36
epoch [82/200] batch [2/3] time 2.063 (2.177) data 0.000 (0.111) loss 0.2798 (0.3888) acc 93.7500 (89.0625) lr 1.3090e-03 eta 0:12:53
epoch [82/200] batch [3/3] time 2.074 (2.143) data 0.000 (0.074) loss 0.3999 (0.3925) acc 93.7500 (90.6250) lr 1.2940e-03 eta 0:12:38
epoch [83/200] batch [1/3] time 2.297 (2.297) data 0.233 (0.233) loss 0.5601 (0.5601) acc 93.7500 (93.7500) lr 1.2940e-03 eta 0:13:30
epoch [83/200] batch [2/3] time 2.068 (2.183) data 0.000 (0.117) loss 0.4485 (0.5043) acc 93.7500 (93.7500) lr 1.2940e-03 eta 0:12:48
epoch [83/200] batch [3/3] time 2.060 (2.142) data 0.000 (0.078) loss 0.4653 (0.4913) acc 87.5000 (91.6667) lr 1.2790e-03 eta 0:12:31
epoch [84/200] batch [1/3] time 2.312 (2.312) data 0.239 (0.239) loss 0.5400 (0.5400) acc 87.5000 (87.5000) lr 1.2790e-03 eta 0:13:29
epoch [84/200] batch [2/3] time 2.068 (2.190) data 0.000 (0.120) loss 0.1818 (0.3609) acc 100.0000 (93.7500) lr 1.2790e-03 eta 0:12:44
epoch [84/200] batch [3/3] time 2.066 (2.149) data 0.000 (0.080) loss 0.2915 (0.3378) acc 96.8750 (94.7917) lr 1.2639e-03 eta 0:12:27
epoch [85/200] batch [1/3] time 2.288 (2.288) data 0.225 (0.225) loss 0.1996 (0.1996) acc 96.8750 (96.8750) lr 1.2639e-03 eta 0:13:13
epoch [85/200] batch [2/3] time 2.070 (2.179) data 0.000 (0.113) loss 0.3882 (0.2939) acc 90.6250 (93.7500) lr 1.2639e-03 eta 0:12:33
epoch [85/200] batch [3/3] time 2.073 (2.144) data 0.000 (0.075) loss 0.6504 (0.4127) acc 78.1250 (88.5417) lr 1.2487e-03 eta 0:12:19
epoch [86/200] batch [1/3] time 2.289 (2.289) data 0.219 (0.219) loss 0.2695 (0.2695) acc 93.7500 (93.7500) lr 1.2487e-03 eta 0:13:07
epoch [86/200] batch [2/3] time 2.062 (2.176) data 0.000 (0.110) loss 0.1898 (0.2297) acc 96.8750 (95.3125) lr 1.2487e-03 eta 0:12:26
epoch [86/200] batch [3/3] time 2.056 (2.136) data 0.000 (0.073) loss 0.1177 (0.1923) acc 96.8750 (95.8333) lr 1.2334e-03 eta 0:12:10
epoch [87/200] batch [1/3] time 2.275 (2.275) data 0.219 (0.219) loss 0.1465 (0.1465) acc 100.0000 (100.0000) lr 1.2334e-03 eta 0:12:55
epoch [87/200] batch [2/3] time 2.070 (2.172) data 0.000 (0.110) loss 0.2732 (0.2098) acc 90.6250 (95.3125) lr 1.2334e-03 eta 0:12:18
epoch [87/200] batch [3/3] time 2.077 (2.141) data 0.000 (0.073) loss 0.4321 (0.2839) acc 90.6250 (93.7500) lr 1.2181e-03 eta 0:12:05
epoch [88/200] batch [1/3] time 2.296 (2.296) data 0.222 (0.222) loss 0.5952 (0.5952) acc 87.5000 (87.5000) lr 1.2181e-03 eta 0:12:55
epoch [88/200] batch [2/3] time 2.073 (2.184) data 0.000 (0.111) loss 0.7485 (0.6719) acc 87.5000 (87.5000) lr 1.2181e-03 eta 0:12:16
epoch [88/200] batch [3/3] time 2.048 (2.139) data 0.000 (0.074) loss 0.1134 (0.4857) acc 96.8750 (90.6250) lr 1.2028e-03 eta 0:11:58
epoch [89/200] batch [1/3] time 2.296 (2.296) data 0.218 (0.218) loss 0.4089 (0.4089) acc 93.7500 (93.7500) lr 1.2028e-03 eta 0:12:49
epoch [89/200] batch [2/3] time 2.076 (2.186) data 0.000 (0.109) loss 0.2957 (0.3523) acc 93.7500 (93.7500) lr 1.2028e-03 eta 0:12:10
epoch [89/200] batch [3/3] time 2.055 (2.142) data 0.000 (0.073) loss 0.1008 (0.2685) acc 100.0000 (95.8333) lr 1.1874e-03 eta 0:11:53
epoch [90/200] batch [1/3] time 2.285 (2.285) data 0.217 (0.217) loss 0.2042 (0.2042) acc 96.8750 (96.8750) lr 1.1874e-03 eta 0:12:38
epoch [90/200] batch [2/3] time 2.078 (2.181) data 0.000 (0.109) loss 0.4944 (0.3493) acc 90.6250 (93.7500) lr 1.1874e-03 eta 0:12:02
epoch [90/200] batch [3/3] time 2.069 (2.144) data 0.000 (0.073) loss 0.3613 (0.3533) acc 93.7500 (93.7500) lr 1.1719e-03 eta 0:11:47
epoch [91/200] batch [1/3] time 2.300 (2.300) data 0.233 (0.233) loss 0.1703 (0.1703) acc 100.0000 (100.0000) lr 1.1719e-03 eta 0:12:36
epoch [91/200] batch [2/3] time 2.075 (2.187) data 0.000 (0.117) loss 0.5410 (0.3557) acc 87.5000 (93.7500) lr 1.1719e-03 eta 0:11:57
epoch [91/200] batch [3/3] time 2.064 (2.146) data 0.000 (0.078) loss 0.2009 (0.3041) acc 93.7500 (93.7500) lr 1.1564e-03 eta 0:11:41
epoch [92/200] batch [1/3] time 2.308 (2.308) data 0.234 (0.234) loss 0.5356 (0.5356) acc 87.5000 (87.5000) lr 1.1564e-03 eta 0:12:32
epoch [92/200] batch [2/3] time 2.073 (2.190) data 0.000 (0.117) loss 0.3054 (0.4205) acc 93.7500 (90.6250) lr 1.1564e-03 eta 0:11:51
epoch [92/200] batch [3/3] time 2.075 (2.152) data 0.000 (0.078) loss 0.3943 (0.4118) acc 90.6250 (90.6250) lr 1.1409e-03 eta 0:11:37
epoch [93/200] batch [1/3] time 2.295 (2.295) data 0.216 (0.216) loss 0.6982 (0.6982) acc 81.2500 (81.2500) lr 1.1409e-03 eta 0:12:21
epoch [93/200] batch [2/3] time 2.070 (2.182) data 0.000 (0.108) loss 0.4863 (0.5923) acc 84.3750 (82.8125) lr 1.1409e-03 eta 0:11:42
epoch [93/200] batch [3/3] time 2.064 (2.143) data 0.000 (0.072) loss 0.3552 (0.5133) acc 87.5000 (84.3750) lr 1.1253e-03 eta 0:11:27
epoch [94/200] batch [1/3] time 2.287 (2.287) data 0.228 (0.228) loss 0.4006 (0.4006) acc 93.7500 (93.7500) lr 1.1253e-03 eta 0:12:11
epoch [94/200] batch [2/3] time 2.073 (2.180) data 0.000 (0.114) loss 0.5811 (0.4908) acc 87.5000 (90.6250) lr 1.1253e-03 eta 0:11:35
epoch [94/200] batch [3/3] time 2.049 (2.136) data 0.000 (0.076) loss 0.0872 (0.3563) acc 100.0000 (93.7500) lr 1.1097e-03 eta 0:11:19
epoch [95/200] batch [1/3] time 2.298 (2.298) data 0.226 (0.226) loss 0.3440 (0.3440) acc 90.6250 (90.6250) lr 1.1097e-03 eta 0:12:08
epoch [95/200] batch [2/3] time 2.084 (2.191) data 0.000 (0.113) loss 0.6660 (0.5050) acc 81.2500 (85.9375) lr 1.1097e-03 eta 0:11:32
epoch [95/200] batch [3/3] time 2.073 (2.152) data 0.000 (0.076) loss 0.5498 (0.5199) acc 84.3750 (85.4167) lr 1.0941e-03 eta 0:11:17
epoch [96/200] batch [1/3] time 2.285 (2.285) data 0.214 (0.214) loss 0.2751 (0.2751) acc 96.8750 (96.8750) lr 1.0941e-03 eta 0:11:57
epoch [96/200] batch [2/3] time 2.069 (2.177) data 0.000 (0.107) loss 0.3140 (0.2946) acc 93.7500 (95.3125) lr 1.0941e-03 eta 0:11:21
epoch [96/200] batch [3/3] time 2.043 (2.132) data 0.000 (0.071) loss 0.1580 (0.2490) acc 96.8750 (95.8333) lr 1.0785e-03 eta 0:11:05
epoch [97/200] batch [1/3] time 2.289 (2.289) data 0.218 (0.218) loss 0.3396 (0.3396) acc 96.8750 (96.8750) lr 1.0785e-03 eta 0:11:51
epoch [97/200] batch [2/3] time 2.068 (2.179) data 0.000 (0.109) loss 0.2605 (0.3000) acc 96.8750 (96.8750) lr 1.0785e-03 eta 0:11:15
epoch [97/200] batch [3/3] time 2.045 (2.134) data 0.000 (0.073) loss 0.0727 (0.2243) acc 100.0000 (97.9167) lr 1.0628e-03 eta 0:10:59
epoch [98/200] batch [1/3] time 2.292 (2.292) data 0.216 (0.216) loss 0.5669 (0.5669) acc 84.3750 (84.3750) lr 1.0628e-03 eta 0:11:45
epoch [98/200] batch [2/3] time 2.069 (2.181) data 0.000 (0.108) loss 0.3142 (0.4406) acc 93.7500 (89.0625) lr 1.0628e-03 eta 0:11:09
epoch [98/200] batch [3/3] time 2.072 (2.145) data 0.000 (0.072) loss 0.6787 (0.5199) acc 81.2500 (86.4583) lr 1.0471e-03 eta 0:10:56
epoch [99/200] batch [1/3] time 2.301 (2.301) data 0.233 (0.233) loss 0.4250 (0.4250) acc 90.6250 (90.6250) lr 1.0471e-03 eta 0:11:41
epoch [99/200] batch [2/3] time 2.056 (2.178) data 0.000 (0.117) loss 0.3713 (0.3982) acc 87.5000 (89.0625) lr 1.0471e-03 eta 0:11:02
epoch [99/200] batch [3/3] time 2.079 (2.145) data 0.000 (0.078) loss 0.5444 (0.4469) acc 84.3750 (87.5000) lr 1.0314e-03 eta 0:10:50
epoch [100/200] batch [1/3] time 2.279 (2.279) data 0.217 (0.217) loss 0.2273 (0.2273) acc 96.8750 (96.8750) lr 1.0314e-03 eta 0:11:28
epoch [100/200] batch [2/3] time 2.067 (2.173) data 0.000 (0.108) loss 0.4517 (0.3395) acc 90.6250 (93.7500) lr 1.0314e-03 eta 0:10:54
epoch [100/200] batch [3/3] time 2.078 (2.142) data 0.000 (0.072) loss 0.4580 (0.3790) acc 90.6250 (92.7083) lr 1.0157e-03 eta 0:10:42
epoch [101/200] batch [1/3] time 2.283 (2.283) data 0.227 (0.227) loss 0.1022 (0.1022) acc 100.0000 (100.0000) lr 1.0157e-03 eta 0:11:22
epoch [101/200] batch [2/3] time 2.048 (2.166) data 0.000 (0.114) loss 0.1049 (0.1036) acc 96.8750 (98.4375) lr 1.0157e-03 eta 0:10:45
epoch [101/200] batch [3/3] time 2.063 (2.131) data 0.000 (0.076) loss 0.6099 (0.2723) acc 90.6250 (95.8333) lr 1.0000e-03 eta 0:10:33
epoch [102/200] batch [1/3] time 2.335 (2.335) data 0.235 (0.235) loss 0.2075 (0.2075) acc 96.8750 (96.8750) lr 1.0000e-03 eta 0:11:31
epoch [102/200] batch [2/3] time 2.111 (2.223) data 0.000 (0.118) loss 0.1775 (0.1925) acc 96.8750 (96.8750) lr 1.0000e-03 eta 0:10:55
epoch [102/200] batch [3/3] time 2.116 (2.187) data 0.000 (0.078) loss 0.3623 (0.2491) acc 87.5000 (93.7500) lr 9.8429e-04 eta 0:10:43
epoch [103/200] batch [1/3] time 2.350 (2.350) data 0.240 (0.240) loss 0.5439 (0.5439) acc 81.2500 (81.2500) lr 9.8429e-04 eta 0:11:28
epoch [103/200] batch [2/3] time 2.089 (2.219) data 0.000 (0.120) loss 0.2559 (0.3999) acc 93.7500 (87.5000) lr 9.8429e-04 eta 0:10:48
epoch [103/200] batch [3/3] time 2.079 (2.173) data 0.000 (0.080) loss 0.3059 (0.3686) acc 87.5000 (87.5000) lr 9.6859e-04 eta 0:10:32
epoch [104/200] batch [1/3] time 2.313 (2.313) data 0.237 (0.237) loss 0.3176 (0.3176) acc 90.6250 (90.6250) lr 9.6859e-04 eta 0:11:10
epoch [104/200] batch [2/3] time 2.088 (2.200) data 0.000 (0.119) loss 0.6035 (0.4606) acc 84.3750 (87.5000) lr 9.6859e-04 eta 0:10:35
epoch [104/200] batch [3/3] time 2.067 (2.156) data 0.000 (0.079) loss 0.3303 (0.4172) acc 90.6250 (88.5417) lr 9.5289e-04 eta 0:10:20
epoch [105/200] batch [1/3] time 2.293 (2.293) data 0.227 (0.227) loss 0.2416 (0.2416) acc 96.8750 (96.8750) lr 9.5289e-04 eta 0:10:58
epoch [105/200] batch [2/3] time 2.081 (2.187) data 0.000 (0.113) loss 0.5054 (0.3735) acc 87.5000 (92.1875) lr 9.5289e-04 eta 0:10:25
epoch [105/200] batch [3/3] time 2.071 (2.148) data 0.000 (0.076) loss 0.3213 (0.3561) acc 87.5000 (90.6250) lr 9.3721e-04 eta 0:10:12
epoch [106/200] batch [1/3] time 2.282 (2.282) data 0.228 (0.228) loss 0.1215 (0.1215) acc 96.8750 (96.8750) lr 9.3721e-04 eta 0:10:48
epoch [106/200] batch [2/3] time 2.067 (2.174) data 0.000 (0.114) loss 0.3843 (0.2529) acc 90.6250 (93.7500) lr 9.3721e-04 eta 0:10:15
epoch [106/200] batch [3/3] time 2.065 (2.138) data 0.000 (0.076) loss 0.2336 (0.2465) acc 93.7500 (93.7500) lr 9.2154e-04 eta 0:10:02
epoch [107/200] batch [1/3] time 2.271 (2.271) data 0.220 (0.220) loss 0.2756 (0.2756) acc 96.8750 (96.8750) lr 9.2154e-04 eta 0:10:38
epoch [107/200] batch [2/3] time 2.057 (2.164) data 0.000 (0.110) loss 0.1989 (0.2372) acc 96.8750 (96.8750) lr 9.2154e-04 eta 0:10:05
epoch [107/200] batch [3/3] time 2.067 (2.132) data 0.000 (0.073) loss 0.3049 (0.2598) acc 93.7500 (95.8333) lr 9.0589e-04 eta 0:09:54
epoch [108/200] batch [1/3] time 2.296 (2.296) data 0.234 (0.234) loss 0.2184 (0.2184) acc 93.7500 (93.7500) lr 9.0589e-04 eta 0:10:38
epoch [108/200] batch [2/3] time 2.071 (2.184) data 0.000 (0.117) loss 0.2410 (0.2297) acc 93.7500 (93.7500) lr 9.0589e-04 eta 0:10:04
epoch [108/200] batch [3/3] time 2.065 (2.144) data 0.000 (0.078) loss 0.3186 (0.2593) acc 93.7500 (93.7500) lr 8.9027e-04 eta 0:09:51
epoch [109/200] batch [1/3] time 2.291 (2.291) data 0.227 (0.227) loss 0.3516 (0.3516) acc 90.6250 (90.6250) lr 8.9027e-04 eta 0:10:29
epoch [109/200] batch [2/3] time 2.075 (2.183) data 0.000 (0.114) loss 0.4487 (0.4001) acc 90.6250 (90.6250) lr 8.9027e-04 eta 0:09:58
epoch [109/200] batch [3/3] time 2.067 (2.144) data 0.000 (0.076) loss 0.3677 (0.3893) acc 93.7500 (91.6667) lr 8.7467e-04 eta 0:09:45
epoch [110/200] batch [1/3] time 2.289 (2.289) data 0.230 (0.230) loss 0.3823 (0.3823) acc 90.6250 (90.6250) lr 8.7467e-04 eta 0:10:22
epoch [110/200] batch [2/3] time 2.063 (2.176) data 0.000 (0.115) loss 0.5376 (0.4600) acc 90.6250 (90.6250) lr 8.7467e-04 eta 0:09:49
epoch [110/200] batch [3/3] time 2.066 (2.139) data 0.000 (0.077) loss 0.2585 (0.3928) acc 93.7500 (91.6667) lr 8.5910e-04 eta 0:09:37
epoch [111/200] batch [1/3] time 2.296 (2.296) data 0.217 (0.217) loss 0.5264 (0.5264) acc 84.3750 (84.3750) lr 8.5910e-04 eta 0:10:17
epoch [111/200] batch [2/3] time 2.069 (2.182) data 0.000 (0.108) loss 0.3091 (0.4177) acc 90.6250 (87.5000) lr 8.5910e-04 eta 0:09:44
epoch [111/200] batch [3/3] time 2.040 (2.135) data 0.000 (0.072) loss 0.1846 (0.3400) acc 96.8750 (90.6250) lr 8.4357e-04 eta 0:09:29
epoch [112/200] batch [1/3] time 2.303 (2.303) data 0.234 (0.234) loss 0.3257 (0.3257) acc 87.5000 (87.5000) lr 8.4357e-04 eta 0:10:12
epoch [112/200] batch [2/3] time 2.048 (2.175) data 0.000 (0.117) loss 0.1242 (0.2249) acc 96.8750 (92.1875) lr 8.4357e-04 eta 0:09:36
epoch [112/200] batch [3/3] time 2.068 (2.139) data 0.000 (0.078) loss 0.2954 (0.2484) acc 93.7500 (92.7083) lr 8.2807e-04 eta 0:09:24
epoch [113/200] batch [1/3] time 2.290 (2.290) data 0.217 (0.217) loss 0.3518 (0.3518) acc 87.5000 (87.5000) lr 8.2807e-04 eta 0:10:02
epoch [113/200] batch [2/3] time 2.059 (2.175) data 0.000 (0.109) loss 0.1589 (0.2554) acc 96.8750 (92.1875) lr 8.2807e-04 eta 0:09:29
epoch [113/200] batch [3/3] time 2.059 (2.136) data 0.000 (0.072) loss 0.2673 (0.2594) acc 93.7500 (92.7083) lr 8.1262e-04 eta 0:09:17
epoch [114/200] batch [1/3] time 2.310 (2.310) data 0.235 (0.235) loss 0.4282 (0.4282) acc 87.5000 (87.5000) lr 8.1262e-04 eta 0:10:00
epoch [114/200] batch [2/3] time 2.030 (2.170) data 0.000 (0.118) loss 0.0421 (0.2352) acc 100.0000 (93.7500) lr 8.1262e-04 eta 0:09:22
epoch [114/200] batch [3/3] time 2.075 (2.139) data 0.000 (0.079) loss 0.5786 (0.3497) acc 87.5000 (91.6667) lr 7.9721e-04 eta 0:09:11
epoch [115/200] batch [1/3] time 2.291 (2.291) data 0.219 (0.219) loss 0.5640 (0.5640) acc 81.2500 (81.2500) lr 7.9721e-04 eta 0:09:48
epoch [115/200] batch [2/3] time 2.073 (2.182) data 0.000 (0.110) loss 0.3486 (0.4563) acc 90.6250 (85.9375) lr 7.9721e-04 eta 0:09:18
epoch [115/200] batch [3/3] time 2.068 (2.144) data 0.000 (0.073) loss 0.4541 (0.4556) acc 87.5000 (86.4583) lr 7.8186e-04 eta 0:09:06
epoch [116/200] batch [1/3] time 2.303 (2.303) data 0.235 (0.235) loss 0.1915 (0.1915) acc 93.7500 (93.7500) lr 7.8186e-04 eta 0:09:44
epoch [116/200] batch [2/3] time 2.068 (2.185) data 0.000 (0.118) loss 0.2235 (0.2075) acc 93.7500 (93.7500) lr 7.8186e-04 eta 0:09:12
epoch [116/200] batch [3/3] time 2.052 (2.141) data 0.000 (0.079) loss 0.1053 (0.1734) acc 100.0000 (95.8333) lr 7.6655e-04 eta 0:08:59
epoch [117/200] batch [1/3] time 2.277 (2.277) data 0.228 (0.228) loss 0.1055 (0.1055) acc 100.0000 (100.0000) lr 7.6655e-04 eta 0:09:31
epoch [117/200] batch [2/3] time 2.070 (2.174) data 0.000 (0.114) loss 0.2328 (0.1691) acc 96.8750 (98.4375) lr 7.6655e-04 eta 0:09:03
epoch [117/200] batch [3/3] time 2.069 (2.139) data 0.000 (0.076) loss 0.2234 (0.1872) acc 100.0000 (98.9583) lr 7.5131e-04 eta 0:08:52
epoch [118/200] batch [1/3] time 2.290 (2.290) data 0.222 (0.222) loss 0.5132 (0.5132) acc 87.5000 (87.5000) lr 7.5131e-04 eta 0:09:27
epoch [118/200] batch [2/3] time 2.063 (2.177) data 0.000 (0.111) loss 0.2925 (0.4028) acc 90.6250 (89.0625) lr 7.5131e-04 eta 0:08:57
epoch [118/200] batch [3/3] time 2.061 (2.138) data 0.000 (0.074) loss 0.2539 (0.3532) acc 93.7500 (90.6250) lr 7.3613e-04 eta 0:08:45
epoch [119/200] batch [1/3] time 2.303 (2.303) data 0.227 (0.227) loss 0.5449 (0.5449) acc 87.5000 (87.5000) lr 7.3613e-04 eta 0:09:24
epoch [119/200] batch [2/3] time 2.066 (2.185) data 0.000 (0.114) loss 0.3267 (0.4358) acc 96.8750 (92.1875) lr 7.3613e-04 eta 0:08:53
epoch [119/200] batch [3/3] time 2.040 (2.137) data 0.000 (0.076) loss 0.0546 (0.3087) acc 100.0000 (94.7917) lr 7.2101e-04 eta 0:08:39
epoch [120/200] batch [1/3] time 2.299 (2.299) data 0.224 (0.224) loss 0.4846 (0.4846) acc 87.5000 (87.5000) lr 7.2101e-04 eta 0:09:16
epoch [120/200] batch [2/3] time 2.067 (2.183) data 0.000 (0.112) loss 0.2949 (0.3898) acc 93.7500 (90.6250) lr 7.2101e-04 eta 0:08:46
epoch [120/200] batch [3/3] time 2.070 (2.145) data 0.000 (0.075) loss 0.3628 (0.3808) acc 90.6250 (90.6250) lr 7.0596e-04 eta 0:08:34
epoch [121/200] batch [1/3] time 2.299 (2.299) data 0.232 (0.232) loss 0.3938 (0.3938) acc 87.5000 (87.5000) lr 7.0596e-04 eta 0:09:09
epoch [121/200] batch [2/3] time 2.049 (2.174) data 0.000 (0.116) loss 0.1267 (0.2603) acc 100.0000 (93.7500) lr 7.0596e-04 eta 0:08:37
epoch [121/200] batch [3/3] time 2.071 (2.140) data 0.000 (0.077) loss 0.3608 (0.2938) acc 90.6250 (92.7083) lr 6.9098e-04 eta 0:08:27
epoch [122/200] batch [1/3] time 2.290 (2.290) data 0.219 (0.219) loss 0.5537 (0.5537) acc 81.2500 (81.2500) lr 6.9098e-04 eta 0:09:00
epoch [122/200] batch [2/3] time 2.069 (2.180) data 0.000 (0.109) loss 0.1530 (0.3533) acc 100.0000 (90.6250) lr 6.9098e-04 eta 0:08:32
epoch [122/200] batch [3/3] time 2.060 (2.140) data 0.000 (0.073) loss 0.2568 (0.3212) acc 93.7500 (91.6667) lr 6.7608e-04 eta 0:08:20
epoch [123/200] batch [1/3] time 2.268 (2.268) data 0.219 (0.219) loss 0.1152 (0.1152) acc 96.8750 (96.8750) lr 6.7608e-04 eta 0:08:48
epoch [123/200] batch [2/3] time 2.064 (2.166) data 0.000 (0.109) loss 0.1990 (0.1571) acc 96.8750 (96.8750) lr 6.7608e-04 eta 0:08:22
epoch [123/200] batch [3/3] time 2.069 (2.134) data 0.000 (0.073) loss 0.7266 (0.3469) acc 81.2500 (91.6667) lr 6.6126e-04 eta 0:08:12
epoch [124/200] batch [1/3] time 2.292 (2.292) data 0.227 (0.227) loss 0.2952 (0.2952) acc 90.6250 (90.6250) lr 6.6126e-04 eta 0:08:47
epoch [124/200] batch [2/3] time 2.046 (2.169) data 0.000 (0.113) loss 0.0715 (0.1833) acc 100.0000 (95.3125) lr 6.6126e-04 eta 0:08:16
epoch [124/200] batch [3/3] time 2.068 (2.136) data 0.000 (0.076) loss 0.2394 (0.2020) acc 93.7500 (94.7917) lr 6.4653e-04 eta 0:08:06
epoch [125/200] batch [1/3] time 2.276 (2.276) data 0.218 (0.218) loss 0.3259 (0.3259) acc 93.7500 (93.7500) lr 6.4653e-04 eta 0:08:36
epoch [125/200] batch [2/3] time 2.065 (2.171) data 0.000 (0.109) loss 0.2710 (0.2985) acc 96.8750 (95.3125) lr 6.4653e-04 eta 0:08:10
epoch [125/200] batch [3/3] time 2.070 (2.137) data 0.000 (0.073) loss 0.4375 (0.3448) acc 87.5000 (92.7083) lr 6.3188e-04 eta 0:08:00
epoch [126/200] batch [1/3] time 2.300 (2.300) data 0.221 (0.221) loss 0.5713 (0.5713) acc 87.5000 (87.5000) lr 6.3188e-04 eta 0:08:35
epoch [126/200] batch [2/3] time 2.061 (2.180) data 0.000 (0.111) loss 0.2047 (0.3880) acc 93.7500 (90.6250) lr 6.3188e-04 eta 0:08:06
epoch [126/200] batch [3/3] time 2.059 (2.140) data 0.000 (0.074) loss 0.1909 (0.3223) acc 96.8750 (92.7083) lr 6.1732e-04 eta 0:07:55
epoch [127/200] batch [1/3] time 2.277 (2.277) data 0.228 (0.228) loss 0.4182 (0.4182) acc 90.6250 (90.6250) lr 6.1732e-04 eta 0:08:23
epoch [127/200] batch [2/3] time 2.075 (2.176) data 0.000 (0.114) loss 0.3438 (0.3810) acc 93.7500 (92.1875) lr 6.1732e-04 eta 0:07:58
epoch [127/200] batch [3/3] time 2.067 (2.140) data 0.000 (0.076) loss 0.3215 (0.3612) acc 93.7500 (92.7083) lr 6.0285e-04 eta 0:07:48
epoch [128/200] batch [1/3] time 2.293 (2.293) data 0.233 (0.233) loss 0.3101 (0.3101) acc 93.7500 (93.7500) lr 6.0285e-04 eta 0:08:19
epoch [128/200] batch [2/3] time 2.065 (2.179) data 0.000 (0.117) loss 0.2803 (0.2952) acc 96.8750 (95.3125) lr 6.0285e-04 eta 0:07:52
epoch [128/200] batch [3/3] time 2.061 (2.140) data 0.000 (0.078) loss 0.1442 (0.2448) acc 100.0000 (96.8750) lr 5.8849e-04 eta 0:07:42
epoch [129/200] batch [1/3] time 2.287 (2.287) data 0.225 (0.225) loss 0.2014 (0.2014) acc 96.8750 (96.8750) lr 5.8849e-04 eta 0:08:11
epoch [129/200] batch [2/3] time 2.068 (2.178) data 0.000 (0.112) loss 0.4578 (0.3296) acc 90.6250 (93.7500) lr 5.8849e-04 eta 0:07:46
epoch [129/200] batch [3/3] time 2.043 (2.133) data 0.000 (0.075) loss 0.0752 (0.2448) acc 100.0000 (95.8333) lr 5.7422e-04 eta 0:07:34
epoch [130/200] batch [1/3] time 2.274 (2.274) data 0.227 (0.227) loss 0.1403 (0.1403) acc 93.7500 (93.7500) lr 5.7422e-04 eta 0:08:02
epoch [130/200] batch [2/3] time 2.048 (2.161) data 0.000 (0.114) loss 0.0974 (0.1188) acc 96.8750 (95.3125) lr 5.7422e-04 eta 0:07:35
epoch [130/200] batch [3/3] time 2.055 (2.125) data 0.000 (0.076) loss 0.3328 (0.1901) acc 93.7500 (94.7917) lr 5.6006e-04 eta 0:07:26
epoch [131/200] batch [1/3] time 2.297 (2.297) data 0.235 (0.235) loss 0.3628 (0.3628) acc 90.6250 (90.6250) lr 5.6006e-04 eta 0:08:00
epoch [131/200] batch [2/3] time 2.053 (2.175) data 0.000 (0.117) loss 0.1515 (0.2571) acc 96.8750 (93.7500) lr 5.6006e-04 eta 0:07:32
epoch [131/200] batch [3/3] time 2.063 (2.138) data 0.000 (0.078) loss 0.2700 (0.2614) acc 96.8750 (94.7917) lr 5.4601e-04 eta 0:07:22
epoch [132/200] batch [1/3] time 2.299 (2.299) data 0.232 (0.232) loss 0.2756 (0.2756) acc 93.7500 (93.7500) lr 5.4601e-04 eta 0:07:53
epoch [132/200] batch [2/3] time 2.072 (2.185) data 0.000 (0.116) loss 0.3376 (0.3066) acc 93.7500 (93.7500) lr 5.4601e-04 eta 0:07:28
epoch [132/200] batch [3/3] time 2.072 (2.148) data 0.000 (0.077) loss 0.2964 (0.3032) acc 90.6250 (92.7083) lr 5.3207e-04 eta 0:07:18
epoch [133/200] batch [1/3] time 2.262 (2.262) data 0.220 (0.220) loss 0.0536 (0.0536) acc 100.0000 (100.0000) lr 5.3207e-04 eta 0:07:39
epoch [133/200] batch [2/3] time 2.062 (2.162) data 0.000 (0.110) loss 0.3945 (0.2241) acc 87.5000 (93.7500) lr 5.3207e-04 eta 0:07:16
epoch [133/200] batch [3/3] time 2.068 (2.130) data 0.000 (0.073) loss 0.2756 (0.2413) acc 93.7500 (93.7500) lr 5.1825e-04 eta 0:07:08
epoch [134/200] batch [1/3] time 2.283 (2.283) data 0.237 (0.237) loss 0.3457 (0.3457) acc 93.7500 (93.7500) lr 5.1825e-04 eta 0:07:36
epoch [134/200] batch [2/3] time 2.069 (2.176) data 0.000 (0.118) loss 0.1995 (0.2726) acc 100.0000 (96.8750) lr 5.1825e-04 eta 0:07:13
epoch [134/200] batch [3/3] time 2.054 (2.136) data 0.000 (0.079) loss 0.1373 (0.2275) acc 100.0000 (97.9167) lr 5.0454e-04 eta 0:07:02
epoch [135/200] batch [1/3] time 2.293 (2.293) data 0.229 (0.229) loss 0.6470 (0.6470) acc 90.6250 (90.6250) lr 5.0454e-04 eta 0:07:31
epoch [135/200] batch [2/3] time 2.060 (2.177) data 0.000 (0.115) loss 0.3540 (0.5005) acc 93.7500 (92.1875) lr 5.0454e-04 eta 0:07:06
epoch [135/200] batch [3/3] time 2.057 (2.137) data 0.000 (0.076) loss 0.2192 (0.4067) acc 93.7500 (92.7083) lr 4.9096e-04 eta 0:06:56
epoch [136/200] batch [1/3] time 2.272 (2.272) data 0.223 (0.223) loss 0.1788 (0.1788) acc 96.8750 (96.8750) lr 4.9096e-04 eta 0:07:20
epoch [136/200] batch [2/3] time 2.069 (2.171) data 0.000 (0.112) loss 0.2825 (0.2307) acc 90.6250 (93.7500) lr 4.9096e-04 eta 0:06:58
epoch [136/200] batch [3/3] time 2.060 (2.134) data 0.000 (0.074) loss 0.3582 (0.2732) acc 90.6250 (92.7083) lr 4.7750e-04 eta 0:06:49
epoch [137/200] batch [1/3] time 2.281 (2.281) data 0.223 (0.223) loss 0.1599 (0.1599) acc 96.8750 (96.8750) lr 4.7750e-04 eta 0:07:15
epoch [137/200] batch [2/3] time 2.060 (2.171) data 0.000 (0.112) loss 0.2839 (0.2219) acc 96.8750 (96.8750) lr 4.7750e-04 eta 0:06:52
epoch [137/200] batch [3/3] time 2.062 (2.134) data 0.000 (0.075) loss 0.2964 (0.2467) acc 93.7500 (95.8333) lr 4.6417e-04 eta 0:06:43
epoch [138/200] batch [1/3] time 2.293 (2.293) data 0.227 (0.227) loss 0.3044 (0.3044) acc 93.7500 (93.7500) lr 4.6417e-04 eta 0:07:11
epoch [138/200] batch [2/3] time 2.043 (2.168) data 0.000 (0.114) loss 0.1755 (0.2400) acc 93.7500 (93.7500) lr 4.6417e-04 eta 0:06:45
epoch [138/200] batch [3/3] time 2.073 (2.136) data 0.000 (0.076) loss 0.5059 (0.3286) acc 87.5000 (91.6667) lr 4.5098e-04 eta 0:06:37
epoch [139/200] batch [1/3] time 2.301 (2.301) data 0.238 (0.238) loss 0.2920 (0.2920) acc 93.7500 (93.7500) lr 4.5098e-04 eta 0:07:05
epoch [139/200] batch [2/3] time 2.081 (2.191) data 0.000 (0.119) loss 0.3142 (0.3031) acc 93.7500 (93.7500) lr 4.5098e-04 eta 0:06:43
epoch [139/200] batch [3/3] time 2.051 (2.144) data 0.000 (0.079) loss 0.1294 (0.2452) acc 100.0000 (95.8333) lr 4.3792e-04 eta 0:06:32
epoch [140/200] batch [1/3] time 2.289 (2.289) data 0.219 (0.219) loss 0.3433 (0.3433) acc 93.7500 (93.7500) lr 4.3792e-04 eta 0:06:56
epoch [140/200] batch [2/3] time 2.053 (2.171) data 0.000 (0.109) loss 0.2324 (0.2878) acc 90.6250 (92.1875) lr 4.3792e-04 eta 0:06:32
epoch [140/200] batch [3/3] time 2.051 (2.131) data 0.000 (0.073) loss 0.1847 (0.2535) acc 96.8750 (93.7500) lr 4.2499e-04 eta 0:06:23
epoch [141/200] batch [1/3] time 2.289 (2.289) data 0.227 (0.227) loss 0.1703 (0.1703) acc 96.8750 (96.8750) lr 4.2499e-04 eta 0:06:49
epoch [141/200] batch [2/3] time 2.059 (2.174) data 0.000 (0.114) loss 0.1655 (0.1679) acc 96.8750 (96.8750) lr 4.2499e-04 eta 0:06:26
epoch [141/200] batch [3/3] time 2.063 (2.137) data 0.000 (0.076) loss 0.3408 (0.2255) acc 93.7500 (95.8333) lr 4.1221e-04 eta 0:06:18
epoch [142/200] batch [1/3] time 2.287 (2.287) data 0.238 (0.238) loss 0.1571 (0.1571) acc 96.8750 (96.8750) lr 4.1221e-04 eta 0:06:42
epoch [142/200] batch [2/3] time 2.047 (2.167) data 0.000 (0.119) loss 0.1274 (0.1423) acc 100.0000 (98.4375) lr 4.1221e-04 eta 0:06:19
epoch [142/200] batch [3/3] time 2.038 (2.124) data 0.000 (0.079) loss 0.0834 (0.1226) acc 96.8750 (97.9167) lr 3.9958e-04 eta 0:06:09
epoch [143/200] batch [1/3] time 2.286 (2.286) data 0.228 (0.228) loss 0.3125 (0.3125) acc 90.6250 (90.6250) lr 3.9958e-04 eta 0:06:35
epoch [143/200] batch [2/3] time 2.048 (2.167) data 0.000 (0.114) loss 0.1131 (0.2128) acc 96.8750 (93.7500) lr 3.9958e-04 eta 0:06:12
epoch [143/200] batch [3/3] time 2.060 (2.131) data 0.000 (0.076) loss 0.1810 (0.2022) acc 93.7500 (93.7500) lr 3.8709e-04 eta 0:06:04
epoch [144/200] batch [1/3] time 2.291 (2.291) data 0.227 (0.227) loss 0.1918 (0.1918) acc 96.8750 (96.8750) lr 3.8709e-04 eta 0:06:29
epoch [144/200] batch [2/3] time 2.038 (2.164) data 0.000 (0.114) loss 0.1565 (0.1741) acc 96.8750 (96.8750) lr 3.8709e-04 eta 0:06:05
epoch [144/200] batch [3/3] time 2.058 (2.129) data 0.000 (0.076) loss 0.2220 (0.1901) acc 93.7500 (95.8333) lr 3.7476e-04 eta 0:05:57
epoch [145/200] batch [1/3] time 2.298 (2.298) data 0.237 (0.237) loss 0.4436 (0.4436) acc 93.7500 (93.7500) lr 3.7476e-04 eta 0:06:23
epoch [145/200] batch [2/3] time 2.073 (2.185) data 0.000 (0.119) loss 0.4187 (0.4312) acc 93.7500 (93.7500) lr 3.7476e-04 eta 0:06:02
epoch [145/200] batch [3/3] time 2.060 (2.144) data 0.000 (0.079) loss 0.3665 (0.4096) acc 93.7500 (93.7500) lr 3.6258e-04 eta 0:05:53
epoch [146/200] batch [1/3] time 2.282 (2.282) data 0.219 (0.219) loss 0.2494 (0.2494) acc 96.8750 (96.8750) lr 3.6258e-04 eta 0:06:14
epoch [146/200] batch [2/3] time 2.062 (2.172) data 0.000 (0.109) loss 0.1831 (0.2162) acc 96.8750 (96.8750) lr 3.6258e-04 eta 0:05:54
epoch [146/200] batch [3/3] time 2.072 (2.139) data 0.000 (0.073) loss 0.4092 (0.2806) acc 93.7500 (95.8333) lr 3.5055e-04 eta 0:05:46
epoch [147/200] batch [1/3] time 2.303 (2.303) data 0.235 (0.235) loss 0.2114 (0.2114) acc 93.7500 (93.7500) lr 3.5055e-04 eta 0:06:10
epoch [147/200] batch [2/3] time 2.039 (2.171) data 0.000 (0.118) loss 0.0977 (0.1545) acc 93.7500 (93.7500) lr 3.5055e-04 eta 0:05:47
epoch [147/200] batch [3/3] time 2.065 (2.136) data 0.000 (0.079) loss 0.1990 (0.1694) acc 93.7500 (93.7500) lr 3.3869e-04 eta 0:05:39
epoch [148/200] batch [1/3] time 2.294 (2.294) data 0.217 (0.217) loss 0.5024 (0.5024) acc 90.6250 (90.6250) lr 3.3869e-04 eta 0:06:02
epoch [148/200] batch [2/3] time 2.044 (2.169) data 0.000 (0.109) loss 0.1541 (0.3282) acc 96.8750 (93.7500) lr 3.3869e-04 eta 0:05:40
epoch [148/200] batch [3/3] time 2.063 (2.134) data 0.000 (0.072) loss 0.2010 (0.2858) acc 93.7500 (93.7500) lr 3.2699e-04 eta 0:05:32
epoch [149/200] batch [1/3] time 2.298 (2.298) data 0.229 (0.229) loss 0.2208 (0.2208) acc 93.7500 (93.7500) lr 3.2699e-04 eta 0:05:56
epoch [149/200] batch [2/3] time 2.055 (2.176) data 0.000 (0.115) loss 0.1879 (0.2043) acc 96.8750 (95.3125) lr 3.2699e-04 eta 0:05:35
epoch [149/200] batch [3/3] time 2.014 (2.122) data 0.000 (0.076) loss 0.0558 (0.1548) acc 100.0000 (96.8750) lr 3.1545e-04 eta 0:05:24
epoch [150/200] batch [1/3] time 2.282 (2.282) data 0.217 (0.217) loss 0.1930 (0.1930) acc 96.8750 (96.8750) lr 3.1545e-04 eta 0:05:46
epoch [150/200] batch [2/3] time 2.071 (2.176) data 0.000 (0.109) loss 0.4790 (0.3360) acc 87.5000 (92.1875) lr 3.1545e-04 eta 0:05:28
epoch [150/200] batch [3/3] time 2.064 (2.139) data 0.000 (0.073) loss 0.2231 (0.2984) acc 96.8750 (93.7500) lr 3.0409e-04 eta 0:05:20
epoch [151/200] batch [1/3] time 2.277 (2.277) data 0.217 (0.217) loss 0.1210 (0.1210) acc 100.0000 (100.0000) lr 3.0409e-04 eta 0:05:39
epoch [151/200] batch [2/3] time 2.054 (2.166) data 0.000 (0.109) loss 0.1729 (0.1469) acc 96.8750 (98.4375) lr 3.0409e-04 eta 0:05:20
epoch [151/200] batch [3/3] time 2.074 (2.135) data 0.000 (0.072) loss 0.3945 (0.2295) acc 93.7500 (96.8750) lr 2.9289e-04 eta 0:05:13
epoch [152/200] batch [1/3] time 2.291 (2.291) data 0.230 (0.230) loss 0.6177 (0.6177) acc 81.2500 (81.2500) lr 2.9289e-04 eta 0:05:34
epoch [152/200] batch [2/3] time 2.034 (2.163) data 0.000 (0.115) loss 0.0379 (0.3278) acc 100.0000 (90.6250) lr 2.9289e-04 eta 0:05:13
epoch [152/200] batch [3/3] time 2.061 (2.129) data 0.000 (0.077) loss 0.2189 (0.2915) acc 96.8750 (92.7083) lr 2.8187e-04 eta 0:05:06
epoch [153/200] batch [1/3] time 2.299 (2.299) data 0.238 (0.238) loss 0.1357 (0.1357) acc 96.8750 (96.8750) lr 2.8187e-04 eta 0:05:28
epoch [153/200] batch [2/3] time 2.057 (2.178) data 0.000 (0.119) loss 0.1011 (0.1184) acc 100.0000 (98.4375) lr 2.8187e-04 eta 0:05:09
epoch [153/200] batch [3/3] time 2.056 (2.137) data 0.000 (0.079) loss 0.2673 (0.1681) acc 93.7500 (96.8750) lr 2.7103e-04 eta 0:05:01
epoch [154/200] batch [1/3] time 2.284 (2.284) data 0.218 (0.218) loss 0.3533 (0.3533) acc 90.6250 (90.6250) lr 2.7103e-04 eta 0:05:19
epoch [154/200] batch [2/3] time 2.044 (2.164) data 0.000 (0.109) loss 0.1049 (0.2291) acc 100.0000 (95.3125) lr 2.7103e-04 eta 0:05:00
epoch [154/200] batch [3/3] time 2.056 (2.128) data 0.000 (0.073) loss 0.1598 (0.2060) acc 96.8750 (95.8333) lr 2.6037e-04 eta 0:04:53
epoch [155/200] batch [1/3] time 2.277 (2.277) data 0.219 (0.219) loss 0.2271 (0.2271) acc 96.8750 (96.8750) lr 2.6037e-04 eta 0:05:11
epoch [155/200] batch [2/3] time 2.046 (2.161) data 0.000 (0.109) loss 0.0812 (0.1541) acc 100.0000 (98.4375) lr 2.6037e-04 eta 0:04:53
epoch [155/200] batch [3/3] time 2.083 (2.135) data 0.000 (0.073) loss 0.5186 (0.2756) acc 81.2500 (92.7083) lr 2.4989e-04 eta 0:04:48
epoch [156/200] batch [1/3] time 2.294 (2.294) data 0.234 (0.234) loss 0.1636 (0.1636) acc 96.8750 (96.8750) lr 2.4989e-04 eta 0:05:07
epoch [156/200] batch [2/3] time 2.064 (2.179) data 0.000 (0.117) loss 0.1898 (0.1767) acc 96.8750 (96.8750) lr 2.4989e-04 eta 0:04:49
epoch [156/200] batch [3/3] time 2.059 (2.139) data 0.000 (0.078) loss 0.1863 (0.1799) acc 96.8750 (96.8750) lr 2.3959e-04 eta 0:04:42
epoch [157/200] batch [1/3] time 2.295 (2.295) data 0.229 (0.229) loss 0.2076 (0.2076) acc 100.0000 (100.0000) lr 2.3959e-04 eta 0:05:00
epoch [157/200] batch [2/3] time 2.068 (2.182) data 0.000 (0.114) loss 0.3384 (0.2730) acc 87.5000 (93.7500) lr 2.3959e-04 eta 0:04:43
epoch [157/200] batch [3/3] time 2.049 (2.137) data 0.000 (0.076) loss 0.1809 (0.2423) acc 96.8750 (94.7917) lr 2.2949e-04 eta 0:04:35
epoch [158/200] batch [1/3] time 2.283 (2.283) data 0.236 (0.236) loss 0.1525 (0.1525) acc 96.8750 (96.8750) lr 2.2949e-04 eta 0:04:52
epoch [158/200] batch [2/3] time 2.072 (2.177) data 0.000 (0.118) loss 0.3862 (0.2693) acc 87.5000 (92.1875) lr 2.2949e-04 eta 0:04:36
epoch [158/200] batch [3/3] time 2.080 (2.145) data 0.000 (0.079) loss 0.3672 (0.3020) acc 93.7500 (92.7083) lr 2.1957e-04 eta 0:04:30
epoch [159/200] batch [1/3] time 2.279 (2.279) data 0.232 (0.232) loss 0.1689 (0.1689) acc 96.8750 (96.8750) lr 2.1957e-04 eta 0:04:44
epoch [159/200] batch [2/3] time 2.056 (2.167) data 0.000 (0.116) loss 0.1287 (0.1488) acc 96.8750 (96.8750) lr 2.1957e-04 eta 0:04:28
epoch [159/200] batch [3/3] time 2.065 (2.133) data 0.000 (0.077) loss 0.2070 (0.1682) acc 100.0000 (97.9167) lr 2.0984e-04 eta 0:04:22
epoch [160/200] batch [1/3] time 2.300 (2.300) data 0.240 (0.240) loss 0.2292 (0.2292) acc 93.7500 (93.7500) lr 2.0984e-04 eta 0:04:40
epoch [160/200] batch [2/3] time 2.032 (2.166) data 0.000 (0.120) loss 0.0814 (0.1553) acc 100.0000 (96.8750) lr 2.0984e-04 eta 0:04:22
epoch [160/200] batch [3/3] time 2.056 (2.129) data 0.000 (0.080) loss 0.2964 (0.2024) acc 87.5000 (93.7500) lr 2.0032e-04 eta 0:04:15
epoch [161/200] batch [1/3] time 2.287 (2.287) data 0.230 (0.230) loss 0.2191 (0.2191) acc 96.8750 (96.8750) lr 2.0032e-04 eta 0:04:32
epoch [161/200] batch [2/3] time 2.025 (2.156) data 0.000 (0.115) loss 0.0505 (0.1348) acc 100.0000 (98.4375) lr 2.0032e-04 eta 0:04:14
epoch [161/200] batch [3/3] time 2.072 (2.128) data 0.000 (0.077) loss 0.2321 (0.1672) acc 96.8750 (97.9167) lr 1.9098e-04 eta 0:04:08
epoch [162/200] batch [1/3] time 2.261 (2.261) data 0.220 (0.220) loss 0.1229 (0.1229) acc 93.7500 (93.7500) lr 1.9098e-04 eta 0:04:22
epoch [162/200] batch [2/3] time 2.030 (2.145) data 0.000 (0.110) loss 0.1559 (0.1394) acc 96.8750 (95.3125) lr 1.9098e-04 eta 0:04:06
epoch [162/200] batch [3/3] time 2.044 (2.111) data 0.000 (0.073) loss 0.0621 (0.1136) acc 100.0000 (96.8750) lr 1.8185e-04 eta 0:04:00
epoch [163/200] batch [1/3] time 2.291 (2.291) data 0.235 (0.235) loss 0.2100 (0.2100) acc 93.7500 (93.7500) lr 1.8185e-04 eta 0:04:18
epoch [163/200] batch [2/3] time 2.072 (2.181) data 0.000 (0.117) loss 0.2805 (0.2452) acc 90.6250 (92.1875) lr 1.8185e-04 eta 0:04:04
epoch [163/200] batch [3/3] time 2.051 (2.138) data 0.000 (0.078) loss 0.3428 (0.2778) acc 90.6250 (91.6667) lr 1.7292e-04 eta 0:03:57
epoch [164/200] batch [1/3] time 2.286 (2.286) data 0.227 (0.227) loss 0.1111 (0.1111) acc 100.0000 (100.0000) lr 1.7292e-04 eta 0:04:11
epoch [164/200] batch [2/3] time 2.063 (2.174) data 0.000 (0.114) loss 0.4673 (0.2892) acc 90.6250 (95.3125) lr 1.7292e-04 eta 0:03:56
epoch [164/200] batch [3/3] time 2.069 (2.139) data 0.000 (0.076) loss 0.3843 (0.3209) acc 87.5000 (92.7083) lr 1.6419e-04 eta 0:03:51
epoch [165/200] batch [1/3] time 2.294 (2.294) data 0.222 (0.222) loss 0.3857 (0.3857) acc 93.7500 (93.7500) lr 1.6419e-04 eta 0:04:05
epoch [165/200] batch [2/3] time 2.072 (2.183) data 0.000 (0.111) loss 0.4448 (0.4153) acc 84.3750 (89.0625) lr 1.6419e-04 eta 0:03:51
epoch [165/200] batch [3/3] time 2.066 (2.144) data 0.000 (0.074) loss 0.3306 (0.3870) acc 90.6250 (89.5833) lr 1.5567e-04 eta 0:03:45
epoch [166/200] batch [1/3] time 2.303 (2.303) data 0.229 (0.229) loss 0.5513 (0.5513) acc 87.5000 (87.5000) lr 1.5567e-04 eta 0:03:59
epoch [166/200] batch [2/3] time 2.063 (2.183) data 0.000 (0.115) loss 0.1577 (0.3545) acc 96.8750 (92.1875) lr 1.5567e-04 eta 0:03:44
epoch [166/200] batch [3/3] time 2.054 (2.140) data 0.000 (0.077) loss 0.1414 (0.2834) acc 96.8750 (93.7500) lr 1.4736e-04 eta 0:03:38
epoch [167/200] batch [1/3] time 2.284 (2.284) data 0.233 (0.233) loss 0.1940 (0.1940) acc 96.8750 (96.8750) lr 1.4736e-04 eta 0:03:50
epoch [167/200] batch [2/3] time 2.050 (2.167) data 0.000 (0.117) loss 0.1945 (0.1942) acc 96.8750 (96.8750) lr 1.4736e-04 eta 0:03:36
epoch [167/200] batch [3/3] time 2.074 (2.136) data 0.000 (0.078) loss 0.4468 (0.2784) acc 87.5000 (93.7500) lr 1.3926e-04 eta 0:03:31
epoch [168/200] batch [1/3] time 2.294 (2.294) data 0.224 (0.224) loss 0.3521 (0.3521) acc 93.7500 (93.7500) lr 1.3926e-04 eta 0:03:44
epoch [168/200] batch [2/3] time 2.040 (2.167) data 0.000 (0.112) loss 0.1238 (0.2379) acc 100.0000 (96.8750) lr 1.3926e-04 eta 0:03:30
epoch [168/200] batch [3/3] time 2.048 (2.127) data 0.000 (0.075) loss 0.0931 (0.1897) acc 100.0000 (97.9167) lr 1.3137e-04 eta 0:03:24
epoch [169/200] batch [1/3] time 2.284 (2.284) data 0.230 (0.230) loss 0.2267 (0.2267) acc 96.8750 (96.8750) lr 1.3137e-04 eta 0:03:37
epoch [169/200] batch [2/3] time 2.063 (2.174) data 0.000 (0.115) loss 0.2639 (0.2453) acc 93.7500 (95.3125) lr 1.3137e-04 eta 0:03:24
epoch [169/200] batch [3/3] time 2.045 (2.131) data 0.000 (0.077) loss 0.1379 (0.2095) acc 96.8750 (95.8333) lr 1.2369e-04 eta 0:03:18
epoch [170/200] batch [1/3] time 2.292 (2.292) data 0.227 (0.227) loss 0.1965 (0.1965) acc 93.7500 (93.7500) lr 1.2369e-04 eta 0:03:30
epoch [170/200] batch [2/3] time 2.012 (2.152) data 0.000 (0.114) loss 0.1044 (0.1505) acc 96.8750 (95.3125) lr 1.2369e-04 eta 0:03:15
epoch [170/200] batch [3/3] time 2.066 (2.124) data 0.000 (0.076) loss 0.3516 (0.2175) acc 93.7500 (94.7917) lr 1.1623e-04 eta 0:03:11
epoch [171/200] batch [1/3] time 2.273 (2.273) data 0.228 (0.228) loss 0.0841 (0.0841) acc 100.0000 (100.0000) lr 1.1623e-04 eta 0:03:22
epoch [171/200] batch [2/3] time 2.066 (2.170) data 0.000 (0.114) loss 0.3062 (0.1951) acc 90.6250 (95.3125) lr 1.1623e-04 eta 0:03:10
epoch [171/200] batch [3/3] time 2.058 (2.132) data 0.000 (0.076) loss 0.2113 (0.2005) acc 93.7500 (94.7917) lr 1.0899e-04 eta 0:03:05
epoch [172/200] batch [1/3] time 2.248 (2.248) data 0.218 (0.218) loss 0.0604 (0.0604) acc 100.0000 (100.0000) lr 1.0899e-04 eta 0:03:13
epoch [172/200] batch [2/3] time 2.059 (2.154) data 0.000 (0.109) loss 0.1674 (0.1139) acc 100.0000 (100.0000) lr 1.0899e-04 eta 0:03:03
epoch [172/200] batch [3/3] time 2.058 (2.122) data 0.000 (0.073) loss 0.1831 (0.1370) acc 96.8750 (98.9583) lr 1.0197e-04 eta 0:02:58
epoch [173/200] batch [1/3] time 2.243 (2.243) data 0.215 (0.215) loss 0.0504 (0.0504) acc 100.0000 (100.0000) lr 1.0197e-04 eta 0:03:06
epoch [173/200] batch [2/3] time 2.055 (2.149) data 0.000 (0.107) loss 0.2012 (0.1258) acc 96.8750 (98.4375) lr 1.0197e-04 eta 0:02:56
epoch [173/200] batch [3/3] time 2.057 (2.118) data 0.000 (0.072) loss 0.2737 (0.1751) acc 90.6250 (95.8333) lr 9.5173e-05 eta 0:02:51
epoch [174/200] batch [1/3] time 2.279 (2.279) data 0.225 (0.225) loss 0.0977 (0.0977) acc 100.0000 (100.0000) lr 9.5173e-05 eta 0:03:02
epoch [174/200] batch [2/3] time 2.048 (2.164) data 0.000 (0.113) loss 0.1169 (0.1073) acc 96.8750 (98.4375) lr 9.5173e-05 eta 0:02:50
epoch [174/200] batch [3/3] time 2.047 (2.125) data 0.000 (0.075) loss 0.1436 (0.1194) acc 96.8750 (97.9167) lr 8.8597e-05 eta 0:02:45
epoch [175/200] batch [1/3] time 2.264 (2.264) data 0.219 (0.219) loss 0.1587 (0.1587) acc 93.7500 (93.7500) lr 8.8597e-05 eta 0:02:54
epoch [175/200] batch [2/3] time 2.037 (2.151) data 0.000 (0.109) loss 0.2202 (0.1895) acc 90.6250 (92.1875) lr 8.8597e-05 eta 0:02:43
epoch [175/200] batch [3/3] time 2.068 (2.123) data 0.000 (0.073) loss 0.2069 (0.1953) acc 93.7500 (92.7083) lr 8.2245e-05 eta 0:02:39
epoch [176/200] batch [1/3] time 2.277 (2.277) data 0.220 (0.220) loss 0.2155 (0.2155) acc 93.7500 (93.7500) lr 8.2245e-05 eta 0:02:48
epoch [176/200] batch [2/3] time 2.052 (2.165) data 0.000 (0.110) loss 0.1859 (0.2007) acc 93.7500 (93.7500) lr 8.2245e-05 eta 0:02:38
epoch [176/200] batch [3/3] time 2.046 (2.125) data 0.000 (0.074) loss 0.3835 (0.2616) acc 96.8750 (94.7917) lr 7.6120e-05 eta 0:02:33
epoch [177/200] batch [1/3] time 2.281 (2.281) data 0.226 (0.226) loss 0.2101 (0.2101) acc 93.7500 (93.7500) lr 7.6120e-05 eta 0:02:41
epoch [177/200] batch [2/3] time 2.051 (2.166) data 0.000 (0.113) loss 0.3123 (0.2612) acc 90.6250 (92.1875) lr 7.6120e-05 eta 0:02:31
epoch [177/200] batch [3/3] time 2.017 (2.116) data 0.000 (0.075) loss 0.0637 (0.1954) acc 100.0000 (94.7917) lr 7.0224e-05 eta 0:02:26
epoch [178/200] batch [1/3] time 2.281 (2.281) data 0.237 (0.237) loss 0.1370 (0.1370) acc 96.8750 (96.8750) lr 7.0224e-05 eta 0:02:35
epoch [178/200] batch [2/3] time 2.034 (2.158) data 0.000 (0.119) loss 0.0773 (0.1071) acc 100.0000 (98.4375) lr 7.0224e-05 eta 0:02:24
epoch [178/200] batch [3/3] time 2.076 (2.130) data 0.000 (0.079) loss 0.4434 (0.2192) acc 87.5000 (94.7917) lr 6.4556e-05 eta 0:02:20
epoch [179/200] batch [1/3] time 2.286 (2.286) data 0.228 (0.228) loss 0.1570 (0.1570) acc 96.8750 (96.8750) lr 6.4556e-05 eta 0:02:28
epoch [179/200] batch [2/3] time 2.058 (2.172) data 0.000 (0.114) loss 0.2834 (0.2202) acc 93.7500 (95.3125) lr 6.4556e-05 eta 0:02:19
epoch [179/200] batch [3/3] time 2.059 (2.134) data 0.000 (0.076) loss 0.1627 (0.2010) acc 100.0000 (96.8750) lr 5.9119e-05 eta 0:02:14
epoch [180/200] batch [1/3] time 2.265 (2.265) data 0.218 (0.218) loss 0.1160 (0.1160) acc 96.8750 (96.8750) lr 5.9119e-05 eta 0:02:20
epoch [180/200] batch [2/3] time 2.052 (2.159) data 0.000 (0.109) loss 0.2078 (0.1619) acc 96.8750 (96.8750) lr 5.9119e-05 eta 0:02:11
epoch [180/200] batch [3/3] time 2.058 (2.125) data 0.000 (0.073) loss 0.1965 (0.1734) acc 93.7500 (95.8333) lr 5.3915e-05 eta 0:02:07
epoch [181/200] batch [1/3] time 2.277 (2.277) data 0.235 (0.235) loss 0.1340 (0.1340) acc 96.8750 (96.8750) lr 5.3915e-05 eta 0:02:14
epoch [181/200] batch [2/3] time 2.060 (2.168) data 0.000 (0.117) loss 0.1891 (0.1616) acc 96.8750 (96.8750) lr 5.3915e-05 eta 0:02:05
epoch [181/200] batch [3/3] time 2.051 (2.129) data 0.000 (0.078) loss 0.1379 (0.1537) acc 96.8750 (96.8750) lr 4.8943e-05 eta 0:02:01
epoch [182/200] batch [1/3] time 2.276 (2.276) data 0.231 (0.231) loss 0.3062 (0.3062) acc 93.7500 (93.7500) lr 4.8943e-05 eta 0:02:07
epoch [182/200] batch [2/3] time 2.026 (2.151) data 0.000 (0.116) loss 0.0966 (0.2014) acc 96.8750 (95.3125) lr 4.8943e-05 eta 0:01:58
epoch [182/200] batch [3/3] time 2.061 (2.121) data 0.000 (0.077) loss 0.1866 (0.1965) acc 93.7500 (94.7917) lr 4.4207e-05 eta 0:01:54
epoch [183/200] batch [1/3] time 2.288 (2.288) data 0.219 (0.219) loss 0.6426 (0.6426) acc 87.5000 (87.5000) lr 4.4207e-05 eta 0:02:01
epoch [183/200] batch [2/3] time 2.054 (2.171) data 0.000 (0.109) loss 0.2556 (0.4491) acc 96.8750 (92.1875) lr 4.4207e-05 eta 0:01:52
epoch [183/200] batch [3/3] time 2.061 (2.134) data 0.000 (0.073) loss 0.2494 (0.3825) acc 90.6250 (91.6667) lr 3.9706e-05 eta 0:01:48
epoch [184/200] batch [1/3] time 2.284 (2.284) data 0.229 (0.229) loss 0.3579 (0.3579) acc 93.7500 (93.7500) lr 3.9706e-05 eta 0:01:54
epoch [184/200] batch [2/3] time 2.067 (2.176) data 0.000 (0.115) loss 0.2551 (0.3065) acc 96.8750 (95.3125) lr 3.9706e-05 eta 0:01:46
epoch [184/200] batch [3/3] time 2.054 (2.135) data 0.000 (0.076) loss 0.0810 (0.2313) acc 100.0000 (96.8750) lr 3.5443e-05 eta 0:01:42
epoch [185/200] batch [1/3] time 2.296 (2.296) data 0.233 (0.233) loss 0.3586 (0.3586) acc 87.5000 (87.5000) lr 3.5443e-05 eta 0:01:47
epoch [185/200] batch [2/3] time 2.021 (2.159) data 0.000 (0.116) loss 0.0411 (0.1999) acc 100.0000 (93.7500) lr 3.5443e-05 eta 0:01:39
epoch [185/200] batch [3/3] time 2.034 (2.117) data 0.000 (0.078) loss 0.0861 (0.1619) acc 100.0000 (95.8333) lr 3.1417e-05 eta 0:01:35
epoch [186/200] batch [1/3] time 2.286 (2.286) data 0.231 (0.231) loss 0.3716 (0.3716) acc 93.7500 (93.7500) lr 3.1417e-05 eta 0:01:40
epoch [186/200] batch [2/3] time 2.060 (2.173) data 0.000 (0.115) loss 0.2307 (0.3011) acc 93.7500 (93.7500) lr 3.1417e-05 eta 0:01:33
epoch [186/200] batch [3/3] time 2.070 (2.138) data 0.000 (0.077) loss 0.4023 (0.3349) acc 93.7500 (93.7500) lr 2.7630e-05 eta 0:01:29
epoch [187/200] batch [1/3] time 2.245 (2.245) data 0.223 (0.223) loss 0.0369 (0.0369) acc 100.0000 (100.0000) lr 2.7630e-05 eta 0:01:32
epoch [187/200] batch [2/3] time 2.011 (2.128) data 0.000 (0.112) loss 0.0295 (0.0332) acc 100.0000 (100.0000) lr 2.7630e-05 eta 0:01:25
epoch [187/200] batch [3/3] time 2.054 (2.103) data 0.000 (0.075) loss 0.3074 (0.1246) acc 93.7500 (97.9167) lr 2.4083e-05 eta 0:01:22
epoch [188/200] batch [1/3] time 2.278 (2.278) data 0.217 (0.217) loss 0.3164 (0.3164) acc 90.6250 (90.6250) lr 2.4083e-05 eta 0:01:26
epoch [188/200] batch [2/3] time 2.062 (2.170) data 0.000 (0.109) loss 0.1805 (0.2485) acc 96.8750 (93.7500) lr 2.4083e-05 eta 0:01:20
epoch [188/200] batch [3/3] time 2.057 (2.132) data 0.000 (0.072) loss 0.3179 (0.2716) acc 87.5000 (91.6667) lr 2.0777e-05 eta 0:01:16
epoch [189/200] batch [1/3] time 2.284 (2.284) data 0.225 (0.225) loss 0.2169 (0.2169) acc 96.8750 (96.8750) lr 2.0777e-05 eta 0:01:19
epoch [189/200] batch [2/3] time 2.049 (2.166) data 0.000 (0.113) loss 0.1295 (0.1732) acc 100.0000 (98.4375) lr 2.0777e-05 eta 0:01:13
epoch [189/200] batch [3/3] time 2.048 (2.127) data 0.000 (0.075) loss 0.1187 (0.1550) acc 100.0000 (98.9583) lr 1.7713e-05 eta 0:01:10
epoch [190/200] batch [1/3] time 2.264 (2.264) data 0.224 (0.224) loss 0.1622 (0.1622) acc 96.8750 (96.8750) lr 1.7713e-05 eta 0:01:12
epoch [190/200] batch [2/3] time 2.069 (2.166) data 0.000 (0.112) loss 0.2864 (0.2243) acc 93.7500 (95.3125) lr 1.7713e-05 eta 0:01:07
epoch [190/200] batch [3/3] time 2.077 (2.137) data 0.000 (0.075) loss 0.2500 (0.2329) acc 96.8750 (95.8333) lr 1.4891e-05 eta 0:01:04
epoch [191/200] batch [1/3] time 2.284 (2.284) data 0.237 (0.237) loss 0.2290 (0.2290) acc 93.7500 (93.7500) lr 1.4891e-05 eta 0:01:06
epoch [191/200] batch [2/3] time 2.059 (2.171) data 0.000 (0.119) loss 0.5225 (0.3757) acc 87.5000 (90.6250) lr 1.4891e-05 eta 0:01:00
epoch [191/200] batch [3/3] time 2.054 (2.132) data 0.000 (0.079) loss 0.3481 (0.3665) acc 87.5000 (89.5833) lr 1.2312e-05 eta 0:00:57
epoch [192/200] batch [1/3] time 2.267 (2.267) data 0.223 (0.223) loss 0.1605 (0.1605) acc 96.8750 (96.8750) lr 1.2312e-05 eta 0:00:58
epoch [192/200] batch [2/3] time 2.074 (2.171) data 0.000 (0.112) loss 0.3669 (0.2637) acc 93.7500 (95.3125) lr 1.2312e-05 eta 0:00:54
epoch [192/200] batch [3/3] time 2.046 (2.129) data 0.000 (0.075) loss 0.3020 (0.2765) acc 93.7500 (94.7917) lr 9.9763e-06 eta 0:00:51
epoch [193/200] batch [1/3] time 2.249 (2.249) data 0.219 (0.219) loss 0.0393 (0.0393) acc 100.0000 (100.0000) lr 9.9763e-06 eta 0:00:51
epoch [193/200] batch [2/3] time 2.048 (2.149) data 0.000 (0.110) loss 0.1247 (0.0820) acc 100.0000 (100.0000) lr 9.9763e-06 eta 0:00:47
epoch [193/200] batch [3/3] time 2.063 (2.120) data 0.000 (0.073) loss 0.2646 (0.1429) acc 96.8750 (98.9583) lr 7.8853e-06 eta 0:00:44
epoch [194/200] batch [1/3] time 2.260 (2.260) data 0.221 (0.221) loss 0.0772 (0.0772) acc 100.0000 (100.0000) lr 7.8853e-06 eta 0:00:45
epoch [194/200] batch [2/3] time 2.060 (2.160) data 0.000 (0.110) loss 0.1989 (0.1380) acc 100.0000 (100.0000) lr 7.8853e-06 eta 0:00:41
epoch [194/200] batch [3/3] time 2.050 (2.123) data 0.000 (0.074) loss 0.1423 (0.1395) acc 96.8750 (98.9583) lr 6.0390e-06 eta 0:00:38
epoch [195/200] batch [1/3] time 2.301 (2.301) data 0.233 (0.233) loss 0.4570 (0.4570) acc 90.6250 (90.6250) lr 6.0390e-06 eta 0:00:39
epoch [195/200] batch [2/3] time 2.049 (2.175) data 0.000 (0.117) loss 0.1042 (0.2806) acc 96.8750 (93.7500) lr 6.0390e-06 eta 0:00:34
epoch [195/200] batch [3/3] time 2.058 (2.136) data 0.000 (0.078) loss 0.1671 (0.2428) acc 96.8750 (94.7917) lr 4.4380e-06 eta 0:00:32
epoch [196/200] batch [1/3] time 2.249 (2.249) data 0.234 (0.234) loss 0.0263 (0.0263) acc 100.0000 (100.0000) lr 4.4380e-06 eta 0:00:31
epoch [196/200] batch [2/3] time 2.046 (2.148) data 0.000 (0.117) loss 0.0899 (0.0581) acc 100.0000 (100.0000) lr 4.4380e-06 eta 0:00:27
epoch [196/200] batch [3/3] time 2.048 (2.114) data 0.000 (0.078) loss 0.1514 (0.0892) acc 96.8750 (98.9583) lr 3.0827e-06 eta 0:00:25
epoch [197/200] batch [1/3] time 2.294 (2.294) data 0.238 (0.238) loss 0.3650 (0.3650) acc 96.8750 (96.8750) lr 3.0827e-06 eta 0:00:25
epoch [197/200] batch [2/3] time 2.096 (2.195) data 0.000 (0.119) loss 0.1449 (0.2549) acc 96.8750 (96.8750) lr 3.0827e-06 eta 0:00:21
epoch [197/200] batch [3/3] time 2.141 (2.177) data 0.000 (0.079) loss 0.5098 (0.3399) acc 87.5000 (93.7500) lr 1.9733e-06 eta 0:00:19
epoch [198/200] batch [1/3] time 2.300 (2.300) data 0.229 (0.229) loss 0.2434 (0.2434) acc 96.8750 (96.8750) lr 1.9733e-06 eta 0:00:18
epoch [198/200] batch [2/3] time 2.201 (2.250) data 0.000 (0.114) loss 0.1842 (0.2138) acc 96.8750 (96.8750) lr 1.9733e-06 eta 0:00:15
epoch [198/200] batch [3/3] time 2.089 (2.197) data 0.000 (0.076) loss 0.0784 (0.1687) acc 100.0000 (97.9167) lr 1.1101e-06 eta 0:00:13
epoch [199/200] batch [1/3] time 2.298 (2.298) data 0.223 (0.223) loss 0.1897 (0.1897) acc 100.0000 (100.0000) lr 1.1101e-06 eta 0:00:11
epoch [199/200] batch [2/3] time 2.091 (2.194) data 0.000 (0.112) loss 0.3132 (0.2515) acc 96.8750 (98.4375) lr 1.1101e-06 eta 0:00:08
epoch [199/200] batch [3/3] time 2.087 (2.158) data 0.000 (0.074) loss 0.3069 (0.2699) acc 93.7500 (96.8750) lr 4.9344e-07 eta 0:00:06
epoch [200/200] batch [1/3] time 2.283 (2.283) data 0.222 (0.222) loss 0.2347 (0.2347) acc 96.8750 (96.8750) lr 4.9344e-07 eta 0:00:04
epoch [200/200] batch [2/3] time 2.074 (2.178) data 0.000 (0.111) loss 0.2307 (0.2327) acc 96.8750 (96.8750) lr 4.9344e-07 eta 0:00:02
epoch [200/200] batch [3/3] time 2.072 (2.143) data 0.000 (0.074) loss 0.1666 (0.2107) acc 96.8750 (96.8750) lr 1.2337e-07 eta 0:00:00
Checkpoint saved to output/Caltech/1/prompt_learner/model.pth.tar-200
Finish training
Deploy the last-epoch model
Evaluate on the *test* set
=> result
* total: 2,465
* correct: 1,934
* accuracy: 78.5%
* error: 21.5%
* macro_f1: 68.8%
Elapsed: 0:22:23
args2: backbone=, config_file=configs/trainers/CoOp/rn101.yaml, dataset_config_file=configs/datasets/caltech101.yaml, eval_only=False, head=, load_epoch=None, model_dir=, no_train=False,  opts: ['TRAINER.COOP.N_CTX', '16', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '1'], output_dir=output/Caltech/1/2, resume=, root=/home/brandnerkasper/Uni/MP/MP_CustomCoOp/data, seed=2, source_domains=None, target_domains=None, trainer=CoOp, transforms=None
Setting fixed seed: 2
***************
** Arguments **
***************
config_file: configs/trainers/CoOp/rn101.yaml
csc: False
ctp: end
dataset_config_file: configs/datasets/caltech101.yaml
n_ctx: 16
opts: ['TRAINER.COOP.N_CTX', '16', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '1']
output_dir: output/Caltech/1/2
root: /home/brandnerkasper/Uni/MP/MP_CustomCoOp/data
seed: 2
shots: 1
trainer: CoOp
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 4
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: Caltech101
  NUM_LABELED: -1
  NUM_SHOTS: 1
  ROOT: /home/brandnerkasper/Uni/MP/MP_CustomCoOp/data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN101
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 200
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/Caltech/1/2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: 
    N_CTX: 16
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: 
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: CoOp
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 2.0.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 22.04.3 LTS (x86_64)
GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.35

Python version: 3.10.12 (main, Jul  5 2023, 18:54:27) [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-6.2.0-32-generic-x86_64-with-glibc2.35
Is CUDA available: True
CUDA runtime version: 11.5.119
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA GeForce GTX 970
Nvidia driver version: 525.125.06
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Architecture:                       x86_64
CPU op-mode(s):                     32-bit, 64-bit
Address sizes:                      39 bits physical, 48 bits virtual
Byte Order:                         Little Endian
CPU(s):                             4
On-line CPU(s) list:                0-3
Vendor ID:                          GenuineIntel
Model name:                         Intel(R) Xeon(R) CPU E3-1225 v3 @ 3.20GHz
CPU family:                         6
Model:                              60
Thread(s) per core:                 1
Core(s) per socket:                 4
Socket(s):                          1
Stepping:                           3
CPU max MHz:                        3600,0000
CPU min MHz:                        800,0000
BogoMIPS:                           6398.04
Flags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm cpuid_fault invpcid_single pti ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid xsaveopt dtherm ida arat pln pts md_clear flush_l1d
Virtualization:                     VT-x
L1d cache:                          128 KiB (4 instances)
L1i cache:                          128 KiB (4 instances)
L2 cache:                           1 MiB (4 instances)
L3 cache:                           8 MiB (1 instance)
NUMA node(s):                       1
NUMA node0 CPU(s):                  0-3
Vulnerability Gather data sampling: Not affected
Vulnerability Itlb multihit:        KVM: Mitigation: VMX disabled
Vulnerability L1tf:                 Mitigation; PTE Inversion; VMX conditional cache flushes, SMT disabled
Vulnerability Mds:                  Mitigation; Clear CPU buffers; SMT disabled
Vulnerability Meltdown:             Mitigation; PTI
Vulnerability Mmio stale data:      Unknown: No mitigations
Vulnerability Retbleed:             Not affected
Vulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl
Vulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization
Vulnerability Spectre v2:           Mitigation; Retpolines, IBPB conditional, IBRS_FW, STIBP disabled, RSB filling, PBRSB-eIBRS Not affected
Vulnerability Srbds:                Mitigation; Microcode
Vulnerability Tsx async abort:      Not affected

Versions of relevant libraries:
[pip3] numpy==1.25.2
[pip3] open-clip-torch==2.20.0
[pip3] torch==2.0.1
[pip3] torchaudio==2.0.2
[pip3] torchvision==0.15.2
[conda] blas                      1.0                         mkl  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] mkl                       2023.1.0         h213fc3f_46343  
[conda] mkl-service               2.4.0           py310h5eee18b_1  
[conda] mkl_fft                   1.3.6           py310h1128e8f_1  
[conda] mkl_random                1.2.2           py310h1128e8f_1  
[conda] numpy                     1.25.2          py310h5f9d8c6_0  
[conda] numpy-base                1.25.2          py310hb5e798b_0  
[conda] open-clip-torch           2.20.0                   pypi_0    pypi
[conda] pytorch                   2.0.1           py3.10_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchaudio                2.0.2               py310_cu117    pytorch
[conda] torchtriton               2.0.0                     py310    pytorch
[conda] torchvision               0.15.2              py310_cu117    pytorch
        Pillow (9.4.0)

Loading trainer: CoOp
Loading dataset: Caltech101
Reading split from /home/brandnerkasper/Uni/MP/MP_CustomCoOp/data/caltech-101/split_zhou_Caltech101.json
Creating a 1-shot dataset
Creating a 1-shot dataset
Saving preprocessed few-shot data to /home/brandnerkasper/Uni/MP/MP_CustomCoOp/data/caltech-101/split_fewshot/shot_1-seed_2.pkl
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ----------
Dataset    Caltech101
# classes  100
# train_x  100
# val      100
# test     2,465
---------  ----------
Loading CLIP (backbone: RN101)
CLIP(
  (visual): ModifiedResNet(
    (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (act1): ReLU(inplace=True)
    (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (act2): ReLU(inplace=True)
    (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (act3): ReLU(inplace=True)
    (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (layer1): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
        (downsample): Sequential(
          (-1): AvgPool2d(kernel_size=1, stride=1, padding=0)
          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
      )
    )
    (layer2): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
        (downsample): Sequential(
          (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
      )
    )
    (layer3): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
        (downsample): Sequential(
          (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
      )
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
      )
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
      )
      (6): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
      )
      (7): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
      )
      (8): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
      )
      (9): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
      )
      (10): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
      )
      (11): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
      )
      (12): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
      )
      (13): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
      )
      (14): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
      )
      (15): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
      )
      (16): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
      )
      (17): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
      )
      (18): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
      )
      (19): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
      )
      (20): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
      )
      (21): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
      )
      (22): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
      )
    )
    (layer4): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
        (downsample): Sequential(
          (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
      )
    )
    (attnpool): AttentionPool2d(
      (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
      (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
      (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
      (c_proj): Linear(in_features=2048, out_features=512, bias=True)
    )
  )
  (transformer): Transformer(
    (resblocks): ModuleList(
      (0-11): 12 x ResidualAttentionBlock(
        (ln_1): LayerNormFp32((512,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNormFp32((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ls_2): Identity()
      )
    )
  )
  (token_embedding): Embedding(49408, 512)
  (ln_final): LayerNormFp32((512,), eps=1e-05, elementwise_affine=True)
)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
Turning off gradients in both the image and the text encoder
Loading evaluator: Classification
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/Caltech/1/2/tensorboard)
epoch [1/200] batch [1/3] time 2.418 (2.418) data 0.294 (0.294) loss 4.5781 (4.5781) acc 9.3750 (9.3750) lr 1.0000e-05 eta 0:24:08
epoch [1/200] batch [2/3] time 2.089 (2.253) data 0.000 (0.147) loss 4.6250 (4.6016) acc 0.0000 (4.6875) lr 1.0000e-05 eta 0:22:27
epoch [1/200] batch [3/3] time 2.083 (2.197) data 0.000 (0.098) loss 4.4492 (4.5508) acc 6.2500 (5.2083) lr 2.0000e-03 eta 0:21:51
epoch [2/200] batch [1/3] time 2.325 (2.325) data 0.243 (0.243) loss 4.5508 (4.5508) acc 3.1250 (3.1250) lr 2.0000e-03 eta 0:23:05
epoch [2/200] batch [2/3] time 2.088 (2.207) data 0.000 (0.122) loss 4.5859 (4.5684) acc 3.1250 (3.1250) lr 2.0000e-03 eta 0:21:53
epoch [2/200] batch [3/3] time 2.095 (2.170) data 0.000 (0.081) loss 4.5703 (4.5690) acc 9.3750 (5.2083) lr 1.9999e-03 eta 0:21:28
epoch [3/200] batch [1/3] time 2.340 (2.340) data 0.251 (0.251) loss 4.4727 (4.4727) acc 3.1250 (3.1250) lr 1.9999e-03 eta 0:23:07
epoch [3/200] batch [2/3] time 2.095 (2.217) data 0.000 (0.125) loss 4.5273 (4.5000) acc 9.3750 (6.2500) lr 1.9999e-03 eta 0:21:52
epoch [3/200] batch [3/3] time 2.091 (2.175) data 0.000 (0.084) loss 4.5195 (4.5065) acc 9.3750 (7.2917) lr 1.9995e-03 eta 0:21:25
epoch [4/200] batch [1/3] time 2.367 (2.367) data 0.253 (0.253) loss 4.3320 (4.3320) acc 9.3750 (9.3750) lr 1.9995e-03 eta 0:23:16
epoch [4/200] batch [2/3] time 2.095 (2.231) data 0.000 (0.126) loss 4.4805 (4.4062) acc 3.1250 (6.2500) lr 1.9995e-03 eta 0:21:54
epoch [4/200] batch [3/3] time 2.091 (2.184) data 0.000 (0.084) loss 4.4414 (4.4180) acc 3.1250 (5.2083) lr 1.9989e-03 eta 0:21:24
epoch [5/200] batch [1/3] time 2.336 (2.336) data 0.243 (0.243) loss 4.3633 (4.3633) acc 6.2500 (6.2500) lr 1.9989e-03 eta 0:22:51
epoch [5/200] batch [2/3] time 2.092 (2.214) data 0.000 (0.122) loss 4.2344 (4.2988) acc 12.5000 (9.3750) lr 1.9989e-03 eta 0:21:37
epoch [5/200] batch [3/3] time 2.093 (2.173) data 0.000 (0.081) loss 4.2344 (4.2773) acc 6.2500 (8.3333) lr 1.9980e-03 eta 0:21:11
epoch [6/200] batch [1/3] time 2.350 (2.350) data 0.261 (0.261) loss 4.3242 (4.3242) acc 6.2500 (6.2500) lr 1.9980e-03 eta 0:22:52
epoch [6/200] batch [2/3] time 2.100 (2.225) data 0.000 (0.130) loss 4.0938 (4.2090) acc 12.5000 (9.3750) lr 1.9980e-03 eta 0:21:37
epoch [6/200] batch [3/3] time 2.092 (2.181) data 0.000 (0.087) loss 4.4297 (4.2826) acc 6.2500 (8.3333) lr 1.9969e-03 eta 0:21:09
epoch [7/200] batch [1/3] time 2.353 (2.353) data 0.254 (0.254) loss 3.8691 (3.8691) acc 18.7500 (18.7500) lr 1.9969e-03 eta 0:22:47
epoch [7/200] batch [2/3] time 2.093 (2.223) data 0.000 (0.127) loss 3.7207 (3.7949) acc 12.5000 (15.6250) lr 1.9969e-03 eta 0:21:29
epoch [7/200] batch [3/3] time 2.091 (2.179) data 0.000 (0.085) loss 4.0039 (3.8646) acc 9.3750 (13.5417) lr 1.9956e-03 eta 0:21:01
epoch [8/200] batch [1/3] time 2.338 (2.338) data 0.245 (0.245) loss 3.5391 (3.5391) acc 15.6250 (15.6250) lr 1.9956e-03 eta 0:22:31
epoch [8/200] batch [2/3] time 2.092 (2.215) data 0.000 (0.122) loss 4.0391 (3.7891) acc 12.5000 (14.0625) lr 1.9956e-03 eta 0:21:17
epoch [8/200] batch [3/3] time 2.098 (2.176) data 0.000 (0.082) loss 3.5508 (3.7096) acc 21.8750 (16.6667) lr 1.9940e-03 eta 0:20:53
epoch [9/200] batch [1/3] time 2.336 (2.336) data 0.248 (0.248) loss 3.6289 (3.6289) acc 9.3750 (9.3750) lr 1.9940e-03 eta 0:22:23
epoch [9/200] batch [2/3] time 2.098 (2.217) data 0.000 (0.124) loss 3.1543 (3.3916) acc 21.8750 (15.6250) lr 1.9940e-03 eta 0:21:12
epoch [9/200] batch [3/3] time 2.094 (2.176) data 0.000 (0.083) loss 2.8359 (3.2064) acc 34.3750 (21.8750) lr 1.9921e-03 eta 0:20:46
epoch [10/200] batch [1/3] time 2.810 (2.810) data 0.244 (0.244) loss 3.1797 (3.1797) acc 28.1250 (28.1250) lr 1.9921e-03 eta 0:26:47
epoch [10/200] batch [2/3] time 3.214 (3.012) data 0.000 (0.122) loss 2.9805 (3.0801) acc 31.2500 (29.6875) lr 1.9921e-03 eta 0:28:39
epoch [10/200] batch [3/3] time 3.189 (3.071) data 0.000 (0.082) loss 2.7090 (2.9564) acc 46.8750 (35.4167) lr 1.9900e-03 eta 0:29:10
epoch [11/200] batch [1/3] time 3.154 (3.154) data 0.290 (0.290) loss 2.3184 (2.3184) acc 50.0000 (50.0000) lr 1.9900e-03 eta 0:29:54
epoch [11/200] batch [2/3] time 2.570 (2.862) data 0.000 (0.145) loss 2.8848 (2.6016) acc 31.2500 (40.6250) lr 1.9900e-03 eta 0:27:05
epoch [11/200] batch [3/3] time 2.539 (2.754) data 0.000 (0.097) loss 1.8311 (2.3447) acc 53.1250 (44.7917) lr 1.9877e-03 eta 0:26:01
epoch [12/200] batch [1/3] time 2.467 (2.467) data 0.270 (0.270) loss 2.0898 (2.0898) acc 46.8750 (46.8750) lr 1.9877e-03 eta 0:23:16
epoch [12/200] batch [2/3] time 2.422 (2.444) data 0.000 (0.135) loss 2.6445 (2.3672) acc 43.7500 (45.3125) lr 1.9877e-03 eta 0:23:01
epoch [12/200] batch [3/3] time 2.593 (2.494) data 0.000 (0.090) loss 1.7490 (2.1611) acc 50.0000 (46.8750) lr 1.9851e-03 eta 0:23:26
epoch [13/200] batch [1/3] time 2.784 (2.784) data 0.256 (0.256) loss 1.7598 (1.7598) acc 56.2500 (56.2500) lr 1.9851e-03 eta 0:26:07
epoch [13/200] batch [2/3] time 2.341 (2.563) data 0.000 (0.128) loss 2.0410 (1.9004) acc 56.2500 (56.2500) lr 1.9851e-03 eta 0:24:00
epoch [13/200] batch [3/3] time 2.102 (2.409) data 0.000 (0.085) loss 2.0332 (1.9447) acc 37.5000 (50.0000) lr 1.9823e-03 eta 0:22:31
epoch [14/200] batch [1/3] time 2.351 (2.351) data 0.253 (0.253) loss 1.6826 (1.6826) acc 65.6250 (65.6250) lr 1.9823e-03 eta 0:21:56
epoch [14/200] batch [2/3] time 2.095 (2.223) data 0.000 (0.127) loss 1.7871 (1.7349) acc 53.1250 (59.3750) lr 1.9823e-03 eta 0:20:42
epoch [14/200] batch [3/3] time 2.099 (2.182) data 0.000 (0.084) loss 1.4189 (1.6296) acc 62.5000 (60.4167) lr 1.9792e-03 eta 0:20:17
epoch [15/200] batch [1/3] time 2.346 (2.346) data 0.246 (0.246) loss 1.5742 (1.5742) acc 53.1250 (53.1250) lr 1.9792e-03 eta 0:21:46
epoch [15/200] batch [2/3] time 2.137 (2.241) data 0.000 (0.123) loss 1.4111 (1.4927) acc 62.5000 (57.8125) lr 1.9792e-03 eta 0:20:46
epoch [15/200] batch [3/3] time 2.272 (2.252) data 0.000 (0.082) loss 1.8994 (1.6283) acc 50.0000 (55.2083) lr 1.9759e-03 eta 0:20:49
epoch [16/200] batch [1/3] time 2.344 (2.344) data 0.253 (0.253) loss 1.4854 (1.4854) acc 65.6250 (65.6250) lr 1.9759e-03 eta 0:21:38
epoch [16/200] batch [2/3] time 2.097 (2.221) data 0.000 (0.127) loss 1.5566 (1.5210) acc 53.1250 (59.3750) lr 1.9759e-03 eta 0:20:28
epoch [16/200] batch [3/3] time 2.096 (2.179) data 0.000 (0.084) loss 1.3496 (1.4639) acc 62.5000 (60.4167) lr 1.9724e-03 eta 0:20:03
epoch [17/200] batch [1/3] time 2.350 (2.350) data 0.250 (0.250) loss 1.4619 (1.4619) acc 62.5000 (62.5000) lr 1.9724e-03 eta 0:21:34
epoch [17/200] batch [2/3] time 2.099 (2.225) data 0.000 (0.125) loss 1.2490 (1.3555) acc 68.7500 (65.6250) lr 1.9724e-03 eta 0:20:23
epoch [17/200] batch [3/3] time 2.103 (2.184) data 0.000 (0.083) loss 1.2734 (1.3281) acc 62.5000 (64.5833) lr 1.9686e-03 eta 0:19:59
epoch [18/200] batch [1/3] time 2.343 (2.343) data 0.245 (0.245) loss 1.0732 (1.0732) acc 75.0000 (75.0000) lr 1.9686e-03 eta 0:21:23
epoch [18/200] batch [2/3] time 2.103 (2.223) data 0.000 (0.122) loss 1.6064 (1.3398) acc 59.3750 (67.1875) lr 1.9686e-03 eta 0:20:15
epoch [18/200] batch [3/3] time 2.095 (2.180) data 0.000 (0.082) loss 1.5146 (1.3981) acc 62.5000 (65.6250) lr 1.9646e-03 eta 0:19:50
epoch [19/200] batch [1/3] time 2.335 (2.335) data 0.241 (0.241) loss 1.1377 (1.1377) acc 75.0000 (75.0000) lr 1.9646e-03 eta 0:21:12
epoch [19/200] batch [2/3] time 2.093 (2.214) data 0.000 (0.121) loss 1.2100 (1.1738) acc 65.6250 (70.3125) lr 1.9646e-03 eta 0:20:04
epoch [19/200] batch [3/3] time 2.100 (2.176) data 0.000 (0.081) loss 1.2363 (1.1947) acc 71.8750 (70.8333) lr 1.9603e-03 eta 0:19:41
epoch [20/200] batch [1/3] time 2.351 (2.351) data 0.263 (0.263) loss 1.0889 (1.0889) acc 65.6250 (65.6250) lr 1.9603e-03 eta 0:21:14
epoch [20/200] batch [2/3] time 2.093 (2.222) data 0.000 (0.132) loss 0.7666 (0.9277) acc 81.2500 (73.4375) lr 1.9603e-03 eta 0:20:02
epoch [20/200] batch [3/3] time 2.090 (2.178) data 0.000 (0.088) loss 1.0156 (0.9570) acc 68.7500 (71.8750) lr 1.9558e-03 eta 0:19:36
epoch [21/200] batch [1/3] time 2.351 (2.351) data 0.262 (0.262) loss 0.6211 (0.6211) acc 81.2500 (81.2500) lr 1.9558e-03 eta 0:21:07
epoch [21/200] batch [2/3] time 2.098 (2.225) data 0.000 (0.131) loss 1.3623 (0.9917) acc 65.6250 (73.4375) lr 1.9558e-03 eta 0:19:56
epoch [21/200] batch [3/3] time 2.095 (2.182) data 0.000 (0.087) loss 0.7749 (0.9194) acc 78.1250 (75.0000) lr 1.9511e-03 eta 0:19:31
epoch [22/200] batch [1/3] time 2.330 (2.330) data 0.244 (0.244) loss 0.7056 (0.7056) acc 84.3750 (84.3750) lr 1.9511e-03 eta 0:20:48
epoch [22/200] batch [2/3] time 2.094 (2.212) data 0.000 (0.122) loss 0.9307 (0.8181) acc 81.2500 (82.8125) lr 1.9511e-03 eta 0:19:43
epoch [22/200] batch [3/3] time 2.093 (2.172) data 0.000 (0.081) loss 0.7275 (0.7879) acc 81.2500 (82.2917) lr 1.9461e-03 eta 0:19:19
epoch [23/200] batch [1/3] time 2.335 (2.335) data 0.243 (0.243) loss 1.0615 (1.0615) acc 68.7500 (68.7500) lr 1.9461e-03 eta 0:20:44
epoch [23/200] batch [2/3] time 2.096 (2.216) data 0.000 (0.122) loss 0.6387 (0.8501) acc 84.3750 (76.5625) lr 1.9461e-03 eta 0:19:38
epoch [23/200] batch [3/3] time 2.096 (2.176) data 0.000 (0.081) loss 0.9292 (0.8765) acc 78.1250 (77.0833) lr 1.9409e-03 eta 0:19:15
epoch [24/200] batch [1/3] time 2.335 (2.335) data 0.239 (0.239) loss 1.1699 (1.1699) acc 71.8750 (71.8750) lr 1.9409e-03 eta 0:20:37
epoch [24/200] batch [2/3] time 2.092 (2.214) data 0.000 (0.120) loss 0.5527 (0.8613) acc 87.5000 (79.6875) lr 1.9409e-03 eta 0:19:30
epoch [24/200] batch [3/3] time 2.104 (2.177) data 0.000 (0.080) loss 0.6323 (0.7850) acc 87.5000 (82.2917) lr 1.9354e-03 eta 0:19:09
epoch [25/200] batch [1/3] time 2.347 (2.347) data 0.258 (0.258) loss 1.0742 (1.0742) acc 68.7500 (68.7500) lr 1.9354e-03 eta 0:20:36
epoch [25/200] batch [2/3] time 2.096 (2.221) data 0.000 (0.129) loss 1.0430 (1.0586) acc 78.1250 (73.4375) lr 1.9354e-03 eta 0:19:28
epoch [25/200] batch [3/3] time 2.100 (2.181) data 0.000 (0.086) loss 1.1904 (1.1025) acc 59.3750 (68.7500) lr 1.9298e-03 eta 0:19:04
epoch [26/200] batch [1/3] time 2.350 (2.350) data 0.252 (0.252) loss 0.8843 (0.8843) acc 75.0000 (75.0000) lr 1.9298e-03 eta 0:20:31
epoch [26/200] batch [2/3] time 2.098 (2.224) data 0.000 (0.126) loss 0.7773 (0.8308) acc 81.2500 (78.1250) lr 1.9298e-03 eta 0:19:23
epoch [26/200] batch [3/3] time 2.095 (2.181) data 0.000 (0.084) loss 0.6372 (0.7663) acc 84.3750 (80.2083) lr 1.9239e-03 eta 0:18:58
epoch [27/200] batch [1/3] time 2.339 (2.339) data 0.252 (0.252) loss 0.4714 (0.4714) acc 87.5000 (87.5000) lr 1.9239e-03 eta 0:20:18
epoch [27/200] batch [2/3] time 2.084 (2.212) data 0.000 (0.126) loss 0.3667 (0.4191) acc 93.7500 (90.6250) lr 1.9239e-03 eta 0:19:10
epoch [27/200] batch [3/3] time 2.095 (2.173) data 0.000 (0.084) loss 0.8564 (0.5649) acc 81.2500 (87.5000) lr 1.9178e-03 eta 0:18:47
epoch [28/200] batch [1/3] time 2.329 (2.329) data 0.243 (0.243) loss 1.2197 (1.2197) acc 68.7500 (68.7500) lr 1.9178e-03 eta 0:20:06
epoch [28/200] batch [2/3] time 2.085 (2.207) data 0.000 (0.122) loss 0.6328 (0.9263) acc 84.3750 (76.5625) lr 1.9178e-03 eta 0:19:01
epoch [28/200] batch [3/3] time 2.086 (2.167) data 0.000 (0.081) loss 0.4946 (0.7824) acc 90.6250 (81.2500) lr 1.9114e-03 eta 0:18:38
epoch [29/200] batch [1/3] time 2.329 (2.329) data 0.242 (0.242) loss 0.7192 (0.7192) acc 84.3750 (84.3750) lr 1.9114e-03 eta 0:19:59
epoch [29/200] batch [2/3] time 2.096 (2.212) data 0.000 (0.121) loss 0.7148 (0.7170) acc 84.3750 (84.3750) lr 1.9114e-03 eta 0:18:57
epoch [29/200] batch [3/3] time 2.097 (2.174) data 0.000 (0.081) loss 0.8413 (0.7585) acc 71.8750 (80.2083) lr 1.9048e-03 eta 0:18:35
epoch [30/200] batch [1/3] time 2.341 (2.341) data 0.246 (0.246) loss 0.7944 (0.7944) acc 81.2500 (81.2500) lr 1.9048e-03 eta 0:19:58
epoch [30/200] batch [2/3] time 2.094 (2.217) data 0.000 (0.123) loss 1.0791 (0.9368) acc 71.8750 (76.5625) lr 1.9048e-03 eta 0:18:53
epoch [30/200] batch [3/3] time 2.084 (2.173) data 0.000 (0.082) loss 0.3247 (0.7327) acc 93.7500 (82.2917) lr 1.8980e-03 eta 0:18:28
epoch [31/200] batch [1/3] time 2.331 (2.331) data 0.240 (0.240) loss 0.4648 (0.4648) acc 87.5000 (87.5000) lr 1.8980e-03 eta 0:19:46
epoch [31/200] batch [2/3] time 2.091 (2.211) data 0.000 (0.120) loss 0.4531 (0.4590) acc 90.6250 (89.0625) lr 1.8980e-03 eta 0:18:43
epoch [31/200] batch [3/3] time 2.090 (2.171) data 0.000 (0.080) loss 0.5571 (0.4917) acc 87.5000 (88.5417) lr 1.8910e-03 eta 0:18:20
epoch [32/200] batch [1/3] time 2.342 (2.342) data 0.254 (0.254) loss 0.7808 (0.7808) acc 87.5000 (87.5000) lr 1.8910e-03 eta 0:19:45
epoch [32/200] batch [2/3] time 2.092 (2.217) data 0.000 (0.127) loss 0.5190 (0.6499) acc 90.6250 (89.0625) lr 1.8910e-03 eta 0:18:39
epoch [32/200] batch [3/3] time 2.097 (2.177) data 0.000 (0.085) loss 0.7271 (0.6756) acc 84.3750 (87.5000) lr 1.8838e-03 eta 0:18:17
epoch [33/200] batch [1/3] time 2.325 (2.325) data 0.244 (0.244) loss 0.2935 (0.2935) acc 93.7500 (93.7500) lr 1.8838e-03 eta 0:19:29
epoch [33/200] batch [2/3] time 2.091 (2.208) data 0.000 (0.122) loss 1.0869 (0.6902) acc 84.3750 (89.0625) lr 1.8838e-03 eta 0:18:28
epoch [33/200] batch [3/3] time 2.087 (2.168) data 0.000 (0.081) loss 0.6768 (0.6857) acc 84.3750 (87.5000) lr 1.8763e-03 eta 0:18:06
epoch [34/200] batch [1/3] time 2.322 (2.322) data 0.243 (0.243) loss 0.5361 (0.5361) acc 87.5000 (87.5000) lr 1.8763e-03 eta 0:19:21
epoch [34/200] batch [2/3] time 2.090 (2.206) data 0.000 (0.122) loss 0.6313 (0.5837) acc 84.3750 (85.9375) lr 1.8763e-03 eta 0:18:20
epoch [34/200] batch [3/3] time 2.090 (2.167) data 0.000 (0.081) loss 0.3901 (0.5192) acc 90.6250 (87.5000) lr 1.8686e-03 eta 0:17:59
epoch [35/200] batch [1/3] time 2.352 (2.352) data 0.262 (0.262) loss 0.6899 (0.6899) acc 90.6250 (90.6250) lr 1.8686e-03 eta 0:19:29
epoch [35/200] batch [2/3] time 2.088 (2.220) data 0.000 (0.131) loss 0.5791 (0.6345) acc 84.3750 (87.5000) lr 1.8686e-03 eta 0:18:21
epoch [35/200] batch [3/3] time 2.084 (2.174) data 0.000 (0.087) loss 0.2722 (0.5138) acc 96.8750 (90.6250) lr 1.8607e-03 eta 0:17:56
epoch [36/200] batch [1/3] time 2.332 (2.332) data 0.252 (0.252) loss 0.5200 (0.5200) acc 90.6250 (90.6250) lr 1.8607e-03 eta 0:19:12
epoch [36/200] batch [2/3] time 2.088 (2.210) data 0.000 (0.126) loss 0.4009 (0.4604) acc 90.6250 (90.6250) lr 1.8607e-03 eta 0:18:09
epoch [36/200] batch [3/3] time 2.081 (2.167) data 0.000 (0.084) loss 0.4409 (0.4539) acc 87.5000 (89.5833) lr 1.8526e-03 eta 0:17:46
epoch [37/200] batch [1/3] time 2.332 (2.332) data 0.257 (0.257) loss 0.3245 (0.3245) acc 93.7500 (93.7500) lr 1.8526e-03 eta 0:19:05
epoch [37/200] batch [2/3] time 2.086 (2.209) data 0.000 (0.129) loss 0.2966 (0.3105) acc 96.8750 (95.3125) lr 1.8526e-03 eta 0:18:02
epoch [37/200] batch [3/3] time 2.096 (2.171) data 0.000 (0.086) loss 0.6274 (0.4162) acc 84.3750 (91.6667) lr 1.8443e-03 eta 0:17:41
epoch [38/200] batch [1/3] time 2.349 (2.349) data 0.260 (0.260) loss 0.8442 (0.8442) acc 84.3750 (84.3750) lr 1.8443e-03 eta 0:19:06
epoch [38/200] batch [2/3] time 2.084 (2.217) data 0.000 (0.130) loss 0.5698 (0.7070) acc 90.6250 (87.5000) lr 1.8443e-03 eta 0:17:59
epoch [38/200] batch [3/3] time 2.094 (2.176) data 0.000 (0.087) loss 0.6064 (0.6735) acc 90.6250 (88.5417) lr 1.8358e-03 eta 0:17:37
epoch [39/200] batch [1/3] time 2.334 (2.334) data 0.259 (0.259) loss 0.3474 (0.3474) acc 87.5000 (87.5000) lr 1.8358e-03 eta 0:18:51
epoch [39/200] batch [2/3] time 2.083 (2.208) data 0.000 (0.130) loss 0.5371 (0.4423) acc 84.3750 (85.9375) lr 1.8358e-03 eta 0:17:48
epoch [39/200] batch [3/3] time 2.076 (2.164) data 0.000 (0.086) loss 0.4048 (0.4298) acc 90.6250 (87.5000) lr 1.8271e-03 eta 0:17:25
epoch [40/200] batch [1/3] time 2.349 (2.349) data 0.261 (0.261) loss 0.4189 (0.4189) acc 93.7500 (93.7500) lr 1.8271e-03 eta 0:18:52
epoch [40/200] batch [2/3] time 2.079 (2.214) data 0.000 (0.131) loss 0.3977 (0.4083) acc 93.7500 (93.7500) lr 1.8271e-03 eta 0:17:45
epoch [40/200] batch [3/3] time 2.075 (2.168) data 0.000 (0.087) loss 0.4050 (0.4072) acc 90.6250 (92.7083) lr 1.8181e-03 eta 0:17:20
epoch [41/200] batch [1/3] time 2.348 (2.348) data 0.263 (0.263) loss 0.6421 (0.6421) acc 81.2500 (81.2500) lr 1.8181e-03 eta 0:18:44
epoch [41/200] batch [2/3] time 2.074 (2.211) data 0.000 (0.132) loss 0.3809 (0.5115) acc 87.5000 (84.3750) lr 1.8181e-03 eta 0:17:36
epoch [41/200] batch [3/3] time 2.079 (2.167) data 0.000 (0.088) loss 0.2465 (0.4231) acc 100.0000 (89.5833) lr 1.8090e-03 eta 0:17:13
epoch [42/200] batch [1/3] time 2.329 (2.329) data 0.255 (0.255) loss 0.5195 (0.5195) acc 81.2500 (81.2500) lr 1.8090e-03 eta 0:18:28
epoch [42/200] batch [2/3] time 2.086 (2.208) data 0.000 (0.127) loss 0.4802 (0.4999) acc 90.6250 (85.9375) lr 1.8090e-03 eta 0:17:28
epoch [42/200] batch [3/3] time 2.081 (2.166) data 0.000 (0.085) loss 0.6440 (0.5479) acc 81.2500 (84.3750) lr 1.7997e-03 eta 0:17:06
epoch [43/200] batch [1/3] time 2.314 (2.314) data 0.243 (0.243) loss 0.5435 (0.5435) acc 87.5000 (87.5000) lr 1.7997e-03 eta 0:18:14
epoch [43/200] batch [2/3] time 2.081 (2.197) data 0.000 (0.122) loss 0.3740 (0.4587) acc 93.7500 (90.6250) lr 1.7997e-03 eta 0:17:17
epoch [43/200] batch [3/3] time 2.067 (2.154) data 0.000 (0.081) loss 0.2546 (0.3907) acc 93.7500 (91.6667) lr 1.7902e-03 eta 0:16:54
epoch [44/200] batch [1/3] time 2.340 (2.340) data 0.259 (0.259) loss 0.7056 (0.7056) acc 87.5000 (87.5000) lr 1.7902e-03 eta 0:18:20
epoch [44/200] batch [2/3] time 2.091 (2.216) data 0.000 (0.130) loss 0.8115 (0.7585) acc 81.2500 (84.3750) lr 1.7902e-03 eta 0:17:19
epoch [44/200] batch [3/3] time 2.092 (2.175) data 0.000 (0.086) loss 0.8823 (0.7998) acc 84.3750 (84.3750) lr 1.7804e-03 eta 0:16:57
epoch [45/200] batch [1/3] time 2.333 (2.333) data 0.250 (0.250) loss 0.5059 (0.5059) acc 87.5000 (87.5000) lr 1.7804e-03 eta 0:18:09
epoch [45/200] batch [2/3] time 2.088 (2.210) data 0.000 (0.125) loss 0.3062 (0.4060) acc 93.7500 (90.6250) lr 1.7804e-03 eta 0:17:09
epoch [45/200] batch [3/3] time 2.090 (2.170) data 0.000 (0.083) loss 0.8931 (0.5684) acc 75.0000 (85.4167) lr 1.7705e-03 eta 0:16:49
epoch [46/200] batch [1/3] time 2.315 (2.315) data 0.244 (0.244) loss 0.4219 (0.4219) acc 87.5000 (87.5000) lr 1.7705e-03 eta 0:17:54
epoch [46/200] batch [2/3] time 2.090 (2.203) data 0.000 (0.122) loss 0.4041 (0.4130) acc 93.7500 (90.6250) lr 1.7705e-03 eta 0:16:59
epoch [46/200] batch [3/3] time 2.100 (2.169) data 0.000 (0.081) loss 0.6436 (0.4898) acc 84.3750 (88.5417) lr 1.7604e-03 eta 0:16:41
epoch [47/200] batch [1/3] time 2.334 (2.334) data 0.251 (0.251) loss 0.4995 (0.4995) acc 93.7500 (93.7500) lr 1.7604e-03 eta 0:17:55
epoch [47/200] batch [2/3] time 2.078 (2.206) data 0.000 (0.126) loss 0.3975 (0.4485) acc 93.7500 (93.7500) lr 1.7604e-03 eta 0:16:54
epoch [47/200] batch [3/3] time 2.082 (2.165) data 0.000 (0.084) loss 0.5981 (0.4984) acc 84.3750 (90.6250) lr 1.7501e-03 eta 0:16:33
epoch [48/200] batch [1/3] time 2.352 (2.352) data 0.268 (0.268) loss 0.6396 (0.6396) acc 90.6250 (90.6250) lr 1.7501e-03 eta 0:17:57
epoch [48/200] batch [2/3] time 2.091 (2.222) data 0.000 (0.134) loss 0.5088 (0.5742) acc 84.3750 (87.5000) lr 1.7501e-03 eta 0:16:55
epoch [48/200] batch [3/3] time 2.086 (2.176) data 0.000 (0.089) loss 0.6650 (0.6045) acc 84.3750 (86.4583) lr 1.7396e-03 eta 0:16:32
epoch [49/200] batch [1/3] time 2.334 (2.334) data 0.250 (0.250) loss 0.3550 (0.3550) acc 93.7500 (93.7500) lr 1.7396e-03 eta 0:17:42
epoch [49/200] batch [2/3] time 2.086 (2.210) data 0.000 (0.125) loss 0.4749 (0.4149) acc 87.5000 (90.6250) lr 1.7396e-03 eta 0:16:43
epoch [49/200] batch [3/3] time 2.084 (2.168) data 0.000 (0.083) loss 0.5977 (0.4758) acc 84.3750 (88.5417) lr 1.7290e-03 eta 0:16:22
epoch [50/200] batch [1/3] time 2.324 (2.324) data 0.244 (0.244) loss 0.8403 (0.8403) acc 75.0000 (75.0000) lr 1.7290e-03 eta 0:17:30
epoch [50/200] batch [2/3] time 2.086 (2.205) data 0.000 (0.122) loss 0.4089 (0.6246) acc 87.5000 (81.2500) lr 1.7290e-03 eta 0:16:34
epoch [50/200] batch [3/3] time 2.090 (2.167) data 0.000 (0.081) loss 0.6475 (0.6322) acc 87.5000 (83.3333) lr 1.7181e-03 eta 0:16:15
epoch [51/200] batch [1/3] time 2.328 (2.328) data 0.244 (0.244) loss 0.5967 (0.5967) acc 90.6250 (90.6250) lr 1.7181e-03 eta 0:17:25
epoch [51/200] batch [2/3] time 2.080 (2.204) data 0.000 (0.122) loss 0.3716 (0.4841) acc 90.6250 (90.6250) lr 1.7181e-03 eta 0:16:27
epoch [51/200] batch [3/3] time 2.089 (2.166) data 0.000 (0.081) loss 0.7275 (0.5653) acc 84.3750 (88.5417) lr 1.7071e-03 eta 0:16:08
epoch [52/200] batch [1/3] time 2.313 (2.313) data 0.242 (0.242) loss 0.4626 (0.4626) acc 87.5000 (87.5000) lr 1.7071e-03 eta 0:17:11
epoch [52/200] batch [2/3] time 2.079 (2.196) data 0.000 (0.121) loss 0.4248 (0.4437) acc 90.6250 (89.0625) lr 1.7071e-03 eta 0:16:17
epoch [52/200] batch [3/3] time 2.078 (2.157) data 0.000 (0.081) loss 0.4587 (0.4487) acc 87.5000 (88.5417) lr 1.6959e-03 eta 0:15:57
epoch [53/200] batch [1/3] time 2.318 (2.318) data 0.245 (0.245) loss 0.1969 (0.1969) acc 96.8750 (96.8750) lr 1.6959e-03 eta 0:17:06
epoch [53/200] batch [2/3] time 2.083 (2.200) data 0.000 (0.123) loss 0.2654 (0.2311) acc 93.7500 (95.3125) lr 1.6959e-03 eta 0:16:12
epoch [53/200] batch [3/3] time 2.092 (2.164) data 0.000 (0.082) loss 0.3579 (0.2734) acc 90.6250 (93.7500) lr 1.6845e-03 eta 0:15:54
epoch [54/200] batch [1/3] time 2.332 (2.332) data 0.254 (0.254) loss 0.2883 (0.2883) acc 93.7500 (93.7500) lr 1.6845e-03 eta 0:17:06
epoch [54/200] batch [2/3] time 2.086 (2.209) data 0.000 (0.127) loss 0.5283 (0.4083) acc 87.5000 (90.6250) lr 1.6845e-03 eta 0:16:09
epoch [54/200] batch [3/3] time 2.081 (2.166) data 0.000 (0.085) loss 0.2229 (0.3465) acc 100.0000 (93.7500) lr 1.6730e-03 eta 0:15:48
epoch [55/200] batch [1/3] time 2.337 (2.337) data 0.261 (0.261) loss 0.4099 (0.4099) acc 84.3750 (84.3750) lr 1.6730e-03 eta 0:17:01
epoch [55/200] batch [2/3] time 2.087 (2.212) data 0.000 (0.131) loss 0.7129 (0.5614) acc 84.3750 (84.3750) lr 1.6730e-03 eta 0:16:04
epoch [55/200] batch [3/3] time 2.076 (2.167) data 0.001 (0.087) loss 0.4675 (0.5301) acc 90.6250 (86.4583) lr 1.6613e-03 eta 0:15:42
epoch [56/200] batch [1/3] time 2.343 (2.343) data 0.260 (0.260) loss 0.6045 (0.6045) acc 90.6250 (90.6250) lr 1.6613e-03 eta 0:16:56
epoch [56/200] batch [2/3] time 2.084 (2.213) data 0.000 (0.130) loss 0.4741 (0.5393) acc 90.6250 (90.6250) lr 1.6613e-03 eta 0:15:58
epoch [56/200] batch [3/3] time 2.067 (2.165) data 0.000 (0.087) loss 0.2299 (0.4362) acc 96.8750 (92.7083) lr 1.6494e-03 eta 0:15:35
epoch [57/200] batch [1/3] time 2.336 (2.336) data 0.257 (0.257) loss 0.5386 (0.5386) acc 87.5000 (87.5000) lr 1.6494e-03 eta 0:16:46
epoch [57/200] batch [2/3] time 2.080 (2.208) data 0.000 (0.129) loss 0.3699 (0.4542) acc 96.8750 (92.1875) lr 1.6494e-03 eta 0:15:49
epoch [57/200] batch [3/3] time 2.087 (2.168) data 0.000 (0.086) loss 0.6304 (0.5129) acc 84.3750 (89.5833) lr 1.6374e-03 eta 0:15:29
epoch [58/200] batch [1/3] time 2.339 (2.339) data 0.261 (0.261) loss 0.4973 (0.4973) acc 87.5000 (87.5000) lr 1.6374e-03 eta 0:16:41
epoch [58/200] batch [2/3] time 2.079 (2.209) data 0.000 (0.131) loss 0.2698 (0.3835) acc 96.8750 (92.1875) lr 1.6374e-03 eta 0:15:43
epoch [58/200] batch [3/3] time 2.093 (2.170) data 0.000 (0.087) loss 0.7920 (0.5197) acc 81.2500 (88.5417) lr 1.6252e-03 eta 0:15:24
epoch [59/200] batch [1/3] time 2.330 (2.330) data 0.258 (0.258) loss 0.5371 (0.5371) acc 84.3750 (84.3750) lr 1.6252e-03 eta 0:16:30
epoch [59/200] batch [2/3] time 2.088 (2.209) data 0.000 (0.129) loss 0.5688 (0.5530) acc 84.3750 (84.3750) lr 1.6252e-03 eta 0:15:36
epoch [59/200] batch [3/3] time 2.080 (2.166) data 0.000 (0.086) loss 0.3240 (0.4766) acc 93.7500 (87.5000) lr 1.6129e-03 eta 0:15:16
epoch [60/200] batch [1/3] time 2.319 (2.319) data 0.244 (0.244) loss 0.6787 (0.6787) acc 84.3750 (84.3750) lr 1.6129e-03 eta 0:16:18
epoch [60/200] batch [2/3] time 2.086 (2.202) data 0.000 (0.122) loss 0.3557 (0.5172) acc 93.7500 (89.0625) lr 1.6129e-03 eta 0:15:27
epoch [60/200] batch [3/3] time 2.091 (2.165) data 0.000 (0.081) loss 0.4695 (0.5013) acc 90.6250 (89.5833) lr 1.6004e-03 eta 0:15:09
epoch [61/200] batch [1/3] time 2.337 (2.337) data 0.258 (0.258) loss 0.3760 (0.3760) acc 93.7500 (93.7500) lr 1.6004e-03 eta 0:16:19
epoch [61/200] batch [2/3] time 2.131 (2.234) data 0.000 (0.129) loss 0.5229 (0.4495) acc 87.5000 (90.6250) lr 1.6004e-03 eta 0:15:33
epoch [61/200] batch [3/3] time 2.188 (2.219) data 0.000 (0.086) loss 0.4189 (0.4393) acc 93.7500 (91.6667) lr 1.5878e-03 eta 0:15:25
epoch [62/200] batch [1/3] time 2.410 (2.410) data 0.253 (0.253) loss 0.3884 (0.3884) acc 87.5000 (87.5000) lr 1.5878e-03 eta 0:16:42
epoch [62/200] batch [2/3] time 2.134 (2.272) data 0.000 (0.127) loss 0.5117 (0.4501) acc 93.7500 (90.6250) lr 1.5878e-03 eta 0:15:42
epoch [62/200] batch [3/3] time 2.101 (2.215) data 0.000 (0.084) loss 0.3774 (0.4259) acc 90.6250 (90.6250) lr 1.5750e-03 eta 0:15:16
epoch [63/200] batch [1/3] time 2.333 (2.333) data 0.242 (0.242) loss 0.3374 (0.3374) acc 93.7500 (93.7500) lr 1.5750e-03 eta 0:16:03
epoch [63/200] batch [2/3] time 2.072 (2.202) data 0.000 (0.121) loss 0.5366 (0.4370) acc 84.3750 (89.0625) lr 1.5750e-03 eta 0:15:07
epoch [63/200] batch [3/3] time 2.056 (2.154) data 0.000 (0.081) loss 0.3513 (0.4084) acc 90.6250 (89.5833) lr 1.5621e-03 eta 0:14:45
epoch [64/200] batch [1/3] time 2.321 (2.321) data 0.259 (0.259) loss 0.4209 (0.4209) acc 90.6250 (90.6250) lr 1.5621e-03 eta 0:15:51
epoch [64/200] batch [2/3] time 2.057 (2.189) data 0.000 (0.130) loss 0.2150 (0.3179) acc 93.7500 (92.1875) lr 1.5621e-03 eta 0:14:55
epoch [64/200] batch [3/3] time 2.062 (2.147) data 0.000 (0.086) loss 0.5322 (0.3894) acc 93.7500 (92.7083) lr 1.5490e-03 eta 0:14:35
epoch [65/200] batch [1/3] time 2.323 (2.323) data 0.255 (0.255) loss 0.3782 (0.3782) acc 90.6250 (90.6250) lr 1.5490e-03 eta 0:15:45
epoch [65/200] batch [2/3] time 2.052 (2.187) data 0.000 (0.128) loss 0.2563 (0.3173) acc 96.8750 (93.7500) lr 1.5490e-03 eta 0:14:48
epoch [65/200] batch [3/3] time 2.067 (2.147) data 0.000 (0.085) loss 0.6831 (0.4392) acc 84.3750 (90.6250) lr 1.5358e-03 eta 0:14:29
epoch [66/200] batch [1/3] time 2.313 (2.313) data 0.251 (0.251) loss 0.2478 (0.2478) acc 96.8750 (96.8750) lr 1.5358e-03 eta 0:15:34
epoch [66/200] batch [2/3] time 2.058 (2.186) data 0.000 (0.125) loss 0.4746 (0.3612) acc 81.2500 (89.0625) lr 1.5358e-03 eta 0:14:40
epoch [66/200] batch [3/3] time 2.072 (2.148) data 0.000 (0.084) loss 0.6113 (0.4446) acc 81.2500 (86.4583) lr 1.5225e-03 eta 0:14:23
epoch [67/200] batch [1/3] time 2.309 (2.309) data 0.242 (0.242) loss 0.5186 (0.5186) acc 90.6250 (90.6250) lr 1.5225e-03 eta 0:15:25
epoch [67/200] batch [2/3] time 2.057 (2.183) data 0.000 (0.121) loss 0.2705 (0.3945) acc 93.7500 (92.1875) lr 1.5225e-03 eta 0:14:33
epoch [67/200] batch [3/3] time 2.065 (2.144) data 0.000 (0.081) loss 0.3735 (0.3875) acc 90.6250 (91.6667) lr 1.5090e-03 eta 0:14:15
epoch [68/200] batch [1/3] time 2.311 (2.311) data 0.243 (0.243) loss 0.6421 (0.6421) acc 81.2500 (81.2500) lr 1.5090e-03 eta 0:15:19
epoch [68/200] batch [2/3] time 2.060 (2.185) data 0.000 (0.122) loss 0.5874 (0.6147) acc 84.3750 (82.8125) lr 1.5090e-03 eta 0:14:27
epoch [68/200] batch [3/3] time 2.057 (2.143) data 0.000 (0.081) loss 0.3044 (0.5113) acc 93.7500 (86.4583) lr 1.4955e-03 eta 0:14:08
epoch [69/200] batch [1/3] time 2.327 (2.327) data 0.258 (0.258) loss 0.7373 (0.7373) acc 78.1250 (78.1250) lr 1.4955e-03 eta 0:15:19
epoch [69/200] batch [2/3] time 2.042 (2.184) data 0.000 (0.129) loss 0.1415 (0.4394) acc 100.0000 (89.0625) lr 1.4955e-03 eta 0:14:20
epoch [69/200] batch [3/3] time 2.053 (2.140) data 0.000 (0.086) loss 0.2915 (0.3901) acc 90.6250 (89.5833) lr 1.4818e-03 eta 0:14:01
epoch [70/200] batch [1/3] time 2.300 (2.300) data 0.244 (0.244) loss 0.1781 (0.1781) acc 100.0000 (100.0000) lr 1.4818e-03 eta 0:15:01
epoch [70/200] batch [2/3] time 2.067 (2.184) data 0.000 (0.122) loss 0.4414 (0.3098) acc 87.5000 (93.7500) lr 1.4818e-03 eta 0:14:13
epoch [70/200] batch [3/3] time 2.064 (2.144) data 0.000 (0.081) loss 0.5483 (0.3893) acc 90.6250 (92.7083) lr 1.4679e-03 eta 0:13:56
epoch [71/200] batch [1/3] time 2.310 (2.310) data 0.249 (0.249) loss 0.5449 (0.5449) acc 84.3750 (84.3750) lr 1.4679e-03 eta 0:14:58
epoch [71/200] batch [2/3] time 2.067 (2.188) data 0.000 (0.124) loss 0.8447 (0.6948) acc 81.2500 (82.8125) lr 1.4679e-03 eta 0:14:09
epoch [71/200] batch [3/3] time 2.072 (2.150) data 0.000 (0.083) loss 0.3787 (0.5894) acc 90.6250 (85.4167) lr 1.4540e-03 eta 0:13:51
epoch [72/200] batch [1/3] time 2.292 (2.292) data 0.241 (0.241) loss 0.1384 (0.1384) acc 100.0000 (100.0000) lr 1.4540e-03 eta 0:14:44
epoch [72/200] batch [2/3] time 2.059 (2.175) data 0.000 (0.120) loss 0.2217 (0.1801) acc 96.8750 (98.4375) lr 1.4540e-03 eta 0:13:57
epoch [72/200] batch [3/3] time 2.072 (2.141) data 0.000 (0.080) loss 0.4902 (0.2834) acc 90.6250 (95.8333) lr 1.4399e-03 eta 0:13:42
epoch [73/200] batch [1/3] time 2.313 (2.313) data 0.250 (0.250) loss 0.4661 (0.4661) acc 87.5000 (87.5000) lr 1.4399e-03 eta 0:14:45
epoch [73/200] batch [2/3] time 2.049 (2.181) data 0.000 (0.125) loss 0.2365 (0.3513) acc 93.7500 (90.6250) lr 1.4399e-03 eta 0:13:53
epoch [73/200] batch [3/3] time 2.055 (2.139) data 0.000 (0.083) loss 0.3848 (0.3624) acc 84.3750 (88.5417) lr 1.4258e-03 eta 0:13:34
epoch [74/200] batch [1/3] time 2.314 (2.314) data 0.257 (0.257) loss 0.3518 (0.3518) acc 96.8750 (96.8750) lr 1.4258e-03 eta 0:14:39
epoch [74/200] batch [2/3] time 2.068 (2.191) data 0.000 (0.129) loss 0.4563 (0.4041) acc 84.3750 (90.6250) lr 1.4258e-03 eta 0:13:50
epoch [74/200] batch [3/3] time 2.069 (2.150) data 0.000 (0.086) loss 0.5381 (0.4487) acc 84.3750 (88.5417) lr 1.4115e-03 eta 0:13:32
epoch [75/200] batch [1/3] time 2.306 (2.306) data 0.249 (0.249) loss 0.3098 (0.3098) acc 93.7500 (93.7500) lr 1.4115e-03 eta 0:14:29
epoch [75/200] batch [2/3] time 2.062 (2.184) data 0.000 (0.125) loss 0.3298 (0.3198) acc 93.7500 (93.7500) lr 1.4115e-03 eta 0:13:41
epoch [75/200] batch [3/3] time 2.068 (2.145) data 0.000 (0.083) loss 0.7686 (0.4694) acc 75.0000 (87.5000) lr 1.3971e-03 eta 0:13:24
epoch [76/200] batch [1/3] time 2.302 (2.302) data 0.242 (0.242) loss 0.2341 (0.2341) acc 96.8750 (96.8750) lr 1.3971e-03 eta 0:14:21
epoch [76/200] batch [2/3] time 2.058 (2.180) data 0.000 (0.121) loss 0.1494 (0.1918) acc 100.0000 (98.4375) lr 1.3971e-03 eta 0:13:33
epoch [76/200] batch [3/3] time 2.073 (2.144) data 0.000 (0.081) loss 0.5059 (0.2965) acc 90.6250 (95.8333) lr 1.3827e-03 eta 0:13:17
epoch [77/200] batch [1/3] time 2.295 (2.295) data 0.243 (0.243) loss 0.3171 (0.3171) acc 93.7500 (93.7500) lr 1.3827e-03 eta 0:14:11
epoch [77/200] batch [2/3] time 2.052 (2.173) data 0.000 (0.121) loss 0.4004 (0.3588) acc 90.6250 (92.1875) lr 1.3827e-03 eta 0:13:24
epoch [77/200] batch [3/3] time 2.068 (2.138) data 0.000 (0.081) loss 0.4741 (0.3972) acc 87.5000 (90.6250) lr 1.3681e-03 eta 0:13:09
epoch [78/200] batch [1/3] time 2.309 (2.309) data 0.241 (0.241) loss 0.4241 (0.4241) acc 87.5000 (87.5000) lr 1.3681e-03 eta 0:14:09
epoch [78/200] batch [2/3] time 2.055 (2.182) data 0.000 (0.120) loss 0.3511 (0.3876) acc 90.6250 (89.0625) lr 1.3681e-03 eta 0:13:20
epoch [78/200] batch [3/3] time 2.061 (2.142) data 0.000 (0.080) loss 0.3164 (0.3639) acc 93.7500 (90.6250) lr 1.3535e-03 eta 0:13:03
epoch [79/200] batch [1/3] time 2.298 (2.298) data 0.246 (0.246) loss 0.2371 (0.2371) acc 93.7500 (93.7500) lr 1.3535e-03 eta 0:13:58
epoch [79/200] batch [2/3] time 2.065 (2.182) data 0.000 (0.123) loss 0.4380 (0.3375) acc 87.5000 (90.6250) lr 1.3535e-03 eta 0:13:14
epoch [79/200] batch [3/3] time 2.055 (2.140) data 0.000 (0.082) loss 0.4060 (0.3604) acc 87.5000 (89.5833) lr 1.3387e-03 eta 0:12:56
epoch [80/200] batch [1/3] time 2.328 (2.328) data 0.258 (0.258) loss 0.5449 (0.5449) acc 84.3750 (84.3750) lr 1.3387e-03 eta 0:14:02
epoch [80/200] batch [2/3] time 2.065 (2.197) data 0.000 (0.129) loss 0.5410 (0.5430) acc 84.3750 (84.3750) lr 1.3387e-03 eta 0:13:12
epoch [80/200] batch [3/3] time 2.055 (2.149) data 0.000 (0.086) loss 0.1671 (0.4177) acc 96.8750 (88.5417) lr 1.3239e-03 eta 0:12:53
epoch [81/200] batch [1/3] time 2.313 (2.313) data 0.245 (0.245) loss 0.4441 (0.4441) acc 93.7500 (93.7500) lr 1.3239e-03 eta 0:13:50
epoch [81/200] batch [2/3] time 2.069 (2.191) data 0.000 (0.122) loss 0.3706 (0.4073) acc 93.7500 (93.7500) lr 1.3239e-03 eta 0:13:04
epoch [81/200] batch [3/3] time 2.059 (2.147) data 0.000 (0.082) loss 0.3918 (0.4022) acc 93.7500 (93.7500) lr 1.3090e-03 eta 0:12:46
epoch [82/200] batch [1/3] time 2.306 (2.306) data 0.246 (0.246) loss 0.5591 (0.5591) acc 87.5000 (87.5000) lr 1.3090e-03 eta 0:13:40
epoch [82/200] batch [2/3] time 2.069 (2.187) data 0.000 (0.123) loss 0.3545 (0.4568) acc 93.7500 (90.6250) lr 1.3090e-03 eta 0:12:56
epoch [82/200] batch [3/3] time 2.061 (2.145) data 0.000 (0.082) loss 0.2695 (0.3944) acc 93.7500 (91.6667) lr 1.2940e-03 eta 0:12:39
epoch [83/200] batch [1/3] time 2.304 (2.304) data 0.251 (0.251) loss 0.2399 (0.2399) acc 96.8750 (96.8750) lr 1.2940e-03 eta 0:13:33
epoch [83/200] batch [2/3] time 2.059 (2.182) data 0.000 (0.126) loss 0.2749 (0.2574) acc 93.7500 (95.3125) lr 1.2940e-03 eta 0:12:47
epoch [83/200] batch [3/3] time 2.048 (2.137) data 0.000 (0.084) loss 0.1603 (0.2250) acc 96.8750 (95.8333) lr 1.2790e-03 eta 0:12:30
epoch [84/200] batch [1/3] time 2.301 (2.301) data 0.245 (0.245) loss 0.3396 (0.3396) acc 87.5000 (87.5000) lr 1.2790e-03 eta 0:13:25
epoch [84/200] batch [2/3] time 2.067 (2.184) data 0.000 (0.123) loss 0.3550 (0.3473) acc 93.7500 (90.6250) lr 1.2790e-03 eta 0:12:42
epoch [84/200] batch [3/3] time 2.035 (2.135) data 0.000 (0.082) loss 0.1696 (0.2880) acc 96.8750 (92.7083) lr 1.2639e-03 eta 0:12:22
epoch [85/200] batch [1/3] time 2.304 (2.304) data 0.250 (0.250) loss 0.3374 (0.3374) acc 96.8750 (96.8750) lr 1.2639e-03 eta 0:13:19
epoch [85/200] batch [2/3] time 2.066 (2.185) data 0.000 (0.125) loss 0.6069 (0.4722) acc 87.5000 (92.1875) lr 1.2639e-03 eta 0:12:36
epoch [85/200] batch [3/3] time 2.059 (2.143) data 0.000 (0.084) loss 0.4832 (0.4758) acc 87.5000 (90.6250) lr 1.2487e-03 eta 0:12:19
epoch [86/200] batch [1/3] time 2.301 (2.301) data 0.246 (0.246) loss 0.3018 (0.3018) acc 93.7500 (93.7500) lr 1.2487e-03 eta 0:13:11
epoch [86/200] batch [2/3] time 2.067 (2.184) data 0.000 (0.123) loss 0.7666 (0.5342) acc 84.3750 (89.0625) lr 1.2487e-03 eta 0:12:29
epoch [86/200] batch [3/3] time 2.043 (2.137) data 0.000 (0.082) loss 0.1184 (0.3956) acc 100.0000 (92.7083) lr 1.2334e-03 eta 0:12:10
epoch [87/200] batch [1/3] time 2.300 (2.300) data 0.246 (0.246) loss 0.4209 (0.4209) acc 87.5000 (87.5000) lr 1.2334e-03 eta 0:13:04
epoch [87/200] batch [2/3] time 2.060 (2.180) data 0.000 (0.123) loss 0.3291 (0.3750) acc 93.7500 (90.6250) lr 1.2334e-03 eta 0:12:21
epoch [87/200] batch [3/3] time 2.072 (2.144) data 0.000 (0.082) loss 0.6299 (0.4600) acc 87.5000 (89.5833) lr 1.2181e-03 eta 0:12:06
epoch [88/200] batch [1/3] time 2.306 (2.306) data 0.246 (0.246) loss 0.5376 (0.5376) acc 87.5000 (87.5000) lr 1.2181e-03 eta 0:12:59
epoch [88/200] batch [2/3] time 2.051 (2.178) data 0.000 (0.123) loss 0.1550 (0.3463) acc 100.0000 (93.7500) lr 1.2181e-03 eta 0:12:14
epoch [88/200] batch [3/3] time 2.056 (2.138) data 0.000 (0.082) loss 0.4233 (0.3720) acc 90.6250 (92.7083) lr 1.2028e-03 eta 0:11:58
epoch [89/200] batch [1/3] time 2.314 (2.314) data 0.260 (0.260) loss 0.3135 (0.3135) acc 93.7500 (93.7500) lr 1.2028e-03 eta 0:12:55
epoch [89/200] batch [2/3] time 2.042 (2.178) data 0.000 (0.130) loss 0.1603 (0.2369) acc 96.8750 (95.3125) lr 1.2028e-03 eta 0:12:07
epoch [89/200] batch [3/3] time 2.066 (2.141) data 0.000 (0.087) loss 0.6304 (0.3680) acc 87.5000 (92.7083) lr 1.1874e-03 eta 0:11:52
epoch [90/200] batch [1/3] time 2.305 (2.305) data 0.261 (0.261) loss 0.2115 (0.2115) acc 93.7500 (93.7500) lr 1.1874e-03 eta 0:12:45
epoch [90/200] batch [2/3] time 2.060 (2.182) data 0.000 (0.131) loss 0.4551 (0.3333) acc 84.3750 (89.0625) lr 1.1874e-03 eta 0:12:02
epoch [90/200] batch [3/3] time 2.067 (2.144) data 0.000 (0.087) loss 0.4165 (0.3610) acc 93.7500 (90.6250) lr 1.1719e-03 eta 0:11:47
epoch [91/200] batch [1/3] time 2.317 (2.317) data 0.260 (0.260) loss 0.2957 (0.2957) acc 90.6250 (90.6250) lr 1.1719e-03 eta 0:12:42
epoch [91/200] batch [2/3] time 2.054 (2.186) data 0.000 (0.130) loss 0.3303 (0.3130) acc 84.3750 (87.5000) lr 1.1719e-03 eta 0:11:56
epoch [91/200] batch [3/3] time 2.061 (2.144) data 0.000 (0.087) loss 0.5698 (0.3986) acc 81.2500 (85.4167) lr 1.1564e-03 eta 0:11:41
epoch [92/200] batch [1/3] time 2.314 (2.314) data 0.243 (0.243) loss 0.7104 (0.7104) acc 78.1250 (78.1250) lr 1.1564e-03 eta 0:12:34
epoch [92/200] batch [2/3] time 2.052 (2.183) data 0.000 (0.121) loss 0.1875 (0.4490) acc 96.8750 (87.5000) lr 1.1564e-03 eta 0:11:49
epoch [92/200] batch [3/3] time 2.045 (2.137) data 0.000 (0.081) loss 0.1547 (0.3509) acc 96.8750 (90.6250) lr 1.1409e-03 eta 0:11:32
epoch [93/200] batch [1/3] time 2.294 (2.294) data 0.244 (0.244) loss 0.3586 (0.3586) acc 90.6250 (90.6250) lr 1.1409e-03 eta 0:12:20
epoch [93/200] batch [2/3] time 2.061 (2.177) data 0.000 (0.122) loss 0.5640 (0.4613) acc 93.7500 (92.1875) lr 1.1409e-03 eta 0:11:41
epoch [93/200] batch [3/3] time 2.064 (2.139) data 0.000 (0.081) loss 0.4753 (0.4660) acc 90.6250 (91.6667) lr 1.1253e-03 eta 0:11:26
epoch [94/200] batch [1/3] time 2.310 (2.310) data 0.254 (0.254) loss 0.2471 (0.2471) acc 93.7500 (93.7500) lr 1.1253e-03 eta 0:12:19
epoch [94/200] batch [2/3] time 2.053 (2.182) data 0.000 (0.127) loss 0.2192 (0.2332) acc 93.7500 (93.7500) lr 1.1253e-03 eta 0:11:35
epoch [94/200] batch [3/3] time 2.053 (2.139) data 0.000 (0.085) loss 0.2159 (0.2274) acc 96.8750 (94.7917) lr 1.1097e-03 eta 0:11:20
epoch [95/200] batch [1/3] time 2.304 (2.304) data 0.243 (0.243) loss 0.3123 (0.3123) acc 93.7500 (93.7500) lr 1.1097e-03 eta 0:12:10
epoch [95/200] batch [2/3] time 2.069 (2.186) data 0.000 (0.122) loss 0.5117 (0.4120) acc 84.3750 (89.0625) lr 1.1097e-03 eta 0:11:30
epoch [95/200] batch [3/3] time 2.051 (2.141) data 0.000 (0.081) loss 0.1726 (0.3322) acc 96.8750 (91.6667) lr 1.0941e-03 eta 0:11:14
epoch [96/200] batch [1/3] time 2.308 (2.308) data 0.246 (0.246) loss 0.2659 (0.2659) acc 90.6250 (90.6250) lr 1.0941e-03 eta 0:12:04
epoch [96/200] batch [2/3] time 2.058 (2.183) data 0.000 (0.123) loss 0.3120 (0.2889) acc 90.6250 (90.6250) lr 1.0941e-03 eta 0:11:23
epoch [96/200] batch [3/3] time 2.050 (2.139) data 0.000 (0.082) loss 0.3171 (0.2983) acc 93.7500 (91.6667) lr 1.0785e-03 eta 0:11:07
epoch [97/200] batch [1/3] time 2.289 (2.289) data 0.252 (0.252) loss 0.0922 (0.0922) acc 100.0000 (100.0000) lr 1.0785e-03 eta 0:11:51
epoch [97/200] batch [2/3] time 2.068 (2.178) data 0.000 (0.126) loss 0.6060 (0.3491) acc 87.5000 (93.7500) lr 1.0785e-03 eta 0:11:15
epoch [97/200] batch [3/3] time 2.071 (2.142) data 0.000 (0.084) loss 0.3701 (0.3561) acc 90.6250 (92.7083) lr 1.0628e-03 eta 0:11:01
epoch [98/200] batch [1/3] time 2.304 (2.304) data 0.246 (0.246) loss 0.4871 (0.4871) acc 87.5000 (87.5000) lr 1.0628e-03 eta 0:11:49
epoch [98/200] batch [2/3] time 2.052 (2.178) data 0.000 (0.123) loss 0.1299 (0.3085) acc 100.0000 (93.7500) lr 1.0628e-03 eta 0:11:08
epoch [98/200] batch [3/3] time 2.058 (2.138) data 0.000 (0.082) loss 0.4939 (0.3703) acc 90.6250 (92.7083) lr 1.0471e-03 eta 0:10:54
epoch [99/200] batch [1/3] time 2.299 (2.299) data 0.251 (0.251) loss 0.2869 (0.2869) acc 90.6250 (90.6250) lr 1.0471e-03 eta 0:11:41
epoch [99/200] batch [2/3] time 2.052 (2.176) data 0.000 (0.126) loss 0.3467 (0.3168) acc 93.7500 (92.1875) lr 1.0471e-03 eta 0:11:01
epoch [99/200] batch [3/3] time 2.047 (2.133) data 0.000 (0.084) loss 0.5000 (0.3778) acc 90.6250 (91.6667) lr 1.0314e-03 eta 0:10:46
epoch [100/200] batch [1/3] time 2.313 (2.313) data 0.250 (0.250) loss 0.2059 (0.2059) acc 96.8750 (96.8750) lr 1.0314e-03 eta 0:11:38
epoch [100/200] batch [2/3] time 2.041 (2.177) data 0.000 (0.125) loss 0.1761 (0.1910) acc 96.8750 (96.8750) lr 1.0314e-03 eta 0:10:55
epoch [100/200] batch [3/3] time 2.059 (2.138) data 0.000 (0.083) loss 0.4961 (0.2927) acc 93.7500 (95.8333) lr 1.0157e-03 eta 0:10:41
epoch [101/200] batch [1/3] time 2.325 (2.325) data 0.259 (0.259) loss 0.3188 (0.3188) acc 96.8750 (96.8750) lr 1.0157e-03 eta 0:11:35
epoch [101/200] batch [2/3] time 2.031 (2.178) data 0.000 (0.129) loss 0.0771 (0.1980) acc 100.0000 (98.4375) lr 1.0157e-03 eta 0:10:49
epoch [101/200] batch [3/3] time 2.054 (2.137) data 0.000 (0.086) loss 0.2144 (0.2035) acc 96.8750 (97.9167) lr 1.0000e-03 eta 0:10:34
epoch [102/200] batch [1/3] time 2.300 (2.300) data 0.262 (0.262) loss 0.2383 (0.2383) acc 90.6250 (90.6250) lr 1.0000e-03 eta 0:11:20
epoch [102/200] batch [2/3] time 2.070 (2.185) data 0.000 (0.131) loss 0.7480 (0.4932) acc 87.5000 (89.0625) lr 1.0000e-03 eta 0:10:44
epoch [102/200] batch [3/3] time 2.050 (2.140) data 0.000 (0.087) loss 0.3486 (0.4450) acc 90.6250 (89.5833) lr 9.8429e-04 eta 0:10:29
epoch [103/200] batch [1/3] time 2.296 (2.296) data 0.262 (0.262) loss 0.6650 (0.6650) acc 84.3750 (84.3750) lr 9.8429e-04 eta 0:11:12
epoch [103/200] batch [2/3] time 2.041 (2.168) data 0.000 (0.131) loss 0.1027 (0.3839) acc 100.0000 (92.1875) lr 9.8429e-04 eta 0:10:33
epoch [103/200] batch [3/3] time 2.062 (2.133) data 0.000 (0.087) loss 0.2136 (0.3271) acc 93.7500 (92.7083) lr 9.6859e-04 eta 0:10:20
epoch [104/200] batch [1/3] time 2.317 (2.317) data 0.254 (0.254) loss 0.2737 (0.2737) acc 93.7500 (93.7500) lr 9.6859e-04 eta 0:11:11
epoch [104/200] batch [2/3] time 2.057 (2.187) data 0.000 (0.127) loss 0.1818 (0.2277) acc 96.8750 (95.3125) lr 9.6859e-04 eta 0:10:32
epoch [104/200] batch [3/3] time 2.058 (2.144) data 0.000 (0.085) loss 0.1873 (0.2142) acc 96.8750 (95.8333) lr 9.5289e-04 eta 0:10:17
epoch [105/200] batch [1/3] time 2.318 (2.318) data 0.262 (0.262) loss 0.2433 (0.2433) acc 93.7500 (93.7500) lr 9.5289e-04 eta 0:11:05
epoch [105/200] batch [2/3] time 2.062 (2.190) data 0.000 (0.131) loss 0.4138 (0.3286) acc 90.6250 (92.1875) lr 9.5289e-04 eta 0:10:26
epoch [105/200] batch [3/3] time 2.064 (2.148) data 0.000 (0.087) loss 0.4009 (0.3527) acc 90.6250 (91.6667) lr 9.3721e-04 eta 0:10:12
epoch [106/200] batch [1/3] time 2.304 (2.304) data 0.244 (0.244) loss 0.2480 (0.2480) acc 93.7500 (93.7500) lr 9.3721e-04 eta 0:10:54
epoch [106/200] batch [2/3] time 2.057 (2.180) data 0.000 (0.122) loss 0.2100 (0.2290) acc 96.8750 (95.3125) lr 9.3721e-04 eta 0:10:17
epoch [106/200] batch [3/3] time 2.045 (2.135) data 0.000 (0.082) loss 0.0879 (0.1820) acc 100.0000 (96.8750) lr 9.2154e-04 eta 0:10:02
epoch [107/200] batch [1/3] time 2.312 (2.312) data 0.250 (0.250) loss 0.2817 (0.2817) acc 93.7500 (93.7500) lr 9.2154e-04 eta 0:10:49
epoch [107/200] batch [2/3] time 2.061 (2.186) data 0.000 (0.125) loss 0.4297 (0.3557) acc 90.6250 (92.1875) lr 9.2154e-04 eta 0:10:12
epoch [107/200] batch [3/3] time 2.032 (2.135) data 0.000 (0.083) loss 0.1080 (0.2732) acc 100.0000 (94.7917) lr 9.0589e-04 eta 0:09:55
epoch [108/200] batch [1/3] time 2.315 (2.315) data 0.243 (0.243) loss 0.9165 (0.9165) acc 78.1250 (78.1250) lr 9.0589e-04 eta 0:10:43
epoch [108/200] batch [2/3] time 2.060 (2.187) data 0.000 (0.122) loss 0.2708 (0.5936) acc 93.7500 (85.9375) lr 9.0589e-04 eta 0:10:05
epoch [108/200] batch [3/3] time 2.055 (2.143) data 0.000 (0.081) loss 0.1877 (0.4583) acc 96.8750 (89.5833) lr 8.9027e-04 eta 0:09:51
epoch [109/200] batch [1/3] time 2.290 (2.290) data 0.243 (0.243) loss 0.3137 (0.3137) acc 93.7500 (93.7500) lr 8.9027e-04 eta 0:10:29
epoch [109/200] batch [2/3] time 2.047 (2.168) data 0.000 (0.121) loss 0.0766 (0.1952) acc 100.0000 (96.8750) lr 8.9027e-04 eta 0:09:54
epoch [109/200] batch [3/3] time 2.068 (2.135) data 0.000 (0.081) loss 0.5889 (0.3264) acc 81.2500 (91.6667) lr 8.7467e-04 eta 0:09:42
epoch [110/200] batch [1/3] time 2.308 (2.308) data 0.246 (0.246) loss 0.5176 (0.5176) acc 93.7500 (93.7500) lr 8.7467e-04 eta 0:10:27
epoch [110/200] batch [2/3] time 2.062 (2.185) data 0.000 (0.123) loss 0.3340 (0.4258) acc 96.8750 (95.3125) lr 8.7467e-04 eta 0:09:52
epoch [110/200] batch [3/3] time 2.049 (2.140) data 0.000 (0.082) loss 0.3455 (0.3990) acc 90.6250 (93.7500) lr 8.5910e-04 eta 0:09:37
epoch [111/200] batch [1/3] time 2.316 (2.316) data 0.255 (0.255) loss 0.4121 (0.4121) acc 90.6250 (90.6250) lr 8.5910e-04 eta 0:10:22
epoch [111/200] batch [2/3] time 2.054 (2.185) data 0.000 (0.128) loss 0.3215 (0.3668) acc 90.6250 (90.6250) lr 8.5910e-04 eta 0:09:45
epoch [111/200] batch [3/3] time 2.068 (2.146) data 0.000 (0.085) loss 0.5420 (0.4252) acc 93.7500 (91.6667) lr 8.4357e-04 eta 0:09:32
epoch [112/200] batch [1/3] time 2.300 (2.300) data 0.243 (0.243) loss 0.3491 (0.3491) acc 96.8750 (96.8750) lr 8.4357e-04 eta 0:10:11
epoch [112/200] batch [2/3] time 2.057 (2.179) data 0.000 (0.122) loss 0.2573 (0.3032) acc 96.8750 (96.8750) lr 8.4357e-04 eta 0:09:37
epoch [112/200] batch [3/3] time 2.063 (2.140) data 0.000 (0.081) loss 0.3140 (0.3068) acc 96.8750 (96.8750) lr 8.2807e-04 eta 0:09:24
epoch [113/200] batch [1/3] time 2.312 (2.312) data 0.259 (0.259) loss 0.2147 (0.2147) acc 93.7500 (93.7500) lr 8.2807e-04 eta 0:10:08
epoch [113/200] batch [2/3] time 2.036 (2.174) data 0.000 (0.130) loss 0.1613 (0.1880) acc 96.8750 (95.3125) lr 8.2807e-04 eta 0:09:29
epoch [113/200] batch [3/3] time 2.044 (2.130) data 0.000 (0.086) loss 0.2776 (0.2179) acc 93.7500 (94.7917) lr 8.1262e-04 eta 0:09:16
epoch [114/200] batch [1/3] time 2.315 (2.315) data 0.260 (0.260) loss 0.3696 (0.3696) acc 93.7500 (93.7500) lr 8.1262e-04 eta 0:10:01
epoch [114/200] batch [2/3] time 2.054 (2.185) data 0.000 (0.130) loss 0.1345 (0.2521) acc 96.8750 (95.3125) lr 8.1262e-04 eta 0:09:25
epoch [114/200] batch [3/3] time 2.063 (2.144) data 0.000 (0.087) loss 0.4753 (0.3265) acc 87.5000 (92.7083) lr 7.9721e-04 eta 0:09:13
epoch [115/200] batch [1/3] time 2.289 (2.289) data 0.253 (0.253) loss 0.0942 (0.0942) acc 96.8750 (96.8750) lr 7.9721e-04 eta 0:09:48
epoch [115/200] batch [2/3] time 2.051 (2.170) data 0.000 (0.127) loss 0.2754 (0.1848) acc 90.6250 (93.7500) lr 7.9721e-04 eta 0:09:15
epoch [115/200] batch [3/3] time 2.048 (2.129) data 0.000 (0.085) loss 0.1412 (0.1703) acc 100.0000 (95.8333) lr 7.8186e-04 eta 0:09:02
epoch [116/200] batch [1/3] time 2.309 (2.309) data 0.260 (0.260) loss 0.2756 (0.2756) acc 93.7500 (93.7500) lr 7.8186e-04 eta 0:09:46
epoch [116/200] batch [2/3] time 2.065 (2.187) data 0.000 (0.130) loss 0.3989 (0.3373) acc 90.6250 (92.1875) lr 7.8186e-04 eta 0:09:13
epoch [116/200] batch [3/3] time 2.054 (2.143) data 0.000 (0.087) loss 0.3176 (0.3307) acc 93.7500 (92.7083) lr 7.6655e-04 eta 0:08:59
epoch [117/200] batch [1/3] time 2.300 (2.300) data 0.245 (0.245) loss 0.2708 (0.2708) acc 96.8750 (96.8750) lr 7.6655e-04 eta 0:09:37
epoch [117/200] batch [2/3] time 2.065 (2.182) data 0.000 (0.123) loss 0.4561 (0.3634) acc 87.5000 (92.1875) lr 7.6655e-04 eta 0:09:05
epoch [117/200] batch [3/3] time 2.071 (2.145) data 0.000 (0.082) loss 0.3081 (0.3450) acc 96.8750 (93.7500) lr 7.5131e-04 eta 0:08:54
epoch [118/200] batch [1/3] time 2.314 (2.314) data 0.260 (0.260) loss 0.2551 (0.2551) acc 90.6250 (90.6250) lr 7.5131e-04 eta 0:09:33
epoch [118/200] batch [2/3] time 2.052 (2.183) data 0.000 (0.130) loss 0.2349 (0.2450) acc 96.8750 (93.7500) lr 7.5131e-04 eta 0:08:59
epoch [118/200] batch [3/3] time 2.053 (2.139) data 0.000 (0.087) loss 0.2296 (0.2399) acc 93.7500 (93.7500) lr 7.3613e-04 eta 0:08:46
epoch [119/200] batch [1/3] time 2.291 (2.291) data 0.246 (0.246) loss 0.2681 (0.2681) acc 96.8750 (96.8750) lr 7.3613e-04 eta 0:09:21
epoch [119/200] batch [2/3] time 2.062 (2.177) data 0.000 (0.123) loss 0.3975 (0.3328) acc 96.8750 (96.8750) lr 7.3613e-04 eta 0:08:51
epoch [119/200] batch [3/3] time 2.064 (2.139) data 0.000 (0.082) loss 0.5347 (0.4001) acc 84.3750 (92.7083) lr 7.2101e-04 eta 0:08:39
epoch [120/200] batch [1/3] time 2.305 (2.305) data 0.245 (0.245) loss 0.3367 (0.3367) acc 93.7500 (93.7500) lr 7.2101e-04 eta 0:09:17
epoch [120/200] batch [2/3] time 2.055 (2.180) data 0.000 (0.123) loss 0.1680 (0.2523) acc 100.0000 (96.8750) lr 7.2101e-04 eta 0:08:45
epoch [120/200] batch [3/3] time 2.042 (2.134) data 0.000 (0.082) loss 0.1161 (0.2069) acc 96.8750 (96.8750) lr 7.0596e-04 eta 0:08:32
epoch [121/200] batch [1/3] time 2.307 (2.307) data 0.245 (0.245) loss 0.3315 (0.3315) acc 90.6250 (90.6250) lr 7.0596e-04 eta 0:09:11
epoch [121/200] batch [2/3] time 2.056 (2.182) data 0.000 (0.122) loss 0.3376 (0.3346) acc 87.5000 (89.0625) lr 7.0596e-04 eta 0:08:39
epoch [121/200] batch [3/3] time 2.055 (2.139) data 0.000 (0.082) loss 0.2976 (0.3223) acc 90.6250 (89.5833) lr 6.9098e-04 eta 0:08:27
epoch [122/200] batch [1/3] time 2.303 (2.303) data 0.244 (0.244) loss 0.2886 (0.2886) acc 93.7500 (93.7500) lr 6.9098e-04 eta 0:09:03
epoch [122/200] batch [2/3] time 2.055 (2.179) data 0.000 (0.122) loss 0.2239 (0.2562) acc 93.7500 (93.7500) lr 6.9098e-04 eta 0:08:32
epoch [122/200] batch [3/3] time 2.045 (2.134) data 0.000 (0.081) loss 0.1185 (0.2103) acc 100.0000 (95.8333) lr 6.7608e-04 eta 0:08:19
epoch [123/200] batch [1/3] time 2.305 (2.305) data 0.250 (0.250) loss 0.2937 (0.2937) acc 93.7500 (93.7500) lr 6.7608e-04 eta 0:08:56
epoch [123/200] batch [2/3] time 2.053 (2.179) data 0.000 (0.125) loss 0.2279 (0.2608) acc 96.8750 (95.3125) lr 6.7608e-04 eta 0:08:25
epoch [123/200] batch [3/3] time 2.050 (2.136) data 0.000 (0.083) loss 0.1182 (0.2133) acc 100.0000 (96.8750) lr 6.6126e-04 eta 0:08:13
epoch [124/200] batch [1/3] time 2.308 (2.308) data 0.244 (0.244) loss 0.2491 (0.2491) acc 96.8750 (96.8750) lr 6.6126e-04 eta 0:08:50
epoch [124/200] batch [2/3] time 2.072 (2.190) data 0.000 (0.122) loss 0.3584 (0.3038) acc 93.7500 (95.3125) lr 6.6126e-04 eta 0:08:21
epoch [124/200] batch [3/3] time 2.045 (2.142) data 0.000 (0.081) loss 0.1394 (0.2490) acc 96.8750 (95.8333) lr 6.4653e-04 eta 0:08:08
epoch [125/200] batch [1/3] time 2.294 (2.294) data 0.245 (0.245) loss 0.1672 (0.1672) acc 100.0000 (100.0000) lr 6.4653e-04 eta 0:08:40
epoch [125/200] batch [2/3] time 2.056 (2.175) data 0.000 (0.123) loss 0.3047 (0.2360) acc 93.7500 (96.8750) lr 6.4653e-04 eta 0:08:11
epoch [125/200] batch [3/3] time 2.032 (2.127) data 0.000 (0.082) loss 0.0796 (0.1838) acc 100.0000 (97.9167) lr 6.3188e-04 eta 0:07:58
epoch [126/200] batch [1/3] time 2.280 (2.280) data 0.252 (0.252) loss 0.1958 (0.1958) acc 96.8750 (96.8750) lr 6.3188e-04 eta 0:08:30
epoch [126/200] batch [2/3] time 2.055 (2.167) data 0.000 (0.126) loss 0.1560 (0.1759) acc 100.0000 (98.4375) lr 6.3188e-04 eta 0:08:03
epoch [126/200] batch [3/3] time 2.021 (2.118) data 0.000 (0.084) loss 0.0429 (0.1316) acc 100.0000 (98.9583) lr 6.1732e-04 eta 0:07:50
epoch [127/200] batch [1/3] time 2.296 (2.296) data 0.262 (0.262) loss 0.1179 (0.1179) acc 93.7500 (93.7500) lr 6.1732e-04 eta 0:08:27
epoch [127/200] batch [2/3] time 2.065 (2.181) data 0.000 (0.131) loss 0.3450 (0.2314) acc 93.7500 (93.7500) lr 6.1732e-04 eta 0:07:59
epoch [127/200] batch [3/3] time 2.064 (2.142) data 0.000 (0.087) loss 0.2756 (0.2462) acc 93.7500 (93.7500) lr 6.0285e-04 eta 0:07:49
epoch [128/200] batch [1/3] time 2.290 (2.290) data 0.258 (0.258) loss 0.0938 (0.0938) acc 100.0000 (100.0000) lr 6.0285e-04 eta 0:08:19
epoch [128/200] batch [2/3] time 2.029 (2.160) data 0.000 (0.129) loss 0.0666 (0.0802) acc 100.0000 (100.0000) lr 6.0285e-04 eta 0:07:48
epoch [128/200] batch [3/3] time 2.062 (2.127) data 0.000 (0.086) loss 0.3110 (0.1571) acc 93.7500 (97.9167) lr 5.8849e-04 eta 0:07:39
epoch [129/200] batch [1/3] time 2.307 (2.307) data 0.260 (0.260) loss 0.2153 (0.2153) acc 93.7500 (93.7500) lr 5.8849e-04 eta 0:08:15
epoch [129/200] batch [2/3] time 2.063 (2.185) data 0.000 (0.130) loss 0.2502 (0.2328) acc 96.8750 (95.3125) lr 5.8849e-04 eta 0:07:47
epoch [129/200] batch [3/3] time 2.051 (2.140) data 0.000 (0.087) loss 0.1901 (0.2185) acc 96.8750 (95.8333) lr 5.7422e-04 eta 0:07:35
epoch [130/200] batch [1/3] time 2.307 (2.307) data 0.254 (0.254) loss 0.2939 (0.2939) acc 90.6250 (90.6250) lr 5.7422e-04 eta 0:08:09
epoch [130/200] batch [2/3] time 2.059 (2.183) data 0.000 (0.127) loss 0.4531 (0.3735) acc 93.7500 (92.1875) lr 5.7422e-04 eta 0:07:40
epoch [130/200] batch [3/3] time 2.057 (2.141) data 0.000 (0.085) loss 0.2930 (0.3467) acc 93.7500 (92.7083) lr 5.6006e-04 eta 0:07:29
epoch [131/200] batch [1/3] time 2.320 (2.320) data 0.262 (0.262) loss 0.3677 (0.3677) acc 90.6250 (90.6250) lr 5.6006e-04 eta 0:08:04
epoch [131/200] batch [2/3] time 2.060 (2.190) data 0.000 (0.131) loss 0.2776 (0.3226) acc 96.8750 (93.7500) lr 5.6006e-04 eta 0:07:35
epoch [131/200] batch [3/3] time 2.052 (2.144) data 0.000 (0.087) loss 0.3008 (0.3153) acc 93.7500 (93.7500) lr 5.4601e-04 eta 0:07:23
epoch [132/200] batch [1/3] time 2.307 (2.307) data 0.251 (0.251) loss 0.3823 (0.3823) acc 90.6250 (90.6250) lr 5.4601e-04 eta 0:07:55
epoch [132/200] batch [2/3] time 2.070 (2.189) data 0.000 (0.126) loss 0.6865 (0.5344) acc 84.3750 (87.5000) lr 5.4601e-04 eta 0:07:28
epoch [132/200] batch [3/3] time 2.051 (2.143) data 0.000 (0.084) loss 0.1432 (0.4040) acc 96.8750 (90.6250) lr 5.3207e-04 eta 0:07:17
epoch [133/200] batch [1/3] time 2.308 (2.308) data 0.245 (0.245) loss 0.3601 (0.3601) acc 87.5000 (87.5000) lr 5.3207e-04 eta 0:07:48
epoch [133/200] batch [2/3] time 2.053 (2.181) data 0.000 (0.123) loss 0.2781 (0.3191) acc 90.6250 (89.0625) lr 5.3207e-04 eta 0:07:20
epoch [133/200] batch [3/3] time 2.066 (2.142) data 0.000 (0.082) loss 0.3079 (0.3153) acc 93.7500 (90.6250) lr 5.1825e-04 eta 0:07:10
epoch [134/200] batch [1/3] time 2.325 (2.325) data 0.261 (0.261) loss 0.3120 (0.3120) acc 90.6250 (90.6250) lr 5.1825e-04 eta 0:07:44
epoch [134/200] batch [2/3] time 2.064 (2.194) data 0.000 (0.131) loss 0.2839 (0.2980) acc 93.7500 (92.1875) lr 5.1825e-04 eta 0:07:16
epoch [134/200] batch [3/3] time 2.050 (2.146) data 0.000 (0.087) loss 0.1638 (0.2533) acc 96.8750 (93.7500) lr 5.0454e-04 eta 0:07:04
epoch [135/200] batch [1/3] time 2.316 (2.316) data 0.259 (0.259) loss 0.2974 (0.2974) acc 93.7500 (93.7500) lr 5.0454e-04 eta 0:07:36
epoch [135/200] batch [2/3] time 2.056 (2.186) data 0.000 (0.129) loss 0.2954 (0.2964) acc 96.8750 (95.3125) lr 5.0454e-04 eta 0:07:08
epoch [135/200] batch [3/3] time 2.053 (2.142) data 0.000 (0.086) loss 0.1805 (0.2578) acc 96.8750 (95.8333) lr 4.9096e-04 eta 0:06:57
epoch [136/200] batch [1/3] time 2.313 (2.313) data 0.261 (0.261) loss 0.2075 (0.2075) acc 96.8750 (96.8750) lr 4.9096e-04 eta 0:07:28
epoch [136/200] batch [2/3] time 2.021 (2.167) data 0.000 (0.131) loss 0.2247 (0.2161) acc 93.7500 (95.3125) lr 4.9096e-04 eta 0:06:58
epoch [136/200] batch [3/3] time 2.054 (2.129) data 0.000 (0.087) loss 0.3218 (0.2513) acc 93.7500 (94.7917) lr 4.7750e-04 eta 0:06:48
epoch [137/200] batch [1/3] time 2.315 (2.315) data 0.253 (0.253) loss 0.2069 (0.2069) acc 93.7500 (93.7500) lr 4.7750e-04 eta 0:07:22
epoch [137/200] batch [2/3] time 2.052 (2.183) data 0.000 (0.127) loss 0.1907 (0.1988) acc 93.7500 (93.7500) lr 4.7750e-04 eta 0:06:54
epoch [137/200] batch [3/3] time 2.053 (2.140) data 0.000 (0.085) loss 0.3110 (0.2362) acc 93.7500 (93.7500) lr 4.6417e-04 eta 0:06:44
epoch [138/200] batch [1/3] time 2.299 (2.299) data 0.260 (0.260) loss 0.2668 (0.2668) acc 90.6250 (90.6250) lr 4.6417e-04 eta 0:07:12
epoch [138/200] batch [2/3] time 2.050 (2.174) data 0.000 (0.130) loss 0.2749 (0.2709) acc 93.7500 (92.1875) lr 4.6417e-04 eta 0:06:46
epoch [138/200] batch [3/3] time 2.051 (2.133) data 0.000 (0.087) loss 0.2267 (0.2561) acc 90.6250 (91.6667) lr 4.5098e-04 eta 0:06:36
epoch [139/200] batch [1/3] time 2.312 (2.312) data 0.263 (0.263) loss 0.2277 (0.2277) acc 93.7500 (93.7500) lr 4.5098e-04 eta 0:07:07
epoch [139/200] batch [2/3] time 2.063 (2.187) data 0.000 (0.131) loss 0.5859 (0.4068) acc 90.6250 (92.1875) lr 4.5098e-04 eta 0:06:42
epoch [139/200] batch [3/3] time 2.038 (2.137) data 0.000 (0.088) loss 0.1052 (0.3063) acc 96.8750 (93.7500) lr 4.3792e-04 eta 0:06:31
epoch [140/200] batch [1/3] time 2.319 (2.319) data 0.253 (0.253) loss 0.4443 (0.4443) acc 87.5000 (87.5000) lr 4.3792e-04 eta 0:07:02
epoch [140/200] batch [2/3] time 2.056 (2.188) data 0.000 (0.127) loss 0.2437 (0.3440) acc 93.7500 (90.6250) lr 4.3792e-04 eta 0:06:36
epoch [140/200] batch [3/3] time 2.052 (2.143) data 0.000 (0.084) loss 0.2260 (0.3046) acc 96.8750 (92.7083) lr 4.2499e-04 eta 0:06:25
epoch [141/200] batch [1/3] time 2.291 (2.291) data 0.246 (0.246) loss 0.1892 (0.1892) acc 96.8750 (96.8750) lr 4.2499e-04 eta 0:06:50
epoch [141/200] batch [2/3] time 2.060 (2.176) data 0.000 (0.123) loss 0.3977 (0.2935) acc 90.6250 (93.7500) lr 4.2499e-04 eta 0:06:27
epoch [141/200] batch [3/3] time 2.048 (2.133) data 0.000 (0.082) loss 0.2471 (0.2780) acc 90.6250 (92.7083) lr 4.1221e-04 eta 0:06:17
epoch [142/200] batch [1/3] time 2.306 (2.306) data 0.253 (0.253) loss 0.2024 (0.2024) acc 100.0000 (100.0000) lr 4.1221e-04 eta 0:06:45
epoch [142/200] batch [2/3] time 2.054 (2.180) data 0.000 (0.126) loss 0.1757 (0.1890) acc 100.0000 (100.0000) lr 4.1221e-04 eta 0:06:21
epoch [142/200] batch [3/3] time 2.038 (2.133) data 0.000 (0.084) loss 0.0682 (0.1487) acc 100.0000 (100.0000) lr 3.9958e-04 eta 0:06:11
epoch [143/200] batch [1/3] time 2.304 (2.304) data 0.251 (0.251) loss 0.2632 (0.2632) acc 87.5000 (87.5000) lr 3.9958e-04 eta 0:06:38
epoch [143/200] batch [2/3] time 2.055 (2.180) data 0.000 (0.126) loss 0.2124 (0.2378) acc 96.8750 (92.1875) lr 3.9958e-04 eta 0:06:14
epoch [143/200] batch [3/3] time 2.042 (2.134) data 0.000 (0.084) loss 0.1031 (0.1929) acc 100.0000 (94.7917) lr 3.8709e-04 eta 0:06:04
epoch [144/200] batch [1/3] time 2.300 (2.300) data 0.252 (0.252) loss 0.1181 (0.1181) acc 100.0000 (100.0000) lr 3.8709e-04 eta 0:06:31
epoch [144/200] batch [2/3] time 2.038 (2.169) data 0.000 (0.126) loss 0.1913 (0.1547) acc 96.8750 (98.4375) lr 3.8709e-04 eta 0:06:06
epoch [144/200] batch [3/3] time 2.050 (2.129) data 0.000 (0.084) loss 0.3298 (0.2131) acc 87.5000 (94.7917) lr 3.7476e-04 eta 0:05:57
epoch [145/200] batch [1/3] time 2.298 (2.298) data 0.252 (0.252) loss 0.2251 (0.2251) acc 90.6250 (90.6250) lr 3.7476e-04 eta 0:06:23
epoch [145/200] batch [2/3] time 2.055 (2.177) data 0.000 (0.126) loss 0.3306 (0.2778) acc 90.6250 (90.6250) lr 3.7476e-04 eta 0:06:01
epoch [145/200] batch [3/3] time 2.059 (2.137) data 0.000 (0.084) loss 0.6045 (0.3867) acc 87.5000 (89.5833) lr 3.6258e-04 eta 0:05:52
epoch [146/200] batch [1/3] time 2.288 (2.288) data 0.245 (0.245) loss 0.2844 (0.2844) acc 93.7500 (93.7500) lr 3.6258e-04 eta 0:06:15
epoch [146/200] batch [2/3] time 2.059 (2.173) data 0.000 (0.122) loss 0.2644 (0.2744) acc 96.8750 (95.3125) lr 3.6258e-04 eta 0:05:54
epoch [146/200] batch [3/3] time 2.051 (2.133) data 0.000 (0.082) loss 0.2212 (0.2567) acc 96.8750 (95.8333) lr 3.5055e-04 eta 0:05:45
epoch [147/200] batch [1/3] time 2.312 (2.312) data 0.263 (0.263) loss 0.2764 (0.2764) acc 93.7500 (93.7500) lr 3.5055e-04 eta 0:06:12
epoch [147/200] batch [2/3] time 2.049 (2.181) data 0.000 (0.131) loss 0.2257 (0.2510) acc 93.7500 (93.7500) lr 3.5055e-04 eta 0:05:48
epoch [147/200] batch [3/3] time 2.068 (2.143) data 0.000 (0.088) loss 0.7412 (0.4144) acc 78.1250 (88.5417) lr 3.3869e-04 eta 0:05:40
epoch [148/200] batch [1/3] time 2.311 (2.311) data 0.253 (0.253) loss 0.2434 (0.2434) acc 96.8750 (96.8750) lr 3.3869e-04 eta 0:06:05
epoch [148/200] batch [2/3] time 2.055 (2.183) data 0.000 (0.127) loss 0.2125 (0.2280) acc 93.7500 (95.3125) lr 3.3869e-04 eta 0:05:42
epoch [148/200] batch [3/3] time 2.053 (2.140) data 0.000 (0.084) loss 0.3015 (0.2525) acc 96.8750 (95.8333) lr 3.2699e-04 eta 0:05:33
epoch [149/200] batch [1/3] time 2.309 (2.309) data 0.243 (0.243) loss 0.6641 (0.6641) acc 78.1250 (78.1250) lr 3.2699e-04 eta 0:05:57
epoch [149/200] batch [2/3] time 2.047 (2.178) data 0.000 (0.121) loss 0.1400 (0.4020) acc 96.8750 (87.5000) lr 3.2699e-04 eta 0:05:35
epoch [149/200] batch [3/3] time 2.047 (2.135) data 0.000 (0.081) loss 0.1267 (0.3103) acc 100.0000 (91.6667) lr 3.1545e-04 eta 0:05:26
epoch [150/200] batch [1/3] time 2.288 (2.288) data 0.246 (0.246) loss 0.3301 (0.3301) acc 96.8750 (96.8750) lr 3.1545e-04 eta 0:05:47
epoch [150/200] batch [2/3] time 2.035 (2.162) data 0.000 (0.123) loss 0.0444 (0.1872) acc 100.0000 (98.4375) lr 3.1545e-04 eta 0:05:26
epoch [150/200] batch [3/3] time 2.034 (2.119) data 0.000 (0.082) loss 0.1103 (0.1616) acc 96.8750 (97.9167) lr 3.0409e-04 eta 0:05:17
epoch [151/200] batch [1/3] time 2.319 (2.319) data 0.253 (0.253) loss 0.4658 (0.4658) acc 84.3750 (84.3750) lr 3.0409e-04 eta 0:05:45
epoch [151/200] batch [2/3] time 2.056 (2.187) data 0.000 (0.127) loss 0.2649 (0.3654) acc 96.8750 (90.6250) lr 3.0409e-04 eta 0:05:23
epoch [151/200] batch [3/3] time 2.061 (2.145) data 0.000 (0.084) loss 0.3518 (0.3608) acc 93.7500 (91.6667) lr 2.9289e-04 eta 0:05:15
epoch [152/200] batch [1/3] time 2.291 (2.291) data 0.250 (0.250) loss 0.1414 (0.1414) acc 96.8750 (96.8750) lr 2.9289e-04 eta 0:05:34
epoch [152/200] batch [2/3] time 2.043 (2.167) data 0.000 (0.125) loss 0.2686 (0.2050) acc 90.6250 (93.7500) lr 2.9289e-04 eta 0:05:14
epoch [152/200] batch [3/3] time 2.053 (2.129) data 0.000 (0.083) loss 0.3274 (0.2458) acc 96.8750 (94.7917) lr 2.8187e-04 eta 0:05:06
epoch [153/200] batch [1/3] time 2.308 (2.308) data 0.252 (0.252) loss 0.3770 (0.3770) acc 93.7500 (93.7500) lr 2.8187e-04 eta 0:05:30
epoch [153/200] batch [2/3] time 2.068 (2.188) data 0.000 (0.126) loss 0.3279 (0.3524) acc 93.7500 (93.7500) lr 2.8187e-04 eta 0:05:10
epoch [153/200] batch [3/3] time 2.059 (2.145) data 0.000 (0.084) loss 0.3582 (0.3543) acc 90.6250 (92.7083) lr 2.7103e-04 eta 0:05:02
epoch [154/200] batch [1/3] time 2.306 (2.306) data 0.251 (0.251) loss 0.2216 (0.2216) acc 93.7500 (93.7500) lr 2.7103e-04 eta 0:05:22
epoch [154/200] batch [2/3] time 2.049 (2.178) data 0.000 (0.125) loss 0.3794 (0.3005) acc 93.7500 (93.7500) lr 2.7103e-04 eta 0:05:02
epoch [154/200] batch [3/3] time 2.063 (2.139) data 0.000 (0.084) loss 0.4512 (0.3507) acc 87.5000 (91.6667) lr 2.6037e-04 eta 0:04:55
epoch [155/200] batch [1/3] time 2.303 (2.303) data 0.259 (0.259) loss 0.0845 (0.0845) acc 100.0000 (100.0000) lr 2.6037e-04 eta 0:05:15
epoch [155/200] batch [2/3] time 2.036 (2.169) data 0.000 (0.129) loss 0.0684 (0.0764) acc 100.0000 (100.0000) lr 2.6037e-04 eta 0:04:55
epoch [155/200] batch [3/3] time 2.060 (2.133) data 0.000 (0.086) loss 0.3196 (0.1575) acc 90.6250 (96.8750) lr 2.4989e-04 eta 0:04:47
epoch [156/200] batch [1/3] time 2.284 (2.284) data 0.254 (0.254) loss 0.0942 (0.0942) acc 100.0000 (100.0000) lr 2.4989e-04 eta 0:05:06
epoch [156/200] batch [2/3] time 2.051 (2.167) data 0.000 (0.127) loss 0.1407 (0.1175) acc 96.8750 (98.4375) lr 2.4989e-04 eta 0:04:48
epoch [156/200] batch [3/3] time 2.049 (2.128) data 0.000 (0.085) loss 0.1677 (0.1342) acc 100.0000 (98.9583) lr 2.3959e-04 eta 0:04:40
epoch [157/200] batch [1/3] time 2.312 (2.312) data 0.260 (0.260) loss 0.2128 (0.2128) acc 93.7500 (93.7500) lr 2.3959e-04 eta 0:05:02
epoch [157/200] batch [2/3] time 2.061 (2.186) data 0.000 (0.130) loss 0.3225 (0.2676) acc 90.6250 (92.1875) lr 2.3959e-04 eta 0:04:44
epoch [157/200] batch [3/3] time 2.050 (2.141) data 0.000 (0.087) loss 0.1543 (0.2299) acc 96.8750 (93.7500) lr 2.2949e-04 eta 0:04:36
epoch [158/200] batch [1/3] time 2.297 (2.297) data 0.254 (0.254) loss 0.1743 (0.1743) acc 96.8750 (96.8750) lr 2.2949e-04 eta 0:04:54
epoch [158/200] batch [2/3] time 2.069 (2.183) data 0.000 (0.127) loss 0.6294 (0.4019) acc 87.5000 (92.1875) lr 2.2949e-04 eta 0:04:37
epoch [158/200] batch [3/3] time 2.037 (2.134) data 0.000 (0.085) loss 0.0846 (0.2961) acc 100.0000 (94.7917) lr 2.1957e-04 eta 0:04:28
epoch [159/200] batch [1/3] time 2.295 (2.295) data 0.243 (0.243) loss 0.1644 (0.1644) acc 96.8750 (96.8750) lr 2.1957e-04 eta 0:04:46
epoch [159/200] batch [2/3] time 2.062 (2.179) data 0.000 (0.121) loss 0.3491 (0.2568) acc 90.6250 (93.7500) lr 2.1957e-04 eta 0:04:30
epoch [159/200] batch [3/3] time 2.050 (2.136) data 0.000 (0.081) loss 0.1896 (0.2344) acc 93.7500 (93.7500) lr 2.0984e-04 eta 0:04:22
epoch [160/200] batch [1/3] time 2.308 (2.308) data 0.246 (0.246) loss 0.3325 (0.3325) acc 93.7500 (93.7500) lr 2.0984e-04 eta 0:04:41
epoch [160/200] batch [2/3] time 2.062 (2.185) data 0.000 (0.123) loss 0.2747 (0.3036) acc 93.7500 (93.7500) lr 2.0984e-04 eta 0:04:24
epoch [160/200] batch [3/3] time 2.038 (2.136) data 0.000 (0.082) loss 0.0961 (0.2344) acc 96.8750 (94.7917) lr 2.0032e-04 eta 0:04:16
epoch [161/200] batch [1/3] time 2.312 (2.312) data 0.261 (0.261) loss 0.2039 (0.2039) acc 96.8750 (96.8750) lr 2.0032e-04 eta 0:04:35
epoch [161/200] batch [2/3] time 2.038 (2.175) data 0.000 (0.131) loss 0.0975 (0.1507) acc 100.0000 (98.4375) lr 2.0032e-04 eta 0:04:16
epoch [161/200] batch [3/3] time 2.032 (2.127) data 0.000 (0.087) loss 0.2627 (0.1880) acc 93.7500 (96.8750) lr 1.9098e-04 eta 0:04:08
epoch [162/200] batch [1/3] time 2.320 (2.320) data 0.263 (0.263) loss 0.1311 (0.1311) acc 100.0000 (100.0000) lr 1.9098e-04 eta 0:04:29
epoch [162/200] batch [2/3] time 2.035 (2.178) data 0.000 (0.131) loss 0.0537 (0.0924) acc 100.0000 (100.0000) lr 1.9098e-04 eta 0:04:10
epoch [162/200] batch [3/3] time 2.032 (2.129) data 0.000 (0.088) loss 0.1039 (0.0963) acc 96.8750 (98.9583) lr 1.8185e-04 eta 0:04:02
epoch [163/200] batch [1/3] time 2.317 (2.317) data 0.260 (0.260) loss 0.1719 (0.1719) acc 100.0000 (100.0000) lr 1.8185e-04 eta 0:04:21
epoch [163/200] batch [2/3] time 2.059 (2.188) data 0.000 (0.130) loss 0.4436 (0.3077) acc 90.6250 (95.3125) lr 1.8185e-04 eta 0:04:05
epoch [163/200] batch [3/3] time 2.051 (2.142) data 0.000 (0.087) loss 0.1702 (0.2619) acc 100.0000 (96.8750) lr 1.7292e-04 eta 0:03:57
epoch [164/200] batch [1/3] time 2.323 (2.323) data 0.260 (0.260) loss 0.4185 (0.4185) acc 87.5000 (87.5000) lr 1.7292e-04 eta 0:04:15
epoch [164/200] batch [2/3] time 2.035 (2.179) data 0.000 (0.130) loss 0.0855 (0.2520) acc 100.0000 (93.7500) lr 1.7292e-04 eta 0:03:57
epoch [164/200] batch [3/3] time 2.057 (2.139) data 0.000 (0.087) loss 0.2971 (0.2670) acc 93.7500 (93.7500) lr 1.6419e-04 eta 0:03:50
epoch [165/200] batch [1/3] time 2.288 (2.288) data 0.253 (0.253) loss 0.1046 (0.1046) acc 96.8750 (96.8750) lr 1.6419e-04 eta 0:04:04
epoch [165/200] batch [2/3] time 2.064 (2.176) data 0.000 (0.126) loss 0.3005 (0.2026) acc 90.6250 (93.7500) lr 1.6419e-04 eta 0:03:50
epoch [165/200] batch [3/3] time 2.052 (2.135) data 0.000 (0.084) loss 0.1852 (0.1968) acc 93.7500 (93.7500) lr 1.5567e-04 eta 0:03:44
epoch [166/200] batch [1/3] time 2.304 (2.304) data 0.248 (0.248) loss 0.3472 (0.3472) acc 90.6250 (90.6250) lr 1.5567e-04 eta 0:03:59
epoch [166/200] batch [2/3] time 2.036 (2.170) data 0.000 (0.124) loss 0.1069 (0.2270) acc 100.0000 (95.3125) lr 1.5567e-04 eta 0:03:43
epoch [166/200] batch [3/3] time 2.062 (2.134) data 0.000 (0.083) loss 0.2825 (0.2455) acc 90.6250 (93.7500) lr 1.4736e-04 eta 0:03:37
epoch [167/200] batch [1/3] time 2.282 (2.282) data 0.264 (0.264) loss 0.1372 (0.1372) acc 96.8750 (96.8750) lr 1.4736e-04 eta 0:03:50
epoch [167/200] batch [2/3] time 2.058 (2.170) data 0.000 (0.132) loss 0.2458 (0.1915) acc 96.8750 (96.8750) lr 1.4736e-04 eta 0:03:36
epoch [167/200] batch [3/3] time 2.033 (2.124) data 0.000 (0.088) loss 0.0645 (0.1492) acc 100.0000 (97.9167) lr 1.3926e-04 eta 0:03:30
epoch [168/200] batch [1/3] time 2.307 (2.307) data 0.247 (0.247) loss 0.2391 (0.2391) acc 93.7500 (93.7500) lr 1.3926e-04 eta 0:03:46
epoch [168/200] batch [2/3] time 2.043 (2.175) data 0.000 (0.123) loss 0.1514 (0.1953) acc 100.0000 (96.8750) lr 1.3926e-04 eta 0:03:30
epoch [168/200] batch [3/3] time 2.061 (2.137) data 0.000 (0.082) loss 0.3191 (0.2365) acc 93.7500 (95.8333) lr 1.3137e-04 eta 0:03:25
epoch [169/200] batch [1/3] time 2.309 (2.309) data 0.259 (0.259) loss 0.1581 (0.1581) acc 96.8750 (96.8750) lr 1.3137e-04 eta 0:03:39
epoch [169/200] batch [2/3] time 2.048 (2.179) data 0.000 (0.129) loss 0.1962 (0.1771) acc 96.8750 (96.8750) lr 1.3137e-04 eta 0:03:24
epoch [169/200] batch [3/3] time 2.052 (2.136) data 0.000 (0.086) loss 0.2241 (0.1928) acc 93.7500 (95.8333) lr 1.2369e-04 eta 0:03:18
epoch [170/200] batch [1/3] time 2.277 (2.277) data 0.244 (0.244) loss 0.0855 (0.0855) acc 100.0000 (100.0000) lr 1.2369e-04 eta 0:03:29
epoch [170/200] batch [2/3] time 2.035 (2.156) data 0.000 (0.122) loss 0.1556 (0.1206) acc 93.7500 (96.8750) lr 1.2369e-04 eta 0:03:16
epoch [170/200] batch [3/3] time 2.045 (2.119) data 0.000 (0.081) loss 0.1141 (0.1184) acc 100.0000 (97.9167) lr 1.1623e-04 eta 0:03:10
epoch [171/200] batch [1/3] time 2.289 (2.289) data 0.247 (0.247) loss 0.1245 (0.1245) acc 96.8750 (96.8750) lr 1.1623e-04 eta 0:03:23
epoch [171/200] batch [2/3] time 2.020 (2.155) data 0.000 (0.124) loss 0.0382 (0.0813) acc 100.0000 (98.4375) lr 1.1623e-04 eta 0:03:09
epoch [171/200] batch [3/3] time 2.060 (2.123) data 0.000 (0.082) loss 0.3569 (0.1732) acc 96.8750 (97.9167) lr 1.0899e-04 eta 0:03:04
epoch [172/200] batch [1/3] time 2.321 (2.321) data 0.259 (0.259) loss 0.2715 (0.2715) acc 90.6250 (90.6250) lr 1.0899e-04 eta 0:03:19
epoch [172/200] batch [2/3] time 2.043 (2.182) data 0.000 (0.130) loss 0.2036 (0.2375) acc 96.8750 (93.7500) lr 1.0899e-04 eta 0:03:05
epoch [172/200] batch [3/3] time 2.052 (2.139) data 0.000 (0.086) loss 0.1523 (0.2091) acc 96.8750 (94.7917) lr 1.0197e-04 eta 0:02:59
epoch [173/200] batch [1/3] time 2.305 (2.305) data 0.244 (0.244) loss 0.4172 (0.4172) acc 90.6250 (90.6250) lr 1.0197e-04 eta 0:03:11
epoch [173/200] batch [2/3] time 2.044 (2.175) data 0.000 (0.122) loss 0.1394 (0.2783) acc 100.0000 (95.3125) lr 1.0197e-04 eta 0:02:58
epoch [173/200] batch [3/3] time 2.053 (2.134) data 0.000 (0.081) loss 0.2815 (0.2794) acc 93.7500 (94.7917) lr 9.5173e-05 eta 0:02:52
epoch [174/200] batch [1/3] time 2.320 (2.320) data 0.253 (0.253) loss 0.5049 (0.5049) acc 90.6250 (90.6250) lr 9.5173e-05 eta 0:03:05
epoch [174/200] batch [2/3] time 2.069 (2.194) data 0.000 (0.127) loss 0.3774 (0.4412) acc 90.6250 (90.6250) lr 9.5173e-05 eta 0:02:53
epoch [174/200] batch [3/3] time 2.047 (2.145) data 0.000 (0.084) loss 0.1428 (0.3417) acc 93.7500 (91.6667) lr 8.8597e-05 eta 0:02:47
epoch [175/200] batch [1/3] time 2.304 (2.304) data 0.262 (0.262) loss 0.0726 (0.0726) acc 100.0000 (100.0000) lr 8.8597e-05 eta 0:02:57
epoch [175/200] batch [2/3] time 2.055 (2.179) data 0.000 (0.131) loss 0.2820 (0.1773) acc 90.6250 (95.3125) lr 8.8597e-05 eta 0:02:45
epoch [175/200] batch [3/3] time 2.040 (2.133) data 0.000 (0.087) loss 0.2163 (0.1903) acc 96.8750 (95.8333) lr 8.2245e-05 eta 0:02:39
epoch [176/200] batch [1/3] time 2.306 (2.306) data 0.245 (0.245) loss 0.4053 (0.4053) acc 93.7500 (93.7500) lr 8.2245e-05 eta 0:02:50
epoch [176/200] batch [2/3] time 2.049 (2.177) data 0.000 (0.122) loss 0.2512 (0.3282) acc 96.8750 (95.3125) lr 8.2245e-05 eta 0:02:38
epoch [176/200] batch [3/3] time 2.067 (2.140) data 0.000 (0.082) loss 0.3281 (0.3282) acc 96.8750 (95.8333) lr 7.6120e-05 eta 0:02:34
epoch [177/200] batch [1/3] time 2.286 (2.286) data 0.245 (0.245) loss 0.1327 (0.1327) acc 100.0000 (100.0000) lr 7.6120e-05 eta 0:02:42
epoch [177/200] batch [2/3] time 2.048 (2.167) data 0.000 (0.122) loss 0.1934 (0.1630) acc 93.7500 (96.8750) lr 7.6120e-05 eta 0:02:31
epoch [177/200] batch [3/3] time 2.040 (2.124) data 0.000 (0.082) loss 0.0903 (0.1388) acc 96.8750 (96.8750) lr 7.0224e-05 eta 0:02:26
epoch [178/200] batch [1/3] time 2.296 (2.296) data 0.242 (0.242) loss 0.2258 (0.2258) acc 96.8750 (96.8750) lr 7.0224e-05 eta 0:02:36
epoch [178/200] batch [2/3] time 2.054 (2.175) data 0.000 (0.121) loss 0.1479 (0.1869) acc 96.8750 (96.8750) lr 7.0224e-05 eta 0:02:25
epoch [178/200] batch [3/3] time 2.059 (2.136) data 0.000 (0.081) loss 0.4380 (0.2706) acc 90.6250 (94.7917) lr 6.4556e-05 eta 0:02:21
epoch [179/200] batch [1/3] time 2.293 (2.293) data 0.253 (0.253) loss 0.3447 (0.3447) acc 93.7500 (93.7500) lr 6.4556e-05 eta 0:02:29
epoch [179/200] batch [2/3] time 2.064 (2.179) data 0.000 (0.126) loss 0.3125 (0.3286) acc 93.7500 (93.7500) lr 6.4556e-05 eta 0:02:19
epoch [179/200] batch [3/3] time 2.054 (2.137) data 0.000 (0.084) loss 0.2128 (0.2900) acc 96.8750 (94.7917) lr 5.9119e-05 eta 0:02:14
epoch [180/200] batch [1/3] time 2.305 (2.305) data 0.250 (0.250) loss 0.2698 (0.2698) acc 93.7500 (93.7500) lr 5.9119e-05 eta 0:02:22
epoch [180/200] batch [2/3] time 2.036 (2.171) data 0.000 (0.125) loss 0.1788 (0.2243) acc 96.8750 (95.3125) lr 5.9119e-05 eta 0:02:12
epoch [180/200] batch [3/3] time 2.028 (2.123) data 0.000 (0.083) loss 0.0967 (0.1818) acc 93.7500 (94.7917) lr 5.3915e-05 eta 0:02:07
epoch [181/200] batch [1/3] time 2.293 (2.293) data 0.245 (0.245) loss 0.2119 (0.2119) acc 96.8750 (96.8750) lr 5.3915e-05 eta 0:02:15
epoch [181/200] batch [2/3] time 2.066 (2.180) data 0.000 (0.123) loss 0.5083 (0.3601) acc 90.6250 (93.7500) lr 5.3915e-05 eta 0:02:06
epoch [181/200] batch [3/3] time 2.045 (2.135) data 0.000 (0.082) loss 0.2524 (0.3242) acc 96.8750 (94.7917) lr 4.8943e-05 eta 0:02:01
epoch [182/200] batch [1/3] time 2.253 (2.253) data 0.246 (0.246) loss 0.0431 (0.0431) acc 100.0000 (100.0000) lr 4.8943e-05 eta 0:02:06
epoch [182/200] batch [2/3] time 2.058 (2.155) data 0.000 (0.123) loss 0.1598 (0.1014) acc 96.8750 (98.4375) lr 4.8943e-05 eta 0:01:58
epoch [182/200] batch [3/3] time 2.048 (2.120) data 0.000 (0.082) loss 0.2959 (0.1662) acc 90.6250 (95.8333) lr 4.4207e-05 eta 0:01:54
epoch [183/200] batch [1/3] time 2.294 (2.294) data 0.245 (0.245) loss 0.2839 (0.2839) acc 93.7500 (93.7500) lr 4.4207e-05 eta 0:02:01
epoch [183/200] batch [2/3] time 2.057 (2.176) data 0.000 (0.123) loss 0.2954 (0.2897) acc 96.8750 (95.3125) lr 4.4207e-05 eta 0:01:53
epoch [183/200] batch [3/3] time 2.018 (2.123) data 0.000 (0.082) loss 0.0840 (0.2211) acc 100.0000 (96.8750) lr 3.9706e-05 eta 0:01:48
epoch [184/200] batch [1/3] time 2.289 (2.289) data 0.253 (0.253) loss 0.1591 (0.1591) acc 93.7500 (93.7500) lr 3.9706e-05 eta 0:01:54
epoch [184/200] batch [2/3] time 2.060 (2.175) data 0.000 (0.127) loss 0.2483 (0.2037) acc 93.7500 (93.7500) lr 3.9706e-05 eta 0:01:46
epoch [184/200] batch [3/3] time 2.051 (2.134) data 0.000 (0.085) loss 0.3123 (0.2399) acc 93.7500 (93.7500) lr 3.5443e-05 eta 0:01:42
epoch [185/200] batch [1/3] time 2.288 (2.288) data 0.246 (0.246) loss 0.0942 (0.0942) acc 96.8750 (96.8750) lr 3.5443e-05 eta 0:01:47
epoch [185/200] batch [2/3] time 2.028 (2.158) data 0.000 (0.123) loss 0.1934 (0.1438) acc 96.8750 (96.8750) lr 3.5443e-05 eta 0:01:39
epoch [185/200] batch [3/3] time 2.034 (2.117) data 0.000 (0.082) loss 0.0828 (0.1235) acc 100.0000 (97.9167) lr 3.1417e-05 eta 0:01:35
epoch [186/200] batch [1/3] time 2.313 (2.313) data 0.254 (0.254) loss 0.2600 (0.2600) acc 93.7500 (93.7500) lr 3.1417e-05 eta 0:01:41
epoch [186/200] batch [2/3] time 2.055 (2.184) data 0.000 (0.127) loss 0.2957 (0.2778) acc 90.6250 (92.1875) lr 3.1417e-05 eta 0:01:33
epoch [186/200] batch [3/3] time 2.059 (2.142) data 0.000 (0.085) loss 0.2072 (0.2543) acc 96.8750 (93.7500) lr 2.7630e-05 eta 0:01:29
epoch [187/200] batch [1/3] time 2.300 (2.300) data 0.245 (0.245) loss 0.2617 (0.2617) acc 90.6250 (90.6250) lr 2.7630e-05 eta 0:01:34
epoch [187/200] batch [2/3] time 2.058 (2.179) data 0.000 (0.123) loss 0.2944 (0.2781) acc 93.7500 (92.1875) lr 2.7630e-05 eta 0:01:27
epoch [187/200] batch [3/3] time 2.050 (2.136) data 0.000 (0.082) loss 0.1794 (0.2452) acc 100.0000 (94.7917) lr 2.4083e-05 eta 0:01:23
epoch [188/200] batch [1/3] time 2.295 (2.295) data 0.252 (0.252) loss 0.3806 (0.3806) acc 93.7500 (93.7500) lr 2.4083e-05 eta 0:01:27
epoch [188/200] batch [2/3] time 2.030 (2.163) data 0.000 (0.126) loss 0.0620 (0.2213) acc 100.0000 (96.8750) lr 2.4083e-05 eta 0:01:20
epoch [188/200] batch [3/3] time 2.055 (2.127) data 0.000 (0.084) loss 0.3982 (0.2803) acc 87.5000 (93.7500) lr 2.0777e-05 eta 0:01:16
epoch [189/200] batch [1/3] time 2.308 (2.308) data 0.256 (0.256) loss 0.2148 (0.2148) acc 93.7500 (93.7500) lr 2.0777e-05 eta 0:01:20
epoch [189/200] batch [2/3] time 2.061 (2.184) data 0.000 (0.128) loss 0.2026 (0.2087) acc 96.8750 (95.3125) lr 2.0777e-05 eta 0:01:14
epoch [189/200] batch [3/3] time 2.065 (2.145) data 0.000 (0.085) loss 0.6406 (0.3527) acc 87.5000 (92.7083) lr 1.7713e-05 eta 0:01:10
epoch [190/200] batch [1/3] time 2.325 (2.325) data 0.266 (0.266) loss 0.2378 (0.2378) acc 96.8750 (96.8750) lr 1.7713e-05 eta 0:01:14
epoch [190/200] batch [2/3] time 2.050 (2.188) data 0.000 (0.133) loss 0.2329 (0.2354) acc 93.7500 (95.3125) lr 1.7713e-05 eta 0:01:07
epoch [190/200] batch [3/3] time 2.048 (2.141) data 0.000 (0.089) loss 0.1743 (0.2150) acc 93.7500 (94.7917) lr 1.4891e-05 eta 0:01:04
epoch [191/200] batch [1/3] time 2.295 (2.295) data 0.244 (0.244) loss 0.1595 (0.1595) acc 100.0000 (100.0000) lr 1.4891e-05 eta 0:01:06
epoch [191/200] batch [2/3] time 2.035 (2.165) data 0.000 (0.122) loss 0.1865 (0.1730) acc 96.8750 (98.4375) lr 1.4891e-05 eta 0:01:00
epoch [191/200] batch [3/3] time 2.044 (2.125) data 0.000 (0.081) loss 0.2961 (0.2141) acc 96.8750 (97.9167) lr 1.2312e-05 eta 0:00:57
epoch [192/200] batch [1/3] time 2.306 (2.306) data 0.251 (0.251) loss 0.1914 (0.1914) acc 96.8750 (96.8750) lr 1.2312e-05 eta 0:00:59
epoch [192/200] batch [2/3] time 2.038 (2.172) data 0.000 (0.126) loss 0.1134 (0.1524) acc 96.8750 (96.8750) lr 1.2312e-05 eta 0:00:54
epoch [192/200] batch [3/3] time 2.065 (2.136) data 0.000 (0.084) loss 0.4373 (0.2474) acc 90.6250 (94.7917) lr 9.9763e-06 eta 0:00:51
epoch [193/200] batch [1/3] time 2.298 (2.298) data 0.253 (0.253) loss 0.2494 (0.2494) acc 96.8750 (96.8750) lr 9.9763e-06 eta 0:00:52
epoch [193/200] batch [2/3] time 2.051 (2.174) data 0.000 (0.126) loss 0.1985 (0.2239) acc 90.6250 (93.7500) lr 9.9763e-06 eta 0:00:47
epoch [193/200] batch [3/3] time 2.041 (2.130) data 0.000 (0.084) loss 0.0878 (0.1786) acc 100.0000 (95.8333) lr 7.8853e-06 eta 0:00:44
epoch [194/200] batch [1/3] time 2.286 (2.286) data 0.243 (0.243) loss 0.1109 (0.1109) acc 100.0000 (100.0000) lr 7.8853e-06 eta 0:00:45
epoch [194/200] batch [2/3] time 2.068 (2.177) data 0.000 (0.122) loss 0.4976 (0.3042) acc 87.5000 (93.7500) lr 7.8853e-06 eta 0:00:41
epoch [194/200] batch [3/3] time 2.059 (2.138) data 0.000 (0.081) loss 0.5615 (0.3900) acc 93.7500 (93.7500) lr 6.0390e-06 eta 0:00:38
epoch [195/200] batch [1/3] time 2.298 (2.298) data 0.243 (0.243) loss 0.3293 (0.3293) acc 93.7500 (93.7500) lr 6.0390e-06 eta 0:00:39
epoch [195/200] batch [2/3] time 2.030 (2.164) data 0.000 (0.122) loss 0.0839 (0.2066) acc 100.0000 (96.8750) lr 6.0390e-06 eta 0:00:34
epoch [195/200] batch [3/3] time 2.036 (2.121) data 0.000 (0.081) loss 0.0813 (0.1648) acc 100.0000 (97.9167) lr 4.4380e-06 eta 0:00:31
epoch [196/200] batch [1/3] time 2.287 (2.287) data 0.255 (0.255) loss 0.2273 (0.2273) acc 96.8750 (96.8750) lr 4.4380e-06 eta 0:00:32
epoch [196/200] batch [2/3] time 2.051 (2.169) data 0.000 (0.128) loss 0.2101 (0.2187) acc 93.7500 (95.3125) lr 4.4380e-06 eta 0:00:28
epoch [196/200] batch [3/3] time 2.048 (2.128) data 0.000 (0.085) loss 0.3997 (0.2790) acc 90.6250 (93.7500) lr 3.0827e-06 eta 0:00:25
epoch [197/200] batch [1/3] time 2.318 (2.318) data 0.260 (0.260) loss 0.2094 (0.2094) acc 96.8750 (96.8750) lr 3.0827e-06 eta 0:00:25
epoch [197/200] batch [2/3] time 2.044 (2.181) data 0.000 (0.130) loss 0.1105 (0.1599) acc 100.0000 (98.4375) lr 3.0827e-06 eta 0:00:21
epoch [197/200] batch [3/3] time 2.050 (2.137) data 0.000 (0.087) loss 0.1641 (0.1613) acc 96.8750 (97.9167) lr 1.9733e-06 eta 0:00:19
epoch [198/200] batch [1/3] time 2.276 (2.276) data 0.246 (0.246) loss 0.1167 (0.1167) acc 96.8750 (96.8750) lr 1.9733e-06 eta 0:00:18
epoch [198/200] batch [2/3] time 2.055 (2.165) data 0.000 (0.123) loss 0.1927 (0.1547) acc 96.8750 (96.8750) lr 1.9733e-06 eta 0:00:15
epoch [198/200] batch [3/3] time 2.056 (2.129) data 0.000 (0.082) loss 0.2485 (0.1860) acc 93.7500 (95.8333) lr 1.1101e-06 eta 0:00:12
epoch [199/200] batch [1/3] time 2.294 (2.294) data 0.251 (0.251) loss 0.1138 (0.1138) acc 96.8750 (96.8750) lr 1.1101e-06 eta 0:00:11
epoch [199/200] batch [2/3] time 2.044 (2.169) data 0.000 (0.126) loss 0.1137 (0.1137) acc 96.8750 (96.8750) lr 1.1101e-06 eta 0:00:08
epoch [199/200] batch [3/3] time 2.040 (2.126) data 0.000 (0.084) loss 0.1027 (0.1101) acc 96.8750 (96.8750) lr 4.9344e-07 eta 0:00:06
epoch [200/200] batch [1/3] time 2.306 (2.306) data 0.255 (0.255) loss 0.1271 (0.1271) acc 100.0000 (100.0000) lr 4.9344e-07 eta 0:00:04
epoch [200/200] batch [2/3] time 2.053 (2.180) data 0.000 (0.127) loss 0.3525 (0.2398) acc 90.6250 (95.3125) lr 4.9344e-07 eta 0:00:02
epoch [200/200] batch [3/3] time 2.051 (2.137) data 0.000 (0.085) loss 0.1028 (0.1942) acc 100.0000 (96.8750) lr 1.2337e-07 eta 0:00:00
Checkpoint saved to output/Caltech/1/2/prompt_learner/model.pth.tar-200
Finish training
Deploy the last-epoch model
Evaluate on the *test* set
=> result
* total: 2,465
* correct: 1,997
* accuracy: 81.0%
* error: 19.0%
* macro_f1: 73.0%
Elapsed: 0:22:14
args2: backbone=, config_file=configs/trainers/CoOp/rn101.yaml, dataset_config_file=configs/datasets/caltech101.yaml, eval_only=False, head=, load_epoch=None, model_dir=, no_train=False,  opts: ['TRAINER.COOP.N_CTX', '16', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '1'], output_dir=output/Caltech/1/2/3, resume=, root=/home/brandnerkasper/Uni/MP/MP_CustomCoOp/data, seed=3, source_domains=None, target_domains=None, trainer=CoOp, transforms=None
Setting fixed seed: 3
***************
** Arguments **
***************
config_file: configs/trainers/CoOp/rn101.yaml
csc: False
ctp: end
dataset_config_file: configs/datasets/caltech101.yaml
n_ctx: 16
opts: ['TRAINER.COOP.N_CTX', '16', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '1']
output_dir: output/Caltech/1/2/3
root: /home/brandnerkasper/Uni/MP/MP_CustomCoOp/data
seed: 3
shots: 1
trainer: CoOp
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 4
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: Caltech101
  NUM_LABELED: -1
  NUM_SHOTS: 1
  ROOT: /home/brandnerkasper/Uni/MP/MP_CustomCoOp/data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN101
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 200
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/Caltech/1/2/3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: 
    N_CTX: 16
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: 
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: CoOp
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 2.0.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 22.04.3 LTS (x86_64)
GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.35

Python version: 3.10.12 (main, Jul  5 2023, 18:54:27) [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-6.2.0-32-generic-x86_64-with-glibc2.35
Is CUDA available: True
CUDA runtime version: 11.5.119
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA GeForce GTX 970
Nvidia driver version: 525.125.06
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Architecture:                       x86_64
CPU op-mode(s):                     32-bit, 64-bit
Address sizes:                      39 bits physical, 48 bits virtual
Byte Order:                         Little Endian
CPU(s):                             4
On-line CPU(s) list:                0-3
Vendor ID:                          GenuineIntel
Model name:                         Intel(R) Xeon(R) CPU E3-1225 v3 @ 3.20GHz
CPU family:                         6
Model:                              60
Thread(s) per core:                 1
Core(s) per socket:                 4
Socket(s):                          1
Stepping:                           3
CPU max MHz:                        3600,0000
CPU min MHz:                        800,0000
BogoMIPS:                           6398.04
Flags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm cpuid_fault invpcid_single pti ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid xsaveopt dtherm ida arat pln pts md_clear flush_l1d
Virtualization:                     VT-x
L1d cache:                          128 KiB (4 instances)
L1i cache:                          128 KiB (4 instances)
L2 cache:                           1 MiB (4 instances)
L3 cache:                           8 MiB (1 instance)
NUMA node(s):                       1
NUMA node0 CPU(s):                  0-3
Vulnerability Gather data sampling: Not affected
Vulnerability Itlb multihit:        KVM: Mitigation: VMX disabled
Vulnerability L1tf:                 Mitigation; PTE Inversion; VMX conditional cache flushes, SMT disabled
Vulnerability Mds:                  Mitigation; Clear CPU buffers; SMT disabled
Vulnerability Meltdown:             Mitigation; PTI
Vulnerability Mmio stale data:      Unknown: No mitigations
Vulnerability Retbleed:             Not affected
Vulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl
Vulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization
Vulnerability Spectre v2:           Mitigation; Retpolines, IBPB conditional, IBRS_FW, STIBP disabled, RSB filling, PBRSB-eIBRS Not affected
Vulnerability Srbds:                Mitigation; Microcode
Vulnerability Tsx async abort:      Not affected

Versions of relevant libraries:
[pip3] numpy==1.25.2
[pip3] open-clip-torch==2.20.0
[pip3] torch==2.0.1
[pip3] torchaudio==2.0.2
[pip3] torchvision==0.15.2
[conda] blas                      1.0                         mkl  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] mkl                       2023.1.0         h213fc3f_46343  
[conda] mkl-service               2.4.0           py310h5eee18b_1  
[conda] mkl_fft                   1.3.6           py310h1128e8f_1  
[conda] mkl_random                1.2.2           py310h1128e8f_1  
[conda] numpy                     1.25.2          py310h5f9d8c6_0  
[conda] numpy-base                1.25.2          py310hb5e798b_0  
[conda] open-clip-torch           2.20.0                   pypi_0    pypi
[conda] pytorch                   2.0.1           py3.10_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchaudio                2.0.2               py310_cu117    pytorch
[conda] torchtriton               2.0.0                     py310    pytorch
[conda] torchvision               0.15.2              py310_cu117    pytorch
        Pillow (9.4.0)

Loading trainer: CoOp
Loading dataset: Caltech101
Reading split from /home/brandnerkasper/Uni/MP/MP_CustomCoOp/data/caltech-101/split_zhou_Caltech101.json
Creating a 1-shot dataset
Creating a 1-shot dataset
Saving preprocessed few-shot data to /home/brandnerkasper/Uni/MP/MP_CustomCoOp/data/caltech-101/split_fewshot/shot_1-seed_3.pkl
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ----------
Dataset    Caltech101
# classes  100
# train_x  100
# val      100
# test     2,465
---------  ----------
Loading CLIP (backbone: RN101)
CLIP(
  (visual): ModifiedResNet(
    (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (act1): ReLU(inplace=True)
    (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (act2): ReLU(inplace=True)
    (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (act3): ReLU(inplace=True)
    (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (layer1): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
        (downsample): Sequential(
          (-1): AvgPool2d(kernel_size=1, stride=1, padding=0)
          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
      )
    )
    (layer2): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
        (downsample): Sequential(
          (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
      )
    )
    (layer3): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
        (downsample): Sequential(
          (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
      )
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
      )
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
      )
      (6): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
      )
      (7): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
      )
      (8): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
      )
      (9): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
      )
      (10): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
      )
      (11): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
      )
      (12): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
      )
      (13): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
      )
      (14): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
      )
      (15): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
      )
      (16): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
      )
      (17): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
      )
      (18): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
      )
      (19): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
      )
      (20): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
      )
      (21): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
      )
      (22): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
      )
    )
    (layer4): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
        (downsample): Sequential(
          (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
      )
    )
    (attnpool): AttentionPool2d(
      (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
      (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
      (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
      (c_proj): Linear(in_features=2048, out_features=512, bias=True)
    )
  )
  (transformer): Transformer(
    (resblocks): ModuleList(
      (0-11): 12 x ResidualAttentionBlock(
        (ln_1): LayerNormFp32((512,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNormFp32((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ls_2): Identity()
      )
    )
  )
  (token_embedding): Embedding(49408, 512)
  (ln_final): LayerNormFp32((512,), eps=1e-05, elementwise_affine=True)
)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
Turning off gradients in both the image and the text encoder
Loading evaluator: Classification
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/Caltech/1/2/3/tensorboard)
epoch [1/200] batch [1/3] time 2.369 (2.369) data 0.278 (0.278) loss 4.5664 (4.5664) acc 3.1250 (3.1250) lr 1.0000e-05 eta 0:23:39
epoch [1/200] batch [2/3] time 2.070 (2.219) data 0.000 (0.139) loss 4.6016 (4.5840) acc 3.1250 (3.1250) lr 1.0000e-05 eta 0:22:07
epoch [1/200] batch [3/3] time 2.071 (2.170) data 0.000 (0.093) loss 4.5664 (4.5781) acc 6.2500 (4.1667) lr 2.0000e-03 eta 0:21:35
epoch [2/200] batch [1/3] time 2.326 (2.326) data 0.256 (0.256) loss 4.5703 (4.5703) acc 3.1250 (3.1250) lr 2.0000e-03 eta 0:23:06
epoch [2/200] batch [2/3] time 2.070 (2.198) data 0.000 (0.128) loss 4.5508 (4.5605) acc 3.1250 (3.1250) lr 2.0000e-03 eta 0:21:47
epoch [2/200] batch [3/3] time 2.070 (2.155) data 0.000 (0.085) loss 4.5742 (4.5651) acc 3.1250 (3.1250) lr 1.9999e-03 eta 0:21:20
epoch [3/200] batch [1/3] time 2.337 (2.337) data 0.260 (0.260) loss 4.5820 (4.5820) acc 0.0000 (0.0000) lr 1.9999e-03 eta 0:23:05
epoch [3/200] batch [2/3] time 2.073 (2.205) data 0.000 (0.130) loss 4.5938 (4.5879) acc 3.1250 (1.5625) lr 1.9999e-03 eta 0:21:45
epoch [3/200] batch [3/3] time 2.073 (2.161) data 0.000 (0.087) loss 4.5117 (4.5625) acc 3.1250 (2.0833) lr 1.9995e-03 eta 0:21:17
epoch [4/200] batch [1/3] time 2.338 (2.338) data 0.267 (0.267) loss 4.5234 (4.5234) acc 9.3750 (9.3750) lr 1.9995e-03 eta 0:22:59
epoch [4/200] batch [2/3] time 2.075 (2.207) data 0.000 (0.133) loss 4.5352 (4.5293) acc 3.1250 (6.2500) lr 1.9995e-03 eta 0:21:39
epoch [4/200] batch [3/3] time 2.077 (2.164) data 0.000 (0.089) loss 4.5469 (4.5352) acc 3.1250 (5.2083) lr 1.9989e-03 eta 0:21:12
epoch [5/200] batch [1/3] time 2.335 (2.335) data 0.257 (0.257) loss 4.5625 (4.5625) acc 3.1250 (3.1250) lr 1.9989e-03 eta 0:22:50
epoch [5/200] batch [2/3] time 2.077 (2.206) data 0.000 (0.129) loss 4.4961 (4.5293) acc 9.3750 (6.2500) lr 1.9989e-03 eta 0:21:32
epoch [5/200] batch [3/3] time 2.078 (2.163) data 0.000 (0.086) loss 4.5039 (4.5208) acc 3.1250 (5.2083) lr 1.9980e-03 eta 0:21:05
epoch [6/200] batch [1/3] time 2.342 (2.342) data 0.266 (0.266) loss 4.4609 (4.4609) acc 6.2500 (6.2500) lr 1.9980e-03 eta 0:22:47
epoch [6/200] batch [2/3] time 2.077 (2.209) data 0.000 (0.133) loss 4.4414 (4.4512) acc 9.3750 (7.8125) lr 1.9980e-03 eta 0:21:28
epoch [6/200] batch [3/3] time 2.076 (2.165) data 0.000 (0.089) loss 4.4961 (4.4661) acc 3.1250 (6.2500) lr 1.9969e-03 eta 0:20:59
epoch [7/200] batch [1/3] time 2.345 (2.345) data 0.268 (0.268) loss 4.3750 (4.3750) acc 9.3750 (9.3750) lr 1.9969e-03 eta 0:22:42
epoch [7/200] batch [2/3] time 2.076 (2.211) data 0.000 (0.134) loss 4.4219 (4.3984) acc 6.2500 (7.8125) lr 1.9969e-03 eta 0:21:22
epoch [7/200] batch [3/3] time 2.079 (2.167) data 0.000 (0.089) loss 4.2422 (4.3464) acc 15.6250 (10.4167) lr 1.9956e-03 eta 0:20:54
epoch [8/200] batch [1/3] time 2.327 (2.327) data 0.250 (0.250) loss 4.2422 (4.2422) acc 0.0000 (0.0000) lr 1.9956e-03 eta 0:22:25
epoch [8/200] batch [2/3] time 2.079 (2.203) data 0.000 (0.125) loss 4.1602 (4.2012) acc 12.5000 (6.2500) lr 1.9956e-03 eta 0:21:11
epoch [8/200] batch [3/3] time 2.075 (2.160) data 0.000 (0.083) loss 3.9648 (4.1224) acc 15.6250 (9.3750) lr 1.9940e-03 eta 0:20:44
epoch [9/200] batch [1/3] time 2.325 (2.325) data 0.252 (0.252) loss 3.9297 (3.9297) acc 12.5000 (12.5000) lr 1.9940e-03 eta 0:22:16
epoch [9/200] batch [2/3] time 2.077 (2.201) data 0.000 (0.126) loss 3.7578 (3.8438) acc 15.6250 (14.0625) lr 1.9940e-03 eta 0:21:03
epoch [9/200] batch [3/3] time 2.076 (2.160) data 0.000 (0.084) loss 4.3047 (3.9974) acc 3.1250 (10.4167) lr 1.9921e-03 eta 0:20:37
epoch [10/200] batch [1/3] time 2.332 (2.332) data 0.258 (0.258) loss 3.8301 (3.8301) acc 21.8750 (21.8750) lr 1.9921e-03 eta 0:22:14
epoch [10/200] batch [2/3] time 2.077 (2.205) data 0.000 (0.129) loss 3.8281 (3.8291) acc 12.5000 (17.1875) lr 1.9921e-03 eta 0:20:58
epoch [10/200] batch [3/3] time 2.075 (2.161) data 0.000 (0.086) loss 3.5664 (3.7415) acc 25.0000 (19.7917) lr 1.9900e-03 eta 0:20:31
epoch [11/200] batch [1/3] time 2.329 (2.329) data 0.253 (0.253) loss 3.1211 (3.1211) acc 34.3750 (34.3750) lr 1.9900e-03 eta 0:22:05
epoch [11/200] batch [2/3] time 2.076 (2.202) data 0.000 (0.126) loss 2.9492 (3.0352) acc 31.2500 (32.8125) lr 1.9900e-03 eta 0:20:50
epoch [11/200] batch [3/3] time 2.078 (2.161) data 0.000 (0.084) loss 3.0801 (3.0501) acc 34.3750 (33.3333) lr 1.9877e-03 eta 0:20:25
epoch [12/200] batch [1/3] time 2.335 (2.335) data 0.259 (0.259) loss 2.9727 (2.9727) acc 34.3750 (34.3750) lr 1.9877e-03 eta 0:22:01
epoch [12/200] batch [2/3] time 2.078 (2.207) data 0.000 (0.130) loss 2.3691 (2.6709) acc 25.0000 (29.6875) lr 1.9877e-03 eta 0:20:46
epoch [12/200] batch [3/3] time 2.075 (2.163) data 0.000 (0.086) loss 2.6895 (2.6771) acc 34.3750 (31.2500) lr 1.9851e-03 eta 0:20:19
epoch [13/200] batch [1/3] time 2.326 (2.326) data 0.252 (0.252) loss 2.6035 (2.6035) acc 34.3750 (34.3750) lr 1.9851e-03 eta 0:21:49
epoch [13/200] batch [2/3] time 2.073 (2.200) data 0.000 (0.126) loss 2.4219 (2.5127) acc 50.0000 (42.1875) lr 1.9851e-03 eta 0:20:36
epoch [13/200] batch [3/3] time 2.074 (2.158) data 0.000 (0.084) loss 1.6289 (2.2181) acc 56.2500 (46.8750) lr 1.9823e-03 eta 0:20:10
epoch [14/200] batch [1/3] time 2.322 (2.322) data 0.252 (0.252) loss 2.0215 (2.0215) acc 43.7500 (43.7500) lr 1.9823e-03 eta 0:21:40
epoch [14/200] batch [2/3] time 2.076 (2.199) data 0.000 (0.126) loss 2.3242 (2.1729) acc 37.5000 (40.6250) lr 1.9823e-03 eta 0:20:29
epoch [14/200] batch [3/3] time 2.078 (2.159) data 0.000 (0.084) loss 2.0449 (2.1302) acc 53.1250 (44.7917) lr 1.9792e-03 eta 0:20:04
epoch [15/200] batch [1/3] time 2.322 (2.322) data 0.252 (0.252) loss 1.9736 (1.9736) acc 50.0000 (50.0000) lr 1.9792e-03 eta 0:21:33
epoch [15/200] batch [2/3] time 2.079 (2.201) data 0.000 (0.126) loss 2.0020 (1.9878) acc 43.7500 (46.8750) lr 1.9792e-03 eta 0:20:23
epoch [15/200] batch [3/3] time 2.074 (2.158) data 0.000 (0.084) loss 1.9766 (1.9840) acc 50.0000 (47.9167) lr 1.9759e-03 eta 0:19:57
epoch [16/200] batch [1/3] time 2.333 (2.333) data 0.258 (0.258) loss 2.0938 (2.0938) acc 50.0000 (50.0000) lr 1.9759e-03 eta 0:21:32
epoch [16/200] batch [2/3] time 2.078 (2.205) data 0.000 (0.129) loss 1.9551 (2.0244) acc 50.0000 (50.0000) lr 1.9759e-03 eta 0:20:19
epoch [16/200] batch [3/3] time 2.079 (2.163) data 0.000 (0.086) loss 1.6064 (1.8851) acc 59.3750 (53.1250) lr 1.9724e-03 eta 0:19:54
epoch [17/200] batch [1/3] time 2.334 (2.334) data 0.253 (0.253) loss 1.5586 (1.5586) acc 59.3750 (59.3750) lr 1.9724e-03 eta 0:21:26
epoch [17/200] batch [2/3] time 2.081 (2.207) data 0.000 (0.127) loss 1.0469 (1.3027) acc 75.0000 (67.1875) lr 1.9724e-03 eta 0:20:14
epoch [17/200] batch [3/3] time 2.083 (2.166) data 0.000 (0.085) loss 1.7588 (1.4548) acc 59.3750 (64.5833) lr 1.9686e-03 eta 0:19:49
epoch [18/200] batch [1/3] time 2.347 (2.347) data 0.269 (0.269) loss 1.5840 (1.5840) acc 59.3750 (59.3750) lr 1.9686e-03 eta 0:21:25
epoch [18/200] batch [2/3] time 2.076 (2.211) data 0.000 (0.135) loss 1.0625 (1.3232) acc 75.0000 (67.1875) lr 1.9686e-03 eta 0:20:09
epoch [18/200] batch [3/3] time 2.078 (2.167) data 0.000 (0.090) loss 1.1045 (1.2503) acc 62.5000 (65.6250) lr 1.9646e-03 eta 0:19:43
epoch [19/200] batch [1/3] time 2.337 (2.337) data 0.261 (0.261) loss 1.0879 (1.0879) acc 71.8750 (71.8750) lr 1.9646e-03 eta 0:21:13
epoch [19/200] batch [2/3] time 2.079 (2.208) data 0.000 (0.130) loss 1.0928 (1.0903) acc 71.8750 (71.8750) lr 1.9646e-03 eta 0:20:01
epoch [19/200] batch [3/3] time 2.081 (2.166) data 0.000 (0.087) loss 1.3945 (1.1917) acc 62.5000 (68.7500) lr 1.9603e-03 eta 0:19:35
epoch [20/200] batch [1/3] time 2.342 (2.342) data 0.260 (0.260) loss 1.1904 (1.1904) acc 65.6250 (65.6250) lr 1.9603e-03 eta 0:21:09
epoch [20/200] batch [2/3] time 2.080 (2.211) data 0.000 (0.130) loss 0.9712 (1.0808) acc 59.3750 (62.5000) lr 1.9603e-03 eta 0:19:56
epoch [20/200] batch [3/3] time 2.080 (2.167) data 0.000 (0.087) loss 1.2490 (1.1369) acc 65.6250 (63.5417) lr 1.9558e-03 eta 0:19:30
epoch [21/200] batch [1/3] time 2.327 (2.327) data 0.250 (0.250) loss 0.8164 (0.8164) acc 81.2500 (81.2500) lr 1.9558e-03 eta 0:20:54
epoch [21/200] batch [2/3] time 2.076 (2.201) data 0.000 (0.125) loss 1.0088 (0.9126) acc 78.1250 (79.6875) lr 1.9558e-03 eta 0:19:44
epoch [21/200] batch [3/3] time 2.083 (2.162) data 0.000 (0.083) loss 0.9683 (0.9312) acc 81.2500 (80.2083) lr 1.9511e-03 eta 0:19:21
epoch [22/200] batch [1/3] time 2.328 (2.328) data 0.251 (0.251) loss 0.8940 (0.8940) acc 78.1250 (78.1250) lr 1.9511e-03 eta 0:20:47
epoch [22/200] batch [2/3] time 2.082 (2.205) data 0.000 (0.125) loss 0.7256 (0.8098) acc 87.5000 (82.8125) lr 1.9511e-03 eta 0:19:39
epoch [22/200] batch [3/3] time 2.083 (2.164) data 0.000 (0.084) loss 1.2852 (0.9683) acc 65.6250 (77.0833) lr 1.9461e-03 eta 0:19:15
epoch [23/200] batch [1/3] time 2.352 (2.352) data 0.272 (0.272) loss 1.1143 (1.1143) acc 65.6250 (65.6250) lr 1.9461e-03 eta 0:20:53
epoch [23/200] batch [2/3] time 2.082 (2.217) data 0.000 (0.136) loss 1.1328 (1.1235) acc 68.7500 (67.1875) lr 1.9461e-03 eta 0:19:39
epoch [23/200] batch [3/3] time 2.086 (2.173) data 0.000 (0.091) loss 1.3984 (1.2152) acc 59.3750 (64.5833) lr 1.9409e-03 eta 0:19:13
epoch [24/200] batch [1/3] time 2.332 (2.332) data 0.261 (0.261) loss 0.6616 (0.6616) acc 78.1250 (78.1250) lr 1.9409e-03 eta 0:20:35
epoch [24/200] batch [2/3] time 2.080 (2.206) data 0.000 (0.131) loss 0.9028 (0.7822) acc 71.8750 (75.0000) lr 1.9409e-03 eta 0:19:27
epoch [24/200] batch [3/3] time 2.083 (2.165) data 0.000 (0.087) loss 0.8267 (0.7970) acc 81.2500 (77.0833) lr 1.9354e-03 eta 0:19:03
epoch [25/200] batch [1/3] time 2.334 (2.334) data 0.258 (0.258) loss 1.1299 (1.1299) acc 71.8750 (71.8750) lr 1.9354e-03 eta 0:20:30
epoch [25/200] batch [2/3] time 2.080 (2.207) data 0.000 (0.129) loss 0.6665 (0.8982) acc 87.5000 (79.6875) lr 1.9354e-03 eta 0:19:21
epoch [25/200] batch [3/3] time 2.082 (2.166) data 0.000 (0.086) loss 1.0342 (0.9435) acc 84.3750 (81.2500) lr 1.9298e-03 eta 0:18:56
epoch [26/200] batch [1/3] time 2.338 (2.338) data 0.261 (0.261) loss 0.9038 (0.9038) acc 78.1250 (78.1250) lr 1.9298e-03 eta 0:20:25
epoch [26/200] batch [2/3] time 2.082 (2.210) data 0.000 (0.131) loss 0.9619 (0.9329) acc 81.2500 (79.6875) lr 1.9298e-03 eta 0:19:15
epoch [26/200] batch [3/3] time 2.083 (2.168) data 0.000 (0.087) loss 1.2402 (1.0353) acc 62.5000 (73.9583) lr 1.9239e-03 eta 0:18:51
epoch [27/200] batch [1/3] time 2.337 (2.337) data 0.258 (0.258) loss 0.6650 (0.6650) acc 81.2500 (81.2500) lr 1.9239e-03 eta 0:20:17
epoch [27/200] batch [2/3] time 2.079 (2.208) data 0.000 (0.129) loss 0.8970 (0.7810) acc 78.1250 (79.6875) lr 1.9239e-03 eta 0:19:08
epoch [27/200] batch [3/3] time 2.084 (2.167) data 0.000 (0.086) loss 0.6211 (0.7277) acc 87.5000 (82.2917) lr 1.9178e-03 eta 0:18:44
epoch [28/200] batch [1/3] time 2.343 (2.343) data 0.258 (0.258) loss 0.7603 (0.7603) acc 81.2500 (81.2500) lr 1.9178e-03 eta 0:20:13
epoch [28/200] batch [2/3] time 2.087 (2.215) data 0.000 (0.129) loss 0.8594 (0.8098) acc 75.0000 (78.1250) lr 1.9178e-03 eta 0:19:05
epoch [28/200] batch [3/3] time 2.079 (2.170) data 0.000 (0.086) loss 0.6870 (0.7689) acc 84.3750 (80.2083) lr 1.9114e-03 eta 0:18:39
epoch [29/200] batch [1/3] time 2.336 (2.336) data 0.256 (0.256) loss 0.5542 (0.5542) acc 87.5000 (87.5000) lr 1.9114e-03 eta 0:20:02
epoch [29/200] batch [2/3] time 2.079 (2.207) data 0.000 (0.128) loss 0.6489 (0.6016) acc 84.3750 (85.9375) lr 1.9114e-03 eta 0:18:54
epoch [29/200] batch [3/3] time 2.071 (2.162) data 0.000 (0.085) loss 0.7310 (0.6447) acc 84.3750 (85.4167) lr 1.9048e-03 eta 0:18:29
epoch [30/200] batch [1/3] time 2.332 (2.332) data 0.250 (0.250) loss 0.5879 (0.5879) acc 87.5000 (87.5000) lr 1.9048e-03 eta 0:19:53
epoch [30/200] batch [2/3] time 2.081 (2.206) data 0.000 (0.125) loss 0.7817 (0.6848) acc 81.2500 (84.3750) lr 1.9048e-03 eta 0:18:47
epoch [30/200] batch [3/3] time 2.080 (2.164) data 0.000 (0.084) loss 0.6982 (0.6893) acc 87.5000 (85.4167) lr 1.8980e-03 eta 0:18:23
epoch [31/200] batch [1/3] time 2.352 (2.352) data 0.269 (0.269) loss 0.9497 (0.9497) acc 75.0000 (75.0000) lr 1.8980e-03 eta 0:19:56
epoch [31/200] batch [2/3] time 2.081 (2.216) data 0.000 (0.134) loss 0.5981 (0.7739) acc 78.1250 (76.5625) lr 1.8980e-03 eta 0:18:45
epoch [31/200] batch [3/3] time 2.073 (2.169) data 0.000 (0.090) loss 0.3313 (0.6264) acc 93.7500 (82.2917) lr 1.8910e-03 eta 0:18:19
epoch [32/200] batch [1/3] time 2.336 (2.336) data 0.260 (0.260) loss 0.4341 (0.4341) acc 93.7500 (93.7500) lr 1.8910e-03 eta 0:19:42
epoch [32/200] batch [2/3] time 2.077 (2.207) data 0.000 (0.130) loss 0.4080 (0.4210) acc 87.5000 (90.6250) lr 1.8910e-03 eta 0:18:34
epoch [32/200] batch [3/3] time 2.085 (2.166) data 0.000 (0.087) loss 1.0117 (0.6179) acc 65.6250 (82.2917) lr 1.8838e-03 eta 0:18:11
epoch [33/200] batch [1/3] time 2.332 (2.332) data 0.257 (0.257) loss 0.7153 (0.7153) acc 87.5000 (87.5000) lr 1.8838e-03 eta 0:19:33
epoch [33/200] batch [2/3] time 2.073 (2.203) data 0.000 (0.129) loss 0.5171 (0.6162) acc 84.3750 (85.9375) lr 1.8838e-03 eta 0:18:25
epoch [33/200] batch [3/3] time 2.084 (2.163) data 0.000 (0.086) loss 0.6836 (0.6387) acc 84.3750 (85.4167) lr 1.8763e-03 eta 0:18:03
epoch [34/200] batch [1/3] time 2.347 (2.347) data 0.267 (0.267) loss 0.6851 (0.6851) acc 87.5000 (87.5000) lr 1.8763e-03 eta 0:19:33
epoch [34/200] batch [2/3] time 2.067 (2.207) data 0.000 (0.134) loss 0.3481 (0.5166) acc 93.7500 (90.6250) lr 1.8763e-03 eta 0:18:21
epoch [34/200] batch [3/3] time 2.080 (2.165) data 0.000 (0.089) loss 0.8955 (0.6429) acc 71.8750 (84.3750) lr 1.8686e-03 eta 0:17:58
epoch [35/200] batch [1/3] time 2.333 (2.333) data 0.257 (0.257) loss 0.7271 (0.7271) acc 78.1250 (78.1250) lr 1.8686e-03 eta 0:19:19
epoch [35/200] batch [2/3] time 2.083 (2.208) data 0.000 (0.128) loss 0.8467 (0.7869) acc 78.1250 (78.1250) lr 1.8686e-03 eta 0:18:15
epoch [35/200] batch [3/3] time 2.077 (2.164) data 0.000 (0.086) loss 0.4976 (0.6904) acc 87.5000 (81.2500) lr 1.8607e-03 eta 0:17:51
epoch [36/200] batch [1/3] time 2.346 (2.346) data 0.265 (0.265) loss 1.0449 (1.0449) acc 75.0000 (75.0000) lr 1.8607e-03 eta 0:19:18
epoch [36/200] batch [2/3] time 2.074 (2.210) data 0.000 (0.133) loss 0.6558 (0.8503) acc 81.2500 (78.1250) lr 1.8607e-03 eta 0:18:09
epoch [36/200] batch [3/3] time 2.086 (2.169) data 0.000 (0.089) loss 0.8203 (0.8403) acc 78.1250 (78.1250) lr 1.8526e-03 eta 0:17:47
epoch [37/200] batch [1/3] time 2.337 (2.337) data 0.265 (0.265) loss 0.7144 (0.7144) acc 81.2500 (81.2500) lr 1.8526e-03 eta 0:19:07
epoch [37/200] batch [2/3] time 2.079 (2.208) data 0.000 (0.133) loss 0.7358 (0.7251) acc 87.5000 (84.3750) lr 1.8526e-03 eta 0:18:01
epoch [37/200] batch [3/3] time 2.087 (2.168) data 0.000 (0.089) loss 1.0439 (0.8314) acc 71.8750 (80.2083) lr 1.8443e-03 eta 0:17:39
epoch [38/200] batch [1/3] time 2.325 (2.325) data 0.250 (0.250) loss 0.4893 (0.4893) acc 90.6250 (90.6250) lr 1.8443e-03 eta 0:18:54
epoch [38/200] batch [2/3] time 2.087 (2.206) data 0.000 (0.125) loss 1.0215 (0.7554) acc 78.1250 (84.3750) lr 1.8443e-03 eta 0:17:54
epoch [38/200] batch [3/3] time 2.078 (2.164) data 0.000 (0.083) loss 0.5435 (0.6847) acc 87.5000 (85.4167) lr 1.8358e-03 eta 0:17:31
epoch [39/200] batch [1/3] time 2.347 (2.347) data 0.267 (0.267) loss 0.4832 (0.4832) acc 90.6250 (90.6250) lr 1.8358e-03 eta 0:18:58
epoch [39/200] batch [2/3] time 2.081 (2.214) data 0.000 (0.134) loss 0.6382 (0.5607) acc 84.3750 (87.5000) lr 1.8358e-03 eta 0:17:51
epoch [39/200] batch [3/3] time 2.081 (2.170) data 0.000 (0.089) loss 0.6636 (0.5950) acc 84.3750 (86.4583) lr 1.8271e-03 eta 0:17:27
epoch [40/200] batch [1/3] time 2.346 (2.346) data 0.266 (0.266) loss 0.5469 (0.5469) acc 84.3750 (84.3750) lr 1.8271e-03 eta 0:18:50
epoch [40/200] batch [2/3] time 2.079 (2.212) data 0.000 (0.133) loss 0.8340 (0.6904) acc 81.2500 (82.8125) lr 1.8271e-03 eta 0:17:44
epoch [40/200] batch [3/3] time 2.081 (2.169) data 0.000 (0.089) loss 0.6807 (0.6872) acc 84.3750 (83.3333) lr 1.8181e-03 eta 0:17:20
epoch [41/200] batch [1/3] time 2.323 (2.323) data 0.251 (0.251) loss 0.3748 (0.3748) acc 87.5000 (87.5000) lr 1.8181e-03 eta 0:18:32
epoch [41/200] batch [2/3] time 2.074 (2.199) data 0.000 (0.125) loss 0.5645 (0.4696) acc 87.5000 (87.5000) lr 1.8181e-03 eta 0:17:30
epoch [41/200] batch [3/3] time 2.081 (2.159) data 0.000 (0.084) loss 0.5913 (0.5102) acc 87.5000 (87.5000) lr 1.8090e-03 eta 0:17:10
epoch [42/200] batch [1/3] time 2.324 (2.324) data 0.249 (0.249) loss 0.3374 (0.3374) acc 93.7500 (93.7500) lr 1.8090e-03 eta 0:18:26
epoch [42/200] batch [2/3] time 2.068 (2.196) data 0.000 (0.125) loss 0.4141 (0.3757) acc 93.7500 (93.7500) lr 1.8090e-03 eta 0:17:23
epoch [42/200] batch [3/3] time 2.085 (2.159) data 0.000 (0.083) loss 0.7134 (0.4883) acc 84.3750 (90.6250) lr 1.7997e-03 eta 0:17:03
epoch [43/200] batch [1/3] time 2.348 (2.348) data 0.266 (0.266) loss 0.8823 (0.8823) acc 78.1250 (78.1250) lr 1.7997e-03 eta 0:18:30
epoch [43/200] batch [2/3] time 2.075 (2.212) data 0.000 (0.133) loss 0.5835 (0.7329) acc 87.5000 (82.8125) lr 1.7997e-03 eta 0:17:23
epoch [43/200] batch [3/3] time 2.078 (2.167) data 0.000 (0.089) loss 0.5903 (0.6854) acc 81.2500 (82.2917) lr 1.7902e-03 eta 0:17:00
epoch [44/200] batch [1/3] time 2.313 (2.313) data 0.250 (0.250) loss 0.2830 (0.2830) acc 87.5000 (87.5000) lr 1.7902e-03 eta 0:18:07
epoch [44/200] batch [2/3] time 2.074 (2.194) data 0.000 (0.125) loss 0.4500 (0.3665) acc 90.6250 (89.0625) lr 1.7902e-03 eta 0:17:08
epoch [44/200] batch [3/3] time 2.078 (2.155) data 0.000 (0.083) loss 0.8311 (0.5213) acc 78.1250 (85.4167) lr 1.7804e-03 eta 0:16:48
epoch [45/200] batch [1/3] time 2.328 (2.328) data 0.249 (0.249) loss 0.6187 (0.6187) acc 84.3750 (84.3750) lr 1.7804e-03 eta 0:18:07
epoch [45/200] batch [2/3] time 2.073 (2.200) data 0.000 (0.124) loss 0.4990 (0.5588) acc 90.6250 (87.5000) lr 1.7804e-03 eta 0:17:05
epoch [45/200] batch [3/3] time 2.074 (2.158) data 0.000 (0.083) loss 0.5752 (0.5643) acc 87.5000 (87.5000) lr 1.7705e-03 eta 0:16:43
epoch [46/200] batch [1/3] time 2.347 (2.347) data 0.265 (0.265) loss 0.4397 (0.4397) acc 87.5000 (87.5000) lr 1.7705e-03 eta 0:18:08
epoch [46/200] batch [2/3] time 2.063 (2.205) data 0.000 (0.133) loss 0.3799 (0.4098) acc 90.6250 (89.0625) lr 1.7705e-03 eta 0:17:00
epoch [46/200] batch [3/3] time 2.067 (2.159) data 0.000 (0.089) loss 0.3318 (0.3838) acc 90.6250 (89.5833) lr 1.7604e-03 eta 0:16:37
epoch [47/200] batch [1/3] time 2.339 (2.339) data 0.267 (0.267) loss 0.3967 (0.3967) acc 90.6250 (90.6250) lr 1.7604e-03 eta 0:17:58
epoch [47/200] batch [2/3] time 2.074 (2.207) data 0.000 (0.133) loss 0.6001 (0.4984) acc 84.3750 (87.5000) lr 1.7604e-03 eta 0:16:55
epoch [47/200] batch [3/3] time 2.062 (2.158) data 0.000 (0.089) loss 0.2830 (0.4266) acc 96.8750 (90.6250) lr 1.7501e-03 eta 0:16:30
epoch [48/200] batch [1/3] time 2.330 (2.330) data 0.259 (0.259) loss 0.4556 (0.4556) acc 90.6250 (90.6250) lr 1.7501e-03 eta 0:17:47
epoch [48/200] batch [2/3] time 2.057 (2.193) data 0.000 (0.130) loss 0.1740 (0.3148) acc 93.7500 (92.1875) lr 1.7501e-03 eta 0:16:42
epoch [48/200] batch [3/3] time 2.075 (2.154) data 0.000 (0.086) loss 0.3772 (0.3356) acc 96.8750 (93.7500) lr 1.7396e-03 eta 0:16:22
epoch [49/200] batch [1/3] time 2.339 (2.339) data 0.257 (0.257) loss 0.6758 (0.6758) acc 81.2500 (81.2500) lr 1.7396e-03 eta 0:17:44
epoch [49/200] batch [2/3] time 2.072 (2.205) data 0.000 (0.129) loss 0.3394 (0.5076) acc 93.7500 (87.5000) lr 1.7396e-03 eta 0:16:41
epoch [49/200] batch [3/3] time 2.074 (2.162) data 0.000 (0.086) loss 0.6616 (0.5589) acc 84.3750 (86.4583) lr 1.7290e-03 eta 0:16:19
epoch [50/200] batch [1/3] time 2.336 (2.336) data 0.258 (0.258) loss 0.5576 (0.5576) acc 84.3750 (84.3750) lr 1.7290e-03 eta 0:17:36
epoch [50/200] batch [2/3] time 2.077 (2.207) data 0.000 (0.129) loss 0.5439 (0.5508) acc 84.3750 (84.3750) lr 1.7290e-03 eta 0:16:35
epoch [50/200] batch [3/3] time 2.078 (2.164) data 0.000 (0.086) loss 0.4502 (0.5173) acc 90.6250 (86.4583) lr 1.7181e-03 eta 0:16:13
epoch [51/200] batch [1/3] time 2.313 (2.313) data 0.250 (0.250) loss 0.3091 (0.3091) acc 87.5000 (87.5000) lr 1.7181e-03 eta 0:17:18
epoch [51/200] batch [2/3] time 2.076 (2.194) data 0.000 (0.125) loss 0.3381 (0.3236) acc 96.8750 (92.1875) lr 1.7181e-03 eta 0:16:22
epoch [51/200] batch [3/3] time 2.065 (2.151) data 0.000 (0.084) loss 0.4517 (0.3663) acc 93.7500 (92.7083) lr 1.7071e-03 eta 0:16:01
epoch [52/200] batch [1/3] time 2.315 (2.315) data 0.250 (0.250) loss 0.3301 (0.3301) acc 90.6250 (90.6250) lr 1.7071e-03 eta 0:17:12
epoch [52/200] batch [2/3] time 2.073 (2.194) data 0.000 (0.125) loss 0.3982 (0.3641) acc 90.6250 (90.6250) lr 1.7071e-03 eta 0:16:16
epoch [52/200] batch [3/3] time 2.065 (2.151) data 0.000 (0.083) loss 0.3516 (0.3599) acc 90.6250 (90.6250) lr 1.6959e-03 eta 0:15:55
epoch [53/200] batch [1/3] time 2.323 (2.323) data 0.250 (0.250) loss 0.4233 (0.4233) acc 90.6250 (90.6250) lr 1.6959e-03 eta 0:17:09
epoch [53/200] batch [2/3] time 2.075 (2.199) data 0.000 (0.125) loss 0.5249 (0.4741) acc 84.3750 (87.5000) lr 1.6959e-03 eta 0:16:12
epoch [53/200] batch [3/3] time 2.070 (2.156) data 0.000 (0.084) loss 0.2996 (0.4159) acc 93.7500 (89.5833) lr 1.6845e-03 eta 0:15:50
epoch [54/200] batch [1/3] time 2.324 (2.324) data 0.265 (0.265) loss 0.3269 (0.3269) acc 90.6250 (90.6250) lr 1.6845e-03 eta 0:17:02
epoch [54/200] batch [2/3] time 2.080 (2.202) data 0.000 (0.133) loss 0.6797 (0.5033) acc 84.3750 (87.5000) lr 1.6845e-03 eta 0:16:06
epoch [54/200] batch [3/3] time 2.075 (2.160) data 0.000 (0.089) loss 0.3506 (0.4524) acc 90.6250 (88.5417) lr 1.6730e-03 eta 0:15:45
epoch [55/200] batch [1/3] time 2.318 (2.318) data 0.251 (0.251) loss 0.4153 (0.4153) acc 90.6250 (90.6250) lr 1.6730e-03 eta 0:16:52
epoch [55/200] batch [2/3] time 2.066 (2.192) data 0.000 (0.126) loss 0.4136 (0.4144) acc 87.5000 (89.0625) lr 1.6730e-03 eta 0:15:55
epoch [55/200] batch [3/3] time 2.067 (2.150) data 0.000 (0.084) loss 0.2737 (0.3675) acc 96.8750 (91.6667) lr 1.6613e-03 eta 0:15:35
epoch [56/200] batch [1/3] time 2.324 (2.324) data 0.248 (0.248) loss 0.5010 (0.5010) acc 90.6250 (90.6250) lr 1.6613e-03 eta 0:16:48
epoch [56/200] batch [2/3] time 2.072 (2.198) data 0.000 (0.124) loss 0.3589 (0.4299) acc 96.8750 (93.7500) lr 1.6613e-03 eta 0:15:51
epoch [56/200] batch [3/3] time 2.065 (2.154) data 0.000 (0.083) loss 0.4802 (0.4467) acc 90.6250 (92.7083) lr 1.6494e-03 eta 0:15:30
epoch [57/200] batch [1/3] time 2.328 (2.328) data 0.259 (0.259) loss 0.2654 (0.2654) acc 96.8750 (96.8750) lr 1.6494e-03 eta 0:16:43
epoch [57/200] batch [2/3] time 2.077 (2.202) data 0.000 (0.129) loss 0.5972 (0.4313) acc 84.3750 (90.6250) lr 1.6494e-03 eta 0:15:47
epoch [57/200] batch [3/3] time 2.069 (2.158) data 0.000 (0.086) loss 0.3477 (0.4034) acc 90.6250 (90.6250) lr 1.6374e-03 eta 0:15:25
epoch [58/200] batch [1/3] time 2.336 (2.336) data 0.257 (0.257) loss 0.6841 (0.6841) acc 81.2500 (81.2500) lr 1.6374e-03 eta 0:16:39
epoch [58/200] batch [2/3] time 2.078 (2.207) data 0.000 (0.128) loss 0.7241 (0.7041) acc 81.2500 (81.2500) lr 1.6374e-03 eta 0:15:42
epoch [58/200] batch [3/3] time 2.072 (2.162) data 0.000 (0.086) loss 0.3123 (0.5735) acc 87.5000 (83.3333) lr 1.6252e-03 eta 0:15:20
epoch [59/200] batch [1/3] time 2.353 (2.353) data 0.266 (0.266) loss 0.6113 (0.6113) acc 87.5000 (87.5000) lr 1.6252e-03 eta 0:16:40
epoch [59/200] batch [2/3] time 2.065 (2.209) data 0.000 (0.133) loss 0.2365 (0.4239) acc 96.8750 (92.1875) lr 1.6252e-03 eta 0:15:36
epoch [59/200] batch [3/3] time 2.072 (2.164) data 0.000 (0.089) loss 0.3733 (0.4070) acc 90.6250 (91.6667) lr 1.6129e-03 eta 0:15:15
epoch [60/200] batch [1/3] time 2.320 (2.320) data 0.258 (0.258) loss 0.2264 (0.2264) acc 93.7500 (93.7500) lr 1.6129e-03 eta 0:16:18
epoch [60/200] batch [2/3] time 2.080 (2.200) data 0.000 (0.129) loss 0.5732 (0.3998) acc 87.5000 (90.6250) lr 1.6129e-03 eta 0:15:26
epoch [60/200] batch [3/3] time 2.077 (2.159) data 0.000 (0.086) loss 0.3848 (0.3948) acc 90.6250 (90.6250) lr 1.6004e-03 eta 0:15:06
epoch [61/200] batch [1/3] time 2.332 (2.332) data 0.259 (0.259) loss 0.4619 (0.4619) acc 90.6250 (90.6250) lr 1.6004e-03 eta 0:16:16
epoch [61/200] batch [2/3] time 2.070 (2.201) data 0.000 (0.130) loss 0.3953 (0.4286) acc 87.5000 (89.0625) lr 1.6004e-03 eta 0:15:20
epoch [61/200] batch [3/3] time 2.082 (2.161) data 0.000 (0.086) loss 0.8491 (0.5688) acc 78.1250 (85.4167) lr 1.5878e-03 eta 0:15:01
epoch [62/200] batch [1/3] time 2.333 (2.333) data 0.260 (0.260) loss 0.5239 (0.5239) acc 90.6250 (90.6250) lr 1.5878e-03 eta 0:16:10
epoch [62/200] batch [2/3] time 2.074 (2.204) data 0.000 (0.130) loss 0.5308 (0.5273) acc 87.5000 (89.0625) lr 1.5878e-03 eta 0:15:14
epoch [62/200] batch [3/3] time 2.083 (2.163) data 0.000 (0.087) loss 0.4873 (0.5140) acc 87.5000 (88.5417) lr 1.5750e-03 eta 0:14:55
epoch [63/200] batch [1/3] time 2.333 (2.333) data 0.262 (0.262) loss 0.3752 (0.3752) acc 87.5000 (87.5000) lr 1.5750e-03 eta 0:16:03
epoch [63/200] batch [2/3] time 2.083 (2.208) data 0.000 (0.131) loss 0.6489 (0.5121) acc 87.5000 (87.5000) lr 1.5750e-03 eta 0:15:09
epoch [63/200] batch [3/3] time 2.077 (2.164) data 0.000 (0.088) loss 0.4885 (0.5042) acc 90.6250 (88.5417) lr 1.5621e-03 eta 0:14:49
epoch [64/200] batch [1/3] time 2.335 (2.335) data 0.268 (0.268) loss 0.3250 (0.3250) acc 93.7500 (93.7500) lr 1.5621e-03 eta 0:15:57
epoch [64/200] batch [2/3] time 2.071 (2.203) data 0.000 (0.134) loss 0.3176 (0.3213) acc 96.8750 (95.3125) lr 1.5621e-03 eta 0:15:01
epoch [64/200] batch [3/3] time 2.077 (2.161) data 0.000 (0.090) loss 0.2820 (0.3082) acc 96.8750 (95.8333) lr 1.5490e-03 eta 0:14:41
epoch [65/200] batch [1/3] time 2.326 (2.326) data 0.259 (0.259) loss 0.2554 (0.2554) acc 93.7500 (93.7500) lr 1.5490e-03 eta 0:15:46
epoch [65/200] batch [2/3] time 2.074 (2.200) data 0.000 (0.129) loss 0.5391 (0.3972) acc 84.3750 (89.0625) lr 1.5490e-03 eta 0:14:53
epoch [65/200] batch [3/3] time 2.069 (2.156) data 0.000 (0.086) loss 0.2856 (0.3600) acc 96.8750 (91.6667) lr 1.5358e-03 eta 0:14:33
epoch [66/200] batch [1/3] time 2.341 (2.341) data 0.266 (0.266) loss 0.4805 (0.4805) acc 87.5000 (87.5000) lr 1.5358e-03 eta 0:15:45
epoch [66/200] batch [2/3] time 2.061 (2.201) data 0.000 (0.133) loss 0.4033 (0.4419) acc 93.7500 (90.6250) lr 1.5358e-03 eta 0:14:47
epoch [66/200] batch [3/3] time 2.086 (2.163) data 0.000 (0.089) loss 0.8135 (0.5658) acc 87.5000 (89.5833) lr 1.5225e-03 eta 0:14:29
epoch [67/200] batch [1/3] time 2.326 (2.326) data 0.260 (0.260) loss 0.3083 (0.3083) acc 90.6250 (90.6250) lr 1.5225e-03 eta 0:15:32
epoch [67/200] batch [2/3] time 2.071 (2.199) data 0.000 (0.130) loss 0.3591 (0.3337) acc 90.6250 (90.6250) lr 1.5225e-03 eta 0:14:39
epoch [67/200] batch [3/3] time 2.076 (2.158) data 0.000 (0.087) loss 0.3738 (0.3471) acc 93.7500 (91.6667) lr 1.5090e-03 eta 0:14:20
epoch [68/200] batch [1/3] time 2.329 (2.329) data 0.260 (0.260) loss 0.4534 (0.4534) acc 93.7500 (93.7500) lr 1.5090e-03 eta 0:15:27
epoch [68/200] batch [2/3] time 2.056 (2.193) data 0.000 (0.130) loss 0.1583 (0.3058) acc 96.8750 (95.3125) lr 1.5090e-03 eta 0:14:30
epoch [68/200] batch [3/3] time 2.082 (2.156) data 0.000 (0.087) loss 0.6489 (0.4202) acc 84.3750 (91.6667) lr 1.4955e-03 eta 0:14:13
epoch [69/200] batch [1/3] time 2.334 (2.334) data 0.255 (0.255) loss 0.5669 (0.5669) acc 87.5000 (87.5000) lr 1.4955e-03 eta 0:15:21
epoch [69/200] batch [2/3] time 2.077 (2.205) data 0.000 (0.128) loss 0.3318 (0.4493) acc 93.7500 (90.6250) lr 1.4955e-03 eta 0:14:28
epoch [69/200] batch [3/3] time 2.083 (2.165) data 0.000 (0.085) loss 0.6030 (0.5006) acc 87.5000 (89.5833) lr 1.4818e-03 eta 0:14:10
epoch [70/200] batch [1/3] time 2.342 (2.342) data 0.267 (0.267) loss 0.6367 (0.6367) acc 81.2500 (81.2500) lr 1.4818e-03 eta 0:15:17
epoch [70/200] batch [2/3] time 2.081 (2.212) data 0.000 (0.134) loss 0.4644 (0.5505) acc 96.8750 (89.0625) lr 1.4818e-03 eta 0:14:24
epoch [70/200] batch [3/3] time 2.079 (2.167) data 0.000 (0.089) loss 0.4077 (0.5029) acc 87.5000 (88.5417) lr 1.4679e-03 eta 0:14:05
epoch [71/200] batch [1/3] time 2.307 (2.307) data 0.251 (0.251) loss 0.1523 (0.1523) acc 93.7500 (93.7500) lr 1.4679e-03 eta 0:14:57
epoch [71/200] batch [2/3] time 2.080 (2.193) data 0.000 (0.125) loss 0.4478 (0.3000) acc 90.6250 (92.1875) lr 1.4679e-03 eta 0:14:11
epoch [71/200] batch [3/3] time 2.073 (2.153) data 0.000 (0.084) loss 0.2030 (0.2677) acc 96.8750 (93.7500) lr 1.4540e-03 eta 0:13:53
epoch [72/200] batch [1/3] time 2.331 (2.331) data 0.248 (0.248) loss 0.4016 (0.4016) acc 93.7500 (93.7500) lr 1.4540e-03 eta 0:14:59
epoch [72/200] batch [2/3] time 2.074 (2.203) data 0.000 (0.124) loss 0.3730 (0.3873) acc 93.7500 (93.7500) lr 1.4540e-03 eta 0:14:08
epoch [72/200] batch [3/3] time 2.069 (2.158) data 0.000 (0.083) loss 0.4731 (0.4159) acc 90.6250 (92.7083) lr 1.4399e-03 eta 0:13:48
epoch [73/200] batch [1/3] time 2.329 (2.329) data 0.259 (0.259) loss 0.2703 (0.2703) acc 93.7500 (93.7500) lr 1.4399e-03 eta 0:14:52
epoch [73/200] batch [2/3] time 2.076 (2.202) data 0.000 (0.130) loss 0.3811 (0.3257) acc 93.7500 (93.7500) lr 1.4399e-03 eta 0:14:01
epoch [73/200] batch [3/3] time 2.071 (2.159) data 0.000 (0.086) loss 0.2744 (0.3086) acc 96.8750 (94.7917) lr 1.4258e-03 eta 0:13:42
epoch [74/200] batch [1/3] time 2.332 (2.332) data 0.265 (0.265) loss 0.2910 (0.2910) acc 90.6250 (90.6250) lr 1.4258e-03 eta 0:14:46
epoch [74/200] batch [2/3] time 2.076 (2.204) data 0.000 (0.133) loss 0.4900 (0.3905) acc 84.3750 (87.5000) lr 1.4258e-03 eta 0:13:55
epoch [74/200] batch [3/3] time 2.073 (2.160) data 0.000 (0.088) loss 0.2932 (0.3581) acc 93.7500 (89.5833) lr 1.4115e-03 eta 0:13:36
epoch [75/200] batch [1/3] time 2.329 (2.329) data 0.259 (0.259) loss 0.2142 (0.2142) acc 93.7500 (93.7500) lr 1.4115e-03 eta 0:14:37
epoch [75/200] batch [2/3] time 2.062 (2.195) data 0.000 (0.130) loss 0.2546 (0.2344) acc 93.7500 (93.7500) lr 1.4115e-03 eta 0:13:45
epoch [75/200] batch [3/3] time 2.075 (2.155) data 0.000 (0.086) loss 0.7021 (0.3903) acc 81.2500 (89.5833) lr 1.3971e-03 eta 0:13:28
epoch [76/200] batch [1/3] time 2.332 (2.332) data 0.266 (0.266) loss 0.2203 (0.2203) acc 96.8750 (96.8750) lr 1.3971e-03 eta 0:14:32
epoch [76/200] batch [2/3] time 2.065 (2.199) data 0.000 (0.133) loss 0.2454 (0.2328) acc 96.8750 (96.8750) lr 1.3971e-03 eta 0:13:40
epoch [76/200] batch [3/3] time 2.069 (2.156) data 0.000 (0.089) loss 0.3145 (0.2601) acc 96.8750 (96.8750) lr 1.3827e-03 eta 0:13:21
epoch [77/200] batch [1/3] time 2.322 (2.322) data 0.251 (0.251) loss 0.3867 (0.3867) acc 87.5000 (87.5000) lr 1.3827e-03 eta 0:14:21
epoch [77/200] batch [2/3] time 2.074 (2.198) data 0.000 (0.126) loss 0.4695 (0.4281) acc 84.3750 (85.9375) lr 1.3827e-03 eta 0:13:33
epoch [77/200] batch [3/3] time 2.074 (2.157) data 0.000 (0.084) loss 0.3079 (0.3880) acc 90.6250 (87.5000) lr 1.3681e-03 eta 0:13:15
epoch [78/200] batch [1/3] time 2.330 (2.330) data 0.265 (0.265) loss 0.2245 (0.2245) acc 93.7500 (93.7500) lr 1.3681e-03 eta 0:14:17
epoch [78/200] batch [2/3] time 2.075 (2.203) data 0.000 (0.133) loss 0.2822 (0.2534) acc 96.8750 (95.3125) lr 1.3681e-03 eta 0:13:28
epoch [78/200] batch [3/3] time 2.073 (2.159) data 0.000 (0.088) loss 0.3555 (0.2874) acc 87.5000 (92.7083) lr 1.3535e-03 eta 0:13:10
epoch [79/200] batch [1/3] time 2.329 (2.329) data 0.250 (0.250) loss 0.6377 (0.6377) acc 84.3750 (84.3750) lr 1.3535e-03 eta 0:14:10
epoch [79/200] batch [2/3] time 2.072 (2.201) data 0.000 (0.125) loss 0.4253 (0.5315) acc 90.6250 (87.5000) lr 1.3535e-03 eta 0:13:21
epoch [79/200] batch [3/3] time 2.072 (2.158) data 0.000 (0.083) loss 0.3408 (0.4679) acc 93.7500 (89.5833) lr 1.3387e-03 eta 0:13:03
epoch [80/200] batch [1/3] time 2.333 (2.333) data 0.251 (0.251) loss 0.5312 (0.5312) acc 87.5000 (87.5000) lr 1.3387e-03 eta 0:14:04
epoch [80/200] batch [2/3] time 2.069 (2.201) data 0.000 (0.125) loss 0.3359 (0.4336) acc 93.7500 (90.6250) lr 1.3387e-03 eta 0:13:14
epoch [80/200] batch [3/3] time 2.075 (2.159) data 0.000 (0.084) loss 0.3186 (0.3953) acc 90.6250 (90.6250) lr 1.3239e-03 eta 0:12:57
epoch [81/200] batch [1/3] time 2.340 (2.340) data 0.266 (0.266) loss 0.3362 (0.3362) acc 93.7500 (93.7500) lr 1.3239e-03 eta 0:14:00
epoch [81/200] batch [2/3] time 2.082 (2.211) data 0.000 (0.133) loss 0.5269 (0.4315) acc 90.6250 (92.1875) lr 1.3239e-03 eta 0:13:11
epoch [81/200] batch [3/3] time 2.067 (2.163) data 0.000 (0.089) loss 0.4507 (0.4379) acc 93.7500 (92.7083) lr 1.3090e-03 eta 0:12:52
epoch [82/200] batch [1/3] time 2.331 (2.331) data 0.252 (0.252) loss 0.4451 (0.4451) acc 90.6250 (90.6250) lr 1.3090e-03 eta 0:13:50
epoch [82/200] batch [2/3] time 2.065 (2.198) data 0.000 (0.126) loss 0.2993 (0.3722) acc 93.7500 (92.1875) lr 1.3090e-03 eta 0:13:00
epoch [82/200] batch [3/3] time 2.075 (2.157) data 0.000 (0.084) loss 0.5327 (0.4257) acc 84.3750 (89.5833) lr 1.2940e-03 eta 0:12:43
epoch [83/200] batch [1/3] time 2.325 (2.325) data 0.258 (0.258) loss 0.2362 (0.2362) acc 93.7500 (93.7500) lr 1.2940e-03 eta 0:13:40
epoch [83/200] batch [2/3] time 2.065 (2.195) data 0.000 (0.129) loss 0.2086 (0.2224) acc 93.7500 (93.7500) lr 1.2940e-03 eta 0:12:52
epoch [83/200] batch [3/3] time 2.077 (2.155) data 0.000 (0.086) loss 0.5610 (0.3353) acc 87.5000 (91.6667) lr 1.2790e-03 eta 0:12:36
epoch [84/200] batch [1/3] time 2.322 (2.322) data 0.258 (0.258) loss 0.1904 (0.1904) acc 100.0000 (100.0000) lr 1.2790e-03 eta 0:13:32
epoch [84/200] batch [2/3] time 2.075 (2.198) data 0.000 (0.129) loss 0.5103 (0.3503) acc 90.6250 (95.3125) lr 1.2790e-03 eta 0:12:47
epoch [84/200] batch [3/3] time 2.052 (2.150) data 0.000 (0.086) loss 0.1237 (0.2748) acc 100.0000 (96.8750) lr 1.2639e-03 eta 0:12:28
epoch [85/200] batch [1/3] time 2.326 (2.326) data 0.250 (0.250) loss 0.5386 (0.5386) acc 90.6250 (90.6250) lr 1.2639e-03 eta 0:13:27
epoch [85/200] batch [2/3] time 2.065 (2.196) data 0.000 (0.125) loss 0.5791 (0.5588) acc 90.6250 (90.6250) lr 1.2639e-03 eta 0:12:39
epoch [85/200] batch [3/3] time 2.067 (2.153) data 0.000 (0.083) loss 0.3220 (0.4799) acc 90.6250 (90.6250) lr 1.2487e-03 eta 0:12:22
epoch [86/200] batch [1/3] time 2.312 (2.312) data 0.248 (0.248) loss 0.1613 (0.1613) acc 100.0000 (100.0000) lr 1.2487e-03 eta 0:13:15
epoch [86/200] batch [2/3] time 2.057 (2.184) data 0.000 (0.124) loss 0.2793 (0.2203) acc 93.7500 (96.8750) lr 1.2487e-03 eta 0:12:29
epoch [86/200] batch [3/3] time 2.079 (2.149) data 0.000 (0.083) loss 0.4951 (0.3119) acc 90.6250 (94.7917) lr 1.2334e-03 eta 0:12:15
epoch [87/200] batch [1/3] time 2.319 (2.319) data 0.260 (0.260) loss 0.5244 (0.5244) acc 90.6250 (90.6250) lr 1.2334e-03 eta 0:13:10
epoch [87/200] batch [2/3] time 2.080 (2.200) data 0.000 (0.130) loss 0.4966 (0.5105) acc 90.6250 (90.6250) lr 1.2334e-03 eta 0:12:27
epoch [87/200] batch [3/3] time 2.176 (2.192) data 0.000 (0.087) loss 0.2013 (0.4074) acc 96.8750 (92.7083) lr 1.2181e-03 eta 0:12:23
epoch [88/200] batch [1/3] time 2.322 (2.322) data 0.262 (0.262) loss 0.2832 (0.2832) acc 93.7500 (93.7500) lr 1.2181e-03 eta 0:13:04
epoch [88/200] batch [2/3] time 2.084 (2.203) data 0.000 (0.131) loss 0.3462 (0.3147) acc 87.5000 (90.6250) lr 1.2181e-03 eta 0:12:22
epoch [88/200] batch [3/3] time 2.074 (2.160) data 0.000 (0.088) loss 0.2213 (0.2836) acc 96.8750 (92.7083) lr 1.2028e-03 eta 0:12:05
epoch [89/200] batch [1/3] time 2.318 (2.318) data 0.250 (0.250) loss 0.2040 (0.2040) acc 96.8750 (96.8750) lr 1.2028e-03 eta 0:12:56
epoch [89/200] batch [2/3] time 2.056 (2.187) data 0.000 (0.125) loss 0.0723 (0.1381) acc 100.0000 (98.4375) lr 1.2028e-03 eta 0:12:10
epoch [89/200] batch [3/3] time 2.075 (2.149) data 0.000 (0.083) loss 0.2018 (0.1593) acc 93.7500 (96.8750) lr 1.1874e-03 eta 0:11:55
epoch [90/200] batch [1/3] time 2.320 (2.320) data 0.249 (0.249) loss 0.2849 (0.2849) acc 96.8750 (96.8750) lr 1.1874e-03 eta 0:12:50
epoch [90/200] batch [2/3] time 2.065 (2.193) data 0.000 (0.125) loss 0.1653 (0.2251) acc 96.8750 (96.8750) lr 1.1874e-03 eta 0:12:05
epoch [90/200] batch [3/3] time 2.067 (2.151) data 0.000 (0.083) loss 0.3303 (0.2602) acc 90.6250 (94.7917) lr 1.1719e-03 eta 0:11:49
epoch [91/200] batch [1/3] time 2.322 (2.322) data 0.261 (0.261) loss 0.3059 (0.3059) acc 90.6250 (90.6250) lr 1.1719e-03 eta 0:12:43
epoch [91/200] batch [2/3] time 2.066 (2.194) data 0.000 (0.131) loss 0.3298 (0.3179) acc 84.3750 (87.5000) lr 1.1719e-03 eta 0:11:59
epoch [91/200] batch [3/3] time 2.065 (2.151) data 0.000 (0.087) loss 0.2727 (0.3028) acc 93.7500 (89.5833) lr 1.1564e-03 eta 0:11:43
epoch [92/200] batch [1/3] time 2.320 (2.320) data 0.250 (0.250) loss 0.3650 (0.3650) acc 90.6250 (90.6250) lr 1.1564e-03 eta 0:12:36
epoch [92/200] batch [2/3] time 2.071 (2.195) data 0.000 (0.125) loss 0.6870 (0.5260) acc 81.2500 (85.9375) lr 1.1564e-03 eta 0:11:53
epoch [92/200] batch [3/3] time 2.070 (2.154) data 0.000 (0.083) loss 0.2264 (0.4261) acc 96.8750 (89.5833) lr 1.1409e-03 eta 0:11:37
epoch [93/200] batch [1/3] time 2.334 (2.334) data 0.268 (0.268) loss 0.3928 (0.3928) acc 90.6250 (90.6250) lr 1.1409e-03 eta 0:12:34
epoch [93/200] batch [2/3] time 2.077 (2.206) data 0.000 (0.134) loss 0.4595 (0.4261) acc 90.6250 (90.6250) lr 1.1409e-03 eta 0:11:50
epoch [93/200] batch [3/3] time 2.065 (2.159) data 0.000 (0.089) loss 0.2515 (0.3679) acc 93.7500 (91.6667) lr 1.1253e-03 eta 0:11:33
epoch [94/200] batch [1/3] time 2.315 (2.315) data 0.254 (0.254) loss 0.2925 (0.2925) acc 96.8750 (96.8750) lr 1.1253e-03 eta 0:12:20
epoch [94/200] batch [2/3] time 2.076 (2.196) data 0.000 (0.127) loss 0.3140 (0.3032) acc 93.7500 (95.3125) lr 1.1253e-03 eta 0:11:40
epoch [94/200] batch [3/3] time 2.052 (2.148) data 0.000 (0.085) loss 0.4233 (0.3433) acc 93.7500 (94.7917) lr 1.1097e-03 eta 0:11:23
epoch [95/200] batch [1/3] time 2.332 (2.332) data 0.251 (0.251) loss 0.4724 (0.4724) acc 87.5000 (87.5000) lr 1.1097e-03 eta 0:12:19
epoch [95/200] batch [2/3] time 2.059 (2.196) data 0.000 (0.126) loss 0.1455 (0.3090) acc 100.0000 (93.7500) lr 1.1097e-03 eta 0:11:33
epoch [95/200] batch [3/3] time 2.055 (2.149) data 0.000 (0.084) loss 0.0903 (0.2361) acc 100.0000 (95.8333) lr 1.0941e-03 eta 0:11:16
epoch [96/200] batch [1/3] time 2.341 (2.341) data 0.268 (0.268) loss 0.4563 (0.4563) acc 87.5000 (87.5000) lr 1.0941e-03 eta 0:12:14
epoch [96/200] batch [2/3] time 2.067 (2.204) data 0.000 (0.134) loss 0.2236 (0.3400) acc 96.8750 (92.1875) lr 1.0941e-03 eta 0:11:29
epoch [96/200] batch [3/3] time 2.059 (2.156) data 0.000 (0.089) loss 0.2281 (0.3027) acc 96.8750 (93.7500) lr 1.0785e-03 eta 0:11:12
epoch [97/200] batch [1/3] time 2.311 (2.311) data 0.260 (0.260) loss 0.1707 (0.1707) acc 96.8750 (96.8750) lr 1.0785e-03 eta 0:11:58
epoch [97/200] batch [2/3] time 2.074 (2.193) data 0.000 (0.130) loss 0.1948 (0.1827) acc 100.0000 (98.4375) lr 1.0785e-03 eta 0:11:19
epoch [97/200] batch [3/3] time 2.076 (2.154) data 0.000 (0.087) loss 0.4377 (0.2677) acc 93.7500 (96.8750) lr 1.0628e-03 eta 0:11:05
epoch [98/200] batch [1/3] time 2.321 (2.321) data 0.247 (0.247) loss 0.3518 (0.3518) acc 93.7500 (93.7500) lr 1.0628e-03 eta 0:11:54
epoch [98/200] batch [2/3] time 2.071 (2.196) data 0.000 (0.124) loss 0.3225 (0.3372) acc 96.8750 (95.3125) lr 1.0628e-03 eta 0:11:14
epoch [98/200] batch [3/3] time 2.076 (2.156) data 0.000 (0.083) loss 0.3943 (0.3562) acc 87.5000 (92.7083) lr 1.0471e-03 eta 0:10:59
epoch [99/200] batch [1/3] time 2.329 (2.329) data 0.249 (0.249) loss 0.2888 (0.2888) acc 100.0000 (100.0000) lr 1.0471e-03 eta 0:11:50
epoch [99/200] batch [2/3] time 2.066 (2.198) data 0.000 (0.125) loss 0.2729 (0.2809) acc 90.6250 (95.3125) lr 1.0471e-03 eta 0:11:08
epoch [99/200] batch [3/3] time 2.069 (2.155) data 0.000 (0.083) loss 0.2271 (0.2629) acc 93.7500 (94.7917) lr 1.0314e-03 eta 0:10:52
epoch [100/200] batch [1/3] time 2.312 (2.312) data 0.248 (0.248) loss 0.2983 (0.2983) acc 93.7500 (93.7500) lr 1.0314e-03 eta 0:11:38
epoch [100/200] batch [2/3] time 2.050 (2.181) data 0.000 (0.124) loss 0.0913 (0.1948) acc 96.8750 (95.3125) lr 1.0314e-03 eta 0:10:56
epoch [100/200] batch [3/3] time 2.058 (2.140) data 0.000 (0.083) loss 0.1586 (0.1827) acc 96.8750 (95.8333) lr 1.0157e-03 eta 0:10:42
epoch [101/200] batch [1/3] time 2.339 (2.339) data 0.269 (0.269) loss 0.1769 (0.1769) acc 96.8750 (96.8750) lr 1.0157e-03 eta 0:11:39
epoch [101/200] batch [2/3] time 2.023 (2.181) data 0.000 (0.135) loss 0.0508 (0.1138) acc 100.0000 (98.4375) lr 1.0157e-03 eta 0:10:49
epoch [101/200] batch [3/3] time 2.064 (2.142) data 0.000 (0.090) loss 0.3376 (0.1884) acc 93.7500 (96.8750) lr 1.0000e-03 eta 0:10:36
epoch [102/200] batch [1/3] time 2.330 (2.330) data 0.266 (0.266) loss 0.3091 (0.3091) acc 90.6250 (90.6250) lr 1.0000e-03 eta 0:11:29
epoch [102/200] batch [2/3] time 2.065 (2.197) data 0.000 (0.133) loss 0.2925 (0.3008) acc 93.7500 (92.1875) lr 1.0000e-03 eta 0:10:48
epoch [102/200] batch [3/3] time 2.069 (2.155) data 0.000 (0.089) loss 0.3047 (0.3021) acc 93.7500 (92.7083) lr 9.8429e-04 eta 0:10:33
epoch [103/200] batch [1/3] time 2.320 (2.320) data 0.251 (0.251) loss 0.3970 (0.3970) acc 90.6250 (90.6250) lr 9.8429e-04 eta 0:11:19
epoch [103/200] batch [2/3] time 2.078 (2.199) data 0.000 (0.125) loss 0.3701 (0.3835) acc 90.6250 (90.6250) lr 9.8429e-04 eta 0:10:42
epoch [103/200] batch [3/3] time 2.049 (2.149) data 0.000 (0.084) loss 0.1230 (0.2967) acc 96.8750 (92.7083) lr 9.6859e-04 eta 0:10:25
epoch [104/200] batch [1/3] time 2.306 (2.306) data 0.266 (0.266) loss 0.0671 (0.0671) acc 100.0000 (100.0000) lr 9.6859e-04 eta 0:11:08
epoch [104/200] batch [2/3] time 2.061 (2.184) data 0.000 (0.133) loss 0.1353 (0.1012) acc 100.0000 (100.0000) lr 9.6859e-04 eta 0:10:31
epoch [104/200] batch [3/3] time 2.063 (2.143) data 0.000 (0.089) loss 0.3242 (0.1755) acc 90.6250 (96.8750) lr 9.5289e-04 eta 0:10:17
epoch [105/200] batch [1/3] time 2.290 (2.290) data 0.250 (0.250) loss 0.0808 (0.0808) acc 100.0000 (100.0000) lr 9.5289e-04 eta 0:10:57
epoch [105/200] batch [2/3] time 2.061 (2.176) data 0.000 (0.125) loss 0.3528 (0.2168) acc 87.5000 (93.7500) lr 9.5289e-04 eta 0:10:22
epoch [105/200] batch [3/3] time 2.067 (2.139) data 0.000 (0.084) loss 0.3250 (0.2528) acc 87.5000 (91.6667) lr 9.3721e-04 eta 0:10:09
epoch [106/200] batch [1/3] time 2.307 (2.307) data 0.248 (0.248) loss 0.1294 (0.1294) acc 96.8750 (96.8750) lr 9.3721e-04 eta 0:10:55
epoch [106/200] batch [2/3] time 2.068 (2.187) data 0.000 (0.124) loss 0.2512 (0.1903) acc 93.7500 (95.3125) lr 9.3721e-04 eta 0:10:19
epoch [106/200] batch [3/3] time 2.067 (2.147) data 0.000 (0.083) loss 0.3271 (0.2359) acc 90.6250 (93.7500) lr 9.2154e-04 eta 0:10:05
epoch [107/200] batch [1/3] time 2.322 (2.322) data 0.250 (0.250) loss 0.2830 (0.2830) acc 93.7500 (93.7500) lr 9.2154e-04 eta 0:10:52
epoch [107/200] batch [2/3] time 2.068 (2.195) data 0.000 (0.125) loss 0.1455 (0.2142) acc 96.8750 (95.3125) lr 9.2154e-04 eta 0:10:14
epoch [107/200] batch [3/3] time 2.070 (2.154) data 0.000 (0.083) loss 0.3296 (0.2527) acc 90.6250 (93.7500) lr 9.0589e-04 eta 0:10:00
epoch [108/200] batch [1/3] time 2.310 (2.310) data 0.258 (0.258) loss 0.3508 (0.3508) acc 93.7500 (93.7500) lr 9.0589e-04 eta 0:10:42
epoch [108/200] batch [2/3] time 2.067 (2.188) data 0.000 (0.129) loss 0.1565 (0.2537) acc 100.0000 (96.8750) lr 9.0589e-04 eta 0:10:06
epoch [108/200] batch [3/3] time 2.066 (2.148) data 0.000 (0.086) loss 0.4026 (0.3033) acc 90.6250 (94.7917) lr 8.9027e-04 eta 0:09:52
epoch [109/200] batch [1/3] time 2.321 (2.321) data 0.256 (0.256) loss 0.2942 (0.2942) acc 93.7500 (93.7500) lr 8.9027e-04 eta 0:10:38
epoch [109/200] batch [2/3] time 2.051 (2.186) data 0.000 (0.128) loss 0.2345 (0.2643) acc 93.7500 (93.7500) lr 8.9027e-04 eta 0:09:58
epoch [109/200] batch [3/3] time 2.067 (2.146) data 0.000 (0.085) loss 0.1539 (0.2275) acc 96.8750 (94.7917) lr 8.7467e-04 eta 0:09:45
epoch [110/200] batch [1/3] time 2.328 (2.328) data 0.251 (0.251) loss 0.3835 (0.3835) acc 87.5000 (87.5000) lr 8.7467e-04 eta 0:10:33
epoch [110/200] batch [2/3] time 2.058 (2.193) data 0.000 (0.126) loss 0.0968 (0.2402) acc 100.0000 (93.7500) lr 8.7467e-04 eta 0:09:54
epoch [110/200] batch [3/3] time 2.072 (2.153) data 0.000 (0.084) loss 0.4163 (0.2989) acc 87.5000 (91.6667) lr 8.5910e-04 eta 0:09:41
epoch [111/200] batch [1/3] time 2.308 (2.308) data 0.249 (0.249) loss 0.1482 (0.1482) acc 96.8750 (96.8750) lr 8.5910e-04 eta 0:10:20
epoch [111/200] batch [2/3] time 2.067 (2.187) data 0.000 (0.125) loss 0.5410 (0.3446) acc 90.6250 (93.7500) lr 8.5910e-04 eta 0:09:46
epoch [111/200] batch [3/3] time 2.061 (2.145) data 0.000 (0.083) loss 0.2369 (0.3087) acc 93.7500 (93.7500) lr 8.4357e-04 eta 0:09:32
epoch [112/200] batch [1/3] time 2.336 (2.336) data 0.267 (0.267) loss 0.3071 (0.3071) acc 93.7500 (93.7500) lr 8.4357e-04 eta 0:10:21
epoch [112/200] batch [2/3] time 2.067 (2.202) data 0.000 (0.134) loss 0.2424 (0.2748) acc 90.6250 (92.1875) lr 8.4357e-04 eta 0:09:43
epoch [112/200] batch [3/3] time 2.057 (2.153) data 0.000 (0.089) loss 0.0750 (0.2082) acc 100.0000 (94.7917) lr 8.2807e-04 eta 0:09:28
epoch [113/200] batch [1/3] time 2.317 (2.317) data 0.267 (0.267) loss 0.1840 (0.1840) acc 93.7500 (93.7500) lr 8.2807e-04 eta 0:10:09
epoch [113/200] batch [2/3] time 2.060 (2.189) data 0.000 (0.133) loss 0.1477 (0.1658) acc 100.0000 (96.8750) lr 8.2807e-04 eta 0:09:33
epoch [113/200] batch [3/3] time 2.080 (2.152) data 0.000 (0.089) loss 0.2939 (0.2085) acc 96.8750 (96.8750) lr 8.1262e-04 eta 0:09:21
epoch [114/200] batch [1/3] time 2.318 (2.318) data 0.250 (0.250) loss 0.2325 (0.2325) acc 96.8750 (96.8750) lr 8.1262e-04 eta 0:10:02
epoch [114/200] batch [2/3] time 2.083 (2.200) data 0.000 (0.125) loss 0.3972 (0.3149) acc 87.5000 (92.1875) lr 8.1262e-04 eta 0:09:29
epoch [114/200] batch [3/3] time 2.068 (2.156) data 0.000 (0.084) loss 0.2935 (0.3077) acc 93.7500 (92.7083) lr 7.9721e-04 eta 0:09:16
epoch [115/200] batch [1/3] time 2.329 (2.329) data 0.251 (0.251) loss 0.4170 (0.4170) acc 90.6250 (90.6250) lr 7.9721e-04 eta 0:09:58
epoch [115/200] batch [2/3] time 2.060 (2.194) data 0.000 (0.126) loss 0.2834 (0.3502) acc 96.8750 (93.7500) lr 7.9721e-04 eta 0:09:21
epoch [115/200] batch [3/3] time 2.061 (2.150) data 0.000 (0.084) loss 0.3027 (0.3344) acc 93.7500 (93.7500) lr 7.8186e-04 eta 0:09:08
epoch [116/200] batch [1/3] time 2.324 (2.324) data 0.248 (0.248) loss 0.3503 (0.3503) acc 93.7500 (93.7500) lr 7.8186e-04 eta 0:09:50
epoch [116/200] batch [2/3] time 2.056 (2.190) data 0.000 (0.124) loss 0.2871 (0.3187) acc 90.6250 (92.1875) lr 7.8186e-04 eta 0:09:14
epoch [116/200] batch [3/3] time 2.072 (2.151) data 0.000 (0.083) loss 0.2993 (0.3123) acc 93.7500 (92.7083) lr 7.6655e-04 eta 0:09:02
epoch [117/200] batch [1/3] time 2.324 (2.324) data 0.258 (0.258) loss 0.1970 (0.1970) acc 93.7500 (93.7500) lr 7.6655e-04 eta 0:09:43
epoch [117/200] batch [2/3] time 2.052 (2.188) data 0.000 (0.129) loss 0.1880 (0.1925) acc 93.7500 (93.7500) lr 7.6655e-04 eta 0:09:07
epoch [117/200] batch [3/3] time 2.063 (2.146) data 0.000 (0.086) loss 0.2274 (0.2041) acc 93.7500 (93.7500) lr 7.5131e-04 eta 0:08:54
epoch [118/200] batch [1/3] time 2.328 (2.328) data 0.268 (0.268) loss 0.2808 (0.2808) acc 93.7500 (93.7500) lr 7.5131e-04 eta 0:09:37
epoch [118/200] batch [2/3] time 2.067 (2.198) data 0.000 (0.134) loss 0.2893 (0.2850) acc 93.7500 (93.7500) lr 7.5131e-04 eta 0:09:02
epoch [118/200] batch [3/3] time 2.043 (2.146) data 0.000 (0.089) loss 0.2588 (0.2763) acc 93.7500 (93.7500) lr 7.3613e-04 eta 0:08:47
epoch [119/200] batch [1/3] time 2.315 (2.315) data 0.253 (0.253) loss 0.4270 (0.4270) acc 90.6250 (90.6250) lr 7.3613e-04 eta 0:09:27
epoch [119/200] batch [2/3] time 2.080 (2.197) data 0.000 (0.126) loss 0.4138 (0.4204) acc 93.7500 (92.1875) lr 7.3613e-04 eta 0:08:56
epoch [119/200] batch [3/3] time 2.055 (2.150) data 0.000 (0.084) loss 0.1581 (0.3330) acc 93.7500 (92.7083) lr 7.2101e-04 eta 0:08:42
epoch [120/200] batch [1/3] time 2.337 (2.337) data 0.266 (0.266) loss 0.2300 (0.2300) acc 96.8750 (96.8750) lr 7.2101e-04 eta 0:09:25
epoch [120/200] batch [2/3] time 2.055 (2.196) data 0.000 (0.133) loss 0.2983 (0.2642) acc 93.7500 (95.3125) lr 7.2101e-04 eta 0:08:49
epoch [120/200] batch [3/3] time 2.070 (2.154) data 0.000 (0.089) loss 0.3655 (0.2979) acc 90.6250 (93.7500) lr 7.0596e-04 eta 0:08:36
epoch [121/200] batch [1/3] time 2.311 (2.311) data 0.267 (0.267) loss 0.0416 (0.0416) acc 100.0000 (100.0000) lr 7.0596e-04 eta 0:09:12
epoch [121/200] batch [2/3] time 2.107 (2.209) data 0.000 (0.134) loss 0.4910 (0.2663) acc 93.7500 (96.8750) lr 7.0596e-04 eta 0:08:45
epoch [121/200] batch [3/3] time 2.190 (2.203) data 0.000 (0.089) loss 0.2612 (0.2646) acc 93.7500 (95.8333) lr 6.9098e-04 eta 0:08:42
epoch [122/200] batch [1/3] time 2.343 (2.343) data 0.260 (0.260) loss 0.4602 (0.4602) acc 87.5000 (87.5000) lr 6.9098e-04 eta 0:09:12
epoch [122/200] batch [2/3] time 2.253 (2.298) data 0.000 (0.130) loss 0.5210 (0.4906) acc 87.5000 (87.5000) lr 6.9098e-04 eta 0:09:00
epoch [122/200] batch [3/3] time 2.161 (2.252) data 0.000 (0.087) loss 0.1749 (0.3854) acc 96.8750 (90.6250) lr 6.7608e-04 eta 0:08:47
epoch [123/200] batch [1/3] time 2.329 (2.329) data 0.251 (0.251) loss 0.2678 (0.2678) acc 90.6250 (90.6250) lr 6.7608e-04 eta 0:09:02
epoch [123/200] batch [2/3] time 2.096 (2.213) data 0.000 (0.126) loss 0.2339 (0.2509) acc 96.8750 (93.7500) lr 6.7608e-04 eta 0:08:33
epoch [123/200] batch [3/3] time 2.194 (2.206) data 0.000 (0.084) loss 0.4304 (0.3107) acc 90.6250 (92.7083) lr 6.6126e-04 eta 0:08:29
epoch [124/200] batch [1/3] time 2.344 (2.344) data 0.274 (0.274) loss 0.1793 (0.1793) acc 96.8750 (96.8750) lr 6.6126e-04 eta 0:08:59
epoch [124/200] batch [2/3] time 2.097 (2.220) data 0.000 (0.137) loss 0.3833 (0.2813) acc 90.6250 (93.7500) lr 6.6126e-04 eta 0:08:28
epoch [124/200] batch [3/3] time 2.314 (2.252) data 0.000 (0.092) loss 0.0456 (0.2027) acc 100.0000 (95.8333) lr 6.4653e-04 eta 0:08:33
epoch [125/200] batch [1/3] time 2.418 (2.418) data 0.263 (0.263) loss 0.3938 (0.3938) acc 90.6250 (90.6250) lr 6.4653e-04 eta 0:09:08
epoch [125/200] batch [2/3] time 2.395 (2.407) data 0.000 (0.131) loss 0.1722 (0.2830) acc 100.0000 (95.3125) lr 6.4653e-04 eta 0:09:03
epoch [125/200] batch [3/3] time 2.081 (2.298) data 0.000 (0.088) loss 0.2703 (0.2788) acc 93.7500 (94.7917) lr 6.3188e-04 eta 0:08:37
epoch [126/200] batch [1/3] time 2.501 (2.501) data 0.256 (0.256) loss 0.2849 (0.2849) acc 93.7500 (93.7500) lr 6.3188e-04 eta 0:09:20
epoch [126/200] batch [2/3] time 2.095 (2.298) data 0.000 (0.128) loss 0.5073 (0.3961) acc 87.5000 (90.6250) lr 6.3188e-04 eta 0:08:32
epoch [126/200] batch [3/3] time 2.085 (2.227) data 0.000 (0.085) loss 0.4001 (0.3975) acc 90.6250 (90.6250) lr 6.1732e-04 eta 0:08:14
epoch [127/200] batch [1/3] time 2.335 (2.335) data 0.253 (0.253) loss 0.1904 (0.1904) acc 96.8750 (96.8750) lr 6.1732e-04 eta 0:08:36
epoch [127/200] batch [2/3] time 2.083 (2.209) data 0.000 (0.126) loss 0.2766 (0.2335) acc 90.6250 (93.7500) lr 6.1732e-04 eta 0:08:05
epoch [127/200] batch [3/3] time 2.081 (2.166) data 0.000 (0.084) loss 0.2676 (0.2449) acc 93.7500 (93.7500) lr 6.0285e-04 eta 0:07:54
epoch [128/200] batch [1/3] time 2.324 (2.324) data 0.256 (0.256) loss 0.1240 (0.1240) acc 96.8750 (96.8750) lr 6.0285e-04 eta 0:08:26
epoch [128/200] batch [2/3] time 2.075 (2.199) data 0.000 (0.128) loss 0.1112 (0.1176) acc 96.8750 (96.8750) lr 6.0285e-04 eta 0:07:57
epoch [128/200] batch [3/3] time 2.080 (2.160) data 0.000 (0.085) loss 0.0824 (0.1059) acc 100.0000 (97.9167) lr 5.8849e-04 eta 0:07:46
epoch [129/200] batch [1/3] time 2.337 (2.337) data 0.261 (0.261) loss 0.3457 (0.3457) acc 93.7500 (93.7500) lr 5.8849e-04 eta 0:08:22
epoch [129/200] batch [2/3] time 2.071 (2.204) data 0.000 (0.131) loss 0.0685 (0.2071) acc 100.0000 (96.8750) lr 5.8849e-04 eta 0:07:51
epoch [129/200] batch [3/3] time 2.089 (2.165) data 0.000 (0.087) loss 0.3838 (0.2660) acc 93.7500 (95.8333) lr 5.7422e-04 eta 0:07:41
epoch [130/200] batch [1/3] time 2.325 (2.325) data 0.261 (0.261) loss 0.1953 (0.1953) acc 90.6250 (90.6250) lr 5.7422e-04 eta 0:08:12
epoch [130/200] batch [2/3] time 2.091 (2.208) data 0.000 (0.130) loss 0.4292 (0.3123) acc 90.6250 (90.6250) lr 5.7422e-04 eta 0:07:45
epoch [130/200] batch [3/3] time 2.081 (2.166) data 0.000 (0.087) loss 0.2383 (0.2876) acc 93.7500 (91.6667) lr 5.6006e-04 eta 0:07:34
epoch [131/200] batch [1/3] time 2.336 (2.336) data 0.266 (0.266) loss 0.1885 (0.1885) acc 93.7500 (93.7500) lr 5.6006e-04 eta 0:08:08
epoch [131/200] batch [2/3] time 2.082 (2.209) data 0.000 (0.133) loss 0.2688 (0.2286) acc 96.8750 (95.3125) lr 5.6006e-04 eta 0:07:39
epoch [131/200] batch [3/3] time 2.091 (2.169) data 0.000 (0.089) loss 0.2681 (0.2418) acc 90.6250 (93.7500) lr 5.4601e-04 eta 0:07:29
epoch [132/200] batch [1/3] time 2.346 (2.346) data 0.259 (0.259) loss 0.3591 (0.3591) acc 90.6250 (90.6250) lr 5.4601e-04 eta 0:08:03
epoch [132/200] batch [2/3] time 2.093 (2.219) data 0.000 (0.129) loss 0.4082 (0.3837) acc 90.6250 (90.6250) lr 5.4601e-04 eta 0:07:34
epoch [132/200] batch [3/3] time 2.092 (2.177) data 0.000 (0.086) loss 0.3645 (0.3773) acc 93.7500 (91.6667) lr 5.3207e-04 eta 0:07:24
epoch [133/200] batch [1/3] time 2.343 (2.343) data 0.261 (0.261) loss 0.2305 (0.2305) acc 93.7500 (93.7500) lr 5.3207e-04 eta 0:07:55
epoch [133/200] batch [2/3] time 2.070 (2.207) data 0.000 (0.131) loss 0.2502 (0.2404) acc 93.7500 (93.7500) lr 5.3207e-04 eta 0:07:25
epoch [133/200] batch [3/3] time 2.082 (2.165) data 0.000 (0.087) loss 0.2291 (0.2366) acc 90.6250 (92.7083) lr 5.1825e-04 eta 0:07:15
epoch [134/200] batch [1/3] time 2.338 (2.338) data 0.251 (0.251) loss 0.3271 (0.3271) acc 87.5000 (87.5000) lr 5.1825e-04 eta 0:07:47
epoch [134/200] batch [2/3] time 2.086 (2.212) data 0.000 (0.126) loss 0.0814 (0.2043) acc 100.0000 (93.7500) lr 5.1825e-04 eta 0:07:20
epoch [134/200] batch [3/3] time 2.072 (2.165) data 0.000 (0.084) loss 0.1932 (0.2006) acc 96.8750 (94.7917) lr 5.0454e-04 eta 0:07:08
epoch [135/200] batch [1/3] time 2.328 (2.328) data 0.253 (0.253) loss 0.1102 (0.1102) acc 100.0000 (100.0000) lr 5.0454e-04 eta 0:07:38
epoch [135/200] batch [2/3] time 2.073 (2.200) data 0.000 (0.126) loss 0.2751 (0.1927) acc 96.8750 (98.4375) lr 5.0454e-04 eta 0:07:11
epoch [135/200] batch [3/3] time 2.093 (2.165) data 0.000 (0.084) loss 0.4019 (0.2624) acc 93.7500 (96.8750) lr 4.9096e-04 eta 0:07:02
epoch [136/200] batch [1/3] time 2.343 (2.343) data 0.271 (0.271) loss 0.1510 (0.1510) acc 96.8750 (96.8750) lr 4.9096e-04 eta 0:07:34
epoch [136/200] batch [2/3] time 2.083 (2.213) data 0.000 (0.135) loss 0.5522 (0.3516) acc 87.5000 (92.1875) lr 4.9096e-04 eta 0:07:07
epoch [136/200] batch [3/3] time 2.090 (2.172) data 0.000 (0.090) loss 0.3655 (0.3562) acc 84.3750 (89.5833) lr 4.7750e-04 eta 0:06:57
epoch [137/200] batch [1/3] time 2.318 (2.318) data 0.252 (0.252) loss 0.1418 (0.1418) acc 100.0000 (100.0000) lr 4.7750e-04 eta 0:07:22
epoch [137/200] batch [2/3] time 2.085 (2.201) data 0.000 (0.126) loss 0.1963 (0.1691) acc 96.8750 (98.4375) lr 4.7750e-04 eta 0:06:58
epoch [137/200] batch [3/3] time 2.061 (2.154) data 0.000 (0.084) loss 0.1327 (0.1569) acc 96.8750 (97.9167) lr 4.6417e-04 eta 0:06:47
epoch [138/200] batch [1/3] time 2.349 (2.349) data 0.263 (0.263) loss 0.2310 (0.2310) acc 96.8750 (96.8750) lr 4.6417e-04 eta 0:07:21
epoch [138/200] batch [2/3] time 2.088 (2.219) data 0.000 (0.132) loss 0.4666 (0.3488) acc 90.6250 (93.7500) lr 4.6417e-04 eta 0:06:54
epoch [138/200] batch [3/3] time 2.090 (2.176) data 0.000 (0.088) loss 0.3367 (0.3447) acc 90.6250 (92.7083) lr 4.5098e-04 eta 0:06:44
epoch [139/200] batch [1/3] time 2.341 (2.341) data 0.262 (0.262) loss 0.1779 (0.1779) acc 93.7500 (93.7500) lr 4.5098e-04 eta 0:07:13
epoch [139/200] batch [2/3] time 2.073 (2.207) data 0.000 (0.131) loss 0.3845 (0.2812) acc 93.7500 (93.7500) lr 4.5098e-04 eta 0:06:46
epoch [139/200] batch [3/3] time 2.091 (2.168) data 0.000 (0.088) loss 0.2874 (0.2832) acc 93.7500 (93.7500) lr 4.3792e-04 eta 0:06:36
epoch [140/200] batch [1/3] time 2.335 (2.335) data 0.259 (0.259) loss 0.1073 (0.1073) acc 100.0000 (100.0000) lr 4.3792e-04 eta 0:07:05
epoch [140/200] batch [2/3] time 2.063 (2.199) data 0.000 (0.130) loss 0.0887 (0.0980) acc 96.8750 (98.4375) lr 4.3792e-04 eta 0:06:38
epoch [140/200] batch [3/3] time 2.067 (2.155) data 0.000 (0.087) loss 0.2583 (0.1514) acc 90.6250 (95.8333) lr 4.2499e-04 eta 0:06:27
epoch [141/200] batch [1/3] time 2.332 (2.332) data 0.251 (0.251) loss 0.4910 (0.4910) acc 87.5000 (87.5000) lr 4.2499e-04 eta 0:06:57
epoch [141/200] batch [2/3] time 2.082 (2.207) data 0.000 (0.125) loss 0.2192 (0.3551) acc 93.7500 (90.6250) lr 4.2499e-04 eta 0:06:32
epoch [141/200] batch [3/3] time 2.058 (2.157) data 0.000 (0.084) loss 0.0643 (0.2582) acc 100.0000 (93.7500) lr 4.1221e-04 eta 0:06:21
epoch [142/200] batch [1/3] time 2.297 (2.297) data 0.257 (0.257) loss 0.0629 (0.0629) acc 100.0000 (100.0000) lr 4.1221e-04 eta 0:06:44
epoch [142/200] batch [2/3] time 2.065 (2.181) data 0.000 (0.129) loss 0.1159 (0.0894) acc 100.0000 (100.0000) lr 4.1221e-04 eta 0:06:21
epoch [142/200] batch [3/3] time 2.085 (2.149) data 0.000 (0.086) loss 0.2271 (0.1353) acc 96.8750 (98.9583) lr 3.9958e-04 eta 0:06:13
epoch [143/200] batch [1/3] time 2.318 (2.318) data 0.253 (0.253) loss 0.1213 (0.1213) acc 96.8750 (96.8750) lr 3.9958e-04 eta 0:06:41
epoch [143/200] batch [2/3] time 2.059 (2.189) data 0.000 (0.126) loss 0.1903 (0.1558) acc 96.8750 (96.8750) lr 3.9958e-04 eta 0:06:16
epoch [143/200] batch [3/3] time 2.079 (2.152) data 0.000 (0.084) loss 0.3213 (0.2110) acc 93.7500 (95.8333) lr 3.8709e-04 eta 0:06:08
epoch [144/200] batch [1/3] time 2.345 (2.345) data 0.271 (0.271) loss 0.1858 (0.1858) acc 96.8750 (96.8750) lr 3.8709e-04 eta 0:06:38
epoch [144/200] batch [2/3] time 2.074 (2.210) data 0.000 (0.136) loss 0.2539 (0.2198) acc 90.6250 (93.7500) lr 3.8709e-04 eta 0:06:13
epoch [144/200] batch [3/3] time 2.077 (2.165) data 0.000 (0.091) loss 0.5088 (0.3162) acc 90.6250 (92.7083) lr 3.7476e-04 eta 0:06:03
epoch [145/200] batch [1/3] time 2.302 (2.302) data 0.254 (0.254) loss 0.0721 (0.0721) acc 100.0000 (100.0000) lr 3.7476e-04 eta 0:06:24
epoch [145/200] batch [2/3] time 2.091 (2.196) data 0.000 (0.127) loss 0.3694 (0.2207) acc 93.7500 (96.8750) lr 3.7476e-04 eta 0:06:04
epoch [145/200] batch [3/3] time 2.072 (2.155) data 0.000 (0.085) loss 0.2791 (0.2402) acc 93.7500 (95.8333) lr 3.6258e-04 eta 0:05:55
epoch [146/200] batch [1/3] time 2.320 (2.320) data 0.250 (0.250) loss 0.2629 (0.2629) acc 93.7500 (93.7500) lr 3.6258e-04 eta 0:06:20
epoch [146/200] batch [2/3] time 2.059 (2.189) data 0.000 (0.125) loss 0.1547 (0.2088) acc 96.8750 (95.3125) lr 3.6258e-04 eta 0:05:56
epoch [146/200] batch [3/3] time 2.087 (2.155) data 0.000 (0.083) loss 0.3816 (0.2664) acc 93.7500 (94.7917) lr 3.5055e-04 eta 0:05:49
epoch [147/200] batch [1/3] time 2.341 (2.341) data 0.260 (0.260) loss 0.2362 (0.2362) acc 93.7500 (93.7500) lr 3.5055e-04 eta 0:06:16
epoch [147/200] batch [2/3] time 2.049 (2.195) data 0.000 (0.130) loss 0.0765 (0.1564) acc 96.8750 (95.3125) lr 3.5055e-04 eta 0:05:51
epoch [147/200] batch [3/3] time 2.080 (2.157) data 0.000 (0.087) loss 0.2179 (0.1769) acc 96.8750 (95.8333) lr 3.3869e-04 eta 0:05:42
epoch [148/200] batch [1/3] time 2.320 (2.320) data 0.253 (0.253) loss 0.2637 (0.2637) acc 96.8750 (96.8750) lr 3.3869e-04 eta 0:06:06
epoch [148/200] batch [2/3] time 2.093 (2.207) data 0.000 (0.126) loss 0.4309 (0.3473) acc 93.7500 (95.3125) lr 3.3869e-04 eta 0:05:46
epoch [148/200] batch [3/3] time 2.073 (2.162) data 0.000 (0.084) loss 0.1964 (0.2970) acc 96.8750 (95.8333) lr 3.2699e-04 eta 0:05:37
epoch [149/200] batch [1/3] time 2.320 (2.320) data 0.271 (0.271) loss 0.1256 (0.1256) acc 96.8750 (96.8750) lr 3.2699e-04 eta 0:05:59
epoch [149/200] batch [2/3] time 2.094 (2.207) data 0.000 (0.135) loss 0.2986 (0.2121) acc 96.8750 (96.8750) lr 3.2699e-04 eta 0:05:39
epoch [149/200] batch [3/3] time 2.102 (2.172) data 0.000 (0.090) loss 0.4104 (0.2782) acc 90.6250 (94.7917) lr 3.1545e-04 eta 0:05:32
epoch [150/200] batch [1/3] time 2.318 (2.318) data 0.255 (0.255) loss 0.1589 (0.1589) acc 96.8750 (96.8750) lr 3.1545e-04 eta 0:05:52
epoch [150/200] batch [2/3] time 2.063 (2.191) data 0.000 (0.127) loss 0.1262 (0.1426) acc 96.8750 (96.8750) lr 3.1545e-04 eta 0:05:30
epoch [150/200] batch [3/3] time 2.071 (2.151) data 0.000 (0.085) loss 0.1780 (0.1544) acc 96.8750 (96.8750) lr 3.0409e-04 eta 0:05:22
epoch [151/200] batch [1/3] time 2.338 (2.338) data 0.269 (0.269) loss 0.2683 (0.2683) acc 93.7500 (93.7500) lr 3.0409e-04 eta 0:05:48
epoch [151/200] batch [2/3] time 2.060 (2.199) data 0.000 (0.135) loss 0.0646 (0.1664) acc 100.0000 (96.8750) lr 3.0409e-04 eta 0:05:25
epoch [151/200] batch [3/3] time 2.092 (2.163) data 0.000 (0.090) loss 0.5063 (0.2797) acc 84.3750 (92.7083) lr 2.9289e-04 eta 0:05:18
epoch [152/200] batch [1/3] time 2.351 (2.351) data 0.258 (0.258) loss 0.5122 (0.5122) acc 90.6250 (90.6250) lr 2.9289e-04 eta 0:05:43
epoch [152/200] batch [2/3] time 2.081 (2.216) data 0.000 (0.129) loss 0.2159 (0.3641) acc 96.8750 (93.7500) lr 2.9289e-04 eta 0:05:21
epoch [152/200] batch [3/3] time 2.089 (2.174) data 0.000 (0.086) loss 0.3469 (0.3584) acc 90.6250 (92.7083) lr 2.8187e-04 eta 0:05:13
epoch [153/200] batch [1/3] time 2.335 (2.335) data 0.264 (0.264) loss 0.1617 (0.1617) acc 100.0000 (100.0000) lr 2.8187e-04 eta 0:05:33
epoch [153/200] batch [2/3] time 2.081 (2.208) data 0.000 (0.132) loss 0.3254 (0.2436) acc 93.7500 (96.8750) lr 2.8187e-04 eta 0:05:13
epoch [153/200] batch [3/3] time 2.081 (2.166) data 0.000 (0.088) loss 0.1592 (0.2155) acc 100.0000 (97.9167) lr 2.7103e-04 eta 0:05:05
epoch [154/200] batch [1/3] time 2.328 (2.328) data 0.253 (0.253) loss 0.2576 (0.2576) acc 96.8750 (96.8750) lr 2.7103e-04 eta 0:05:25
epoch [154/200] batch [2/3] time 2.083 (2.206) data 0.000 (0.126) loss 0.4355 (0.3466) acc 93.7500 (95.3125) lr 2.7103e-04 eta 0:05:06
epoch [154/200] batch [3/3] time 2.073 (2.162) data 0.000 (0.084) loss 0.1388 (0.2773) acc 96.8750 (95.8333) lr 2.6037e-04 eta 0:04:58
epoch [155/200] batch [1/3] time 2.329 (2.329) data 0.264 (0.264) loss 0.1226 (0.1226) acc 100.0000 (100.0000) lr 2.6037e-04 eta 0:05:19
epoch [155/200] batch [2/3] time 2.073 (2.201) data 0.000 (0.132) loss 0.1346 (0.1286) acc 100.0000 (100.0000) lr 2.6037e-04 eta 0:04:59
epoch [155/200] batch [3/3] time 2.081 (2.161) data 0.000 (0.088) loss 0.1865 (0.1479) acc 93.7500 (97.9167) lr 2.4989e-04 eta 0:04:51
epoch [156/200] batch [1/3] time 2.320 (2.320) data 0.250 (0.250) loss 0.1055 (0.1055) acc 100.0000 (100.0000) lr 2.4989e-04 eta 0:05:10
epoch [156/200] batch [2/3] time 2.071 (2.196) data 0.000 (0.125) loss 0.2434 (0.1744) acc 93.7500 (96.8750) lr 2.4989e-04 eta 0:04:52
epoch [156/200] batch [3/3] time 2.094 (2.162) data 0.000 (0.083) loss 0.3391 (0.2293) acc 93.7500 (95.8333) lr 2.3959e-04 eta 0:04:45
epoch [157/200] batch [1/3] time 2.325 (2.325) data 0.250 (0.250) loss 0.1666 (0.1666) acc 100.0000 (100.0000) lr 2.3959e-04 eta 0:05:04
epoch [157/200] batch [2/3] time 2.083 (2.204) data 0.000 (0.125) loss 0.4958 (0.3312) acc 90.6250 (95.3125) lr 2.3959e-04 eta 0:04:46
epoch [157/200] batch [3/3] time 2.086 (2.164) data 0.000 (0.083) loss 0.1609 (0.2745) acc 96.8750 (95.8333) lr 2.2949e-04 eta 0:04:39
epoch [158/200] batch [1/3] time 2.327 (2.327) data 0.260 (0.260) loss 0.1005 (0.1005) acc 100.0000 (100.0000) lr 2.2949e-04 eta 0:04:57
epoch [158/200] batch [2/3] time 2.043 (2.185) data 0.000 (0.130) loss 0.0709 (0.0857) acc 100.0000 (100.0000) lr 2.2949e-04 eta 0:04:37
epoch [158/200] batch [3/3] time 2.061 (2.144) data 0.000 (0.087) loss 0.1759 (0.1157) acc 96.8750 (98.9583) lr 2.1957e-04 eta 0:04:30
epoch [159/200] batch [1/3] time 2.343 (2.343) data 0.268 (0.268) loss 0.2012 (0.2012) acc 96.8750 (96.8750) lr 2.1957e-04 eta 0:04:52
epoch [159/200] batch [2/3] time 2.075 (2.209) data 0.000 (0.134) loss 0.2239 (0.2125) acc 93.7500 (95.3125) lr 2.1957e-04 eta 0:04:33
epoch [159/200] batch [3/3] time 2.090 (2.169) data 0.000 (0.090) loss 0.3633 (0.2628) acc 87.5000 (92.7083) lr 2.0984e-04 eta 0:04:26
epoch [160/200] batch [1/3] time 2.335 (2.335) data 0.269 (0.269) loss 0.3572 (0.3572) acc 93.7500 (93.7500) lr 2.0984e-04 eta 0:04:44
epoch [160/200] batch [2/3] time 2.068 (2.202) data 0.000 (0.135) loss 0.4109 (0.3840) acc 93.7500 (93.7500) lr 2.0984e-04 eta 0:04:26
epoch [160/200] batch [3/3] time 2.078 (2.161) data 0.000 (0.090) loss 0.2812 (0.3498) acc 90.6250 (92.7083) lr 2.0032e-04 eta 0:04:19
epoch [161/200] batch [1/3] time 2.305 (2.305) data 0.252 (0.252) loss 0.0657 (0.0657) acc 100.0000 (100.0000) lr 2.0032e-04 eta 0:04:34
epoch [161/200] batch [2/3] time 2.076 (2.190) data 0.000 (0.126) loss 0.1611 (0.1134) acc 96.8750 (98.4375) lr 2.0032e-04 eta 0:04:18
epoch [161/200] batch [3/3] time 2.058 (2.146) data 0.000 (0.084) loss 0.0637 (0.0968) acc 100.0000 (98.9583) lr 1.9098e-04 eta 0:04:11
epoch [162/200] batch [1/3] time 2.344 (2.344) data 0.260 (0.260) loss 0.3108 (0.3108) acc 90.6250 (90.6250) lr 1.9098e-04 eta 0:04:31
epoch [162/200] batch [2/3] time 2.053 (2.198) data 0.000 (0.130) loss 0.1111 (0.2109) acc 96.8750 (93.7500) lr 1.9098e-04 eta 0:04:12
epoch [162/200] batch [3/3] time 2.074 (2.157) data 0.000 (0.087) loss 0.0923 (0.1714) acc 100.0000 (95.8333) lr 1.8185e-04 eta 0:04:05
epoch [163/200] batch [1/3] time 2.346 (2.346) data 0.260 (0.260) loss 0.5088 (0.5088) acc 84.3750 (84.3750) lr 1.8185e-04 eta 0:04:25
epoch [163/200] batch [2/3] time 2.086 (2.216) data 0.000 (0.130) loss 0.2937 (0.4012) acc 93.7500 (89.0625) lr 1.8185e-04 eta 0:04:08
epoch [163/200] batch [3/3] time 2.062 (2.165) data 0.000 (0.087) loss 0.2131 (0.3385) acc 96.8750 (91.6667) lr 1.7292e-04 eta 0:04:00
epoch [164/200] batch [1/3] time 2.334 (2.334) data 0.267 (0.267) loss 0.1910 (0.1910) acc 96.8750 (96.8750) lr 1.7292e-04 eta 0:04:16
epoch [164/200] batch [2/3] time 2.071 (2.202) data 0.000 (0.134) loss 0.3757 (0.2834) acc 93.7500 (95.3125) lr 1.7292e-04 eta 0:04:00
epoch [164/200] batch [3/3] time 2.078 (2.161) data 0.000 (0.089) loss 0.3232 (0.2967) acc 90.6250 (93.7500) lr 1.6419e-04 eta 0:03:53
epoch [165/200] batch [1/3] time 2.318 (2.318) data 0.248 (0.248) loss 0.1765 (0.1765) acc 96.8750 (96.8750) lr 1.6419e-04 eta 0:04:08
epoch [165/200] batch [2/3] time 2.093 (2.206) data 0.000 (0.124) loss 0.2329 (0.2047) acc 96.8750 (96.8750) lr 1.6419e-04 eta 0:03:53
epoch [165/200] batch [3/3] time 2.083 (2.165) data 0.000 (0.083) loss 0.3188 (0.2428) acc 93.7500 (95.8333) lr 1.5567e-04 eta 0:03:47
epoch [166/200] batch [1/3] time 2.313 (2.313) data 0.255 (0.255) loss 0.0871 (0.0871) acc 100.0000 (100.0000) lr 1.5567e-04 eta 0:04:00
epoch [166/200] batch [2/3] time 2.093 (2.203) data 0.000 (0.127) loss 0.5010 (0.2940) acc 87.5000 (93.7500) lr 1.5567e-04 eta 0:03:46
epoch [166/200] batch [3/3] time 2.090 (2.165) data 0.000 (0.085) loss 0.2050 (0.2643) acc 100.0000 (95.8333) lr 1.4736e-04 eta 0:03:40
epoch [167/200] batch [1/3] time 2.301 (2.301) data 0.251 (0.251) loss 0.0831 (0.0831) acc 96.8750 (96.8750) lr 1.4736e-04 eta 0:03:52
epoch [167/200] batch [2/3] time 2.090 (2.196) data 0.000 (0.125) loss 0.3096 (0.1964) acc 93.7500 (95.3125) lr 1.4736e-04 eta 0:03:39
epoch [167/200] batch [3/3] time 2.068 (2.153) data 0.000 (0.084) loss 0.0942 (0.1623) acc 100.0000 (96.8750) lr 1.3926e-04 eta 0:03:33
epoch [168/200] batch [1/3] time 2.338 (2.338) data 0.272 (0.272) loss 0.2549 (0.2549) acc 93.7500 (93.7500) lr 1.3926e-04 eta 0:03:49
epoch [168/200] batch [2/3] time 2.077 (2.207) data 0.000 (0.136) loss 0.2642 (0.2595) acc 90.6250 (92.1875) lr 1.3926e-04 eta 0:03:34
epoch [168/200] batch [3/3] time 2.077 (2.164) data 0.000 (0.091) loss 0.2397 (0.2529) acc 93.7500 (92.7083) lr 1.3137e-04 eta 0:03:27
epoch [169/200] batch [1/3] time 2.303 (2.303) data 0.252 (0.252) loss 0.0804 (0.0804) acc 100.0000 (100.0000) lr 1.3137e-04 eta 0:03:38
epoch [169/200] batch [2/3] time 2.083 (2.193) data 0.000 (0.126) loss 0.4680 (0.2742) acc 90.6250 (95.3125) lr 1.3137e-04 eta 0:03:26
epoch [169/200] batch [3/3] time 2.084 (2.157) data 0.000 (0.084) loss 0.1592 (0.2359) acc 96.8750 (95.8333) lr 1.2369e-04 eta 0:03:20
epoch [170/200] batch [1/3] time 2.325 (2.325) data 0.254 (0.254) loss 0.1295 (0.1295) acc 100.0000 (100.0000) lr 1.2369e-04 eta 0:03:33
epoch [170/200] batch [2/3] time 2.077 (2.201) data 0.000 (0.127) loss 0.1510 (0.1403) acc 96.8750 (98.4375) lr 1.2369e-04 eta 0:03:20
epoch [170/200] batch [3/3] time 2.065 (2.156) data 0.000 (0.085) loss 0.0957 (0.1254) acc 100.0000 (98.9583) lr 1.1623e-04 eta 0:03:14
epoch [171/200] batch [1/3] time 2.336 (2.336) data 0.253 (0.253) loss 0.2925 (0.2925) acc 93.7500 (93.7500) lr 1.1623e-04 eta 0:03:27
epoch [171/200] batch [2/3] time 2.080 (2.208) data 0.000 (0.127) loss 0.1497 (0.2211) acc 96.8750 (95.3125) lr 1.1623e-04 eta 0:03:14
epoch [171/200] batch [3/3] time 2.070 (2.162) data 0.000 (0.084) loss 0.1687 (0.2036) acc 96.8750 (95.8333) lr 1.0899e-04 eta 0:03:08
epoch [172/200] batch [1/3] time 2.413 (2.413) data 0.253 (0.253) loss 0.0938 (0.0938) acc 96.8750 (96.8750) lr 1.0899e-04 eta 0:03:27
epoch [172/200] batch [2/3] time 2.150 (2.281) data 0.000 (0.126) loss 0.0992 (0.0965) acc 100.0000 (98.4375) lr 1.0899e-04 eta 0:03:13
epoch [172/200] batch [3/3] time 2.135 (2.232) data 0.000 (0.084) loss 0.1591 (0.1174) acc 96.8750 (97.9167) lr 1.0197e-04 eta 0:03:07
epoch [173/200] batch [1/3] time 2.378 (2.378) data 0.272 (0.272) loss 0.2568 (0.2568) acc 93.7500 (93.7500) lr 1.0197e-04 eta 0:03:17
epoch [173/200] batch [2/3] time 2.077 (2.227) data 0.000 (0.136) loss 0.2258 (0.2413) acc 93.7500 (93.7500) lr 1.0197e-04 eta 0:03:02
epoch [173/200] batch [3/3] time 2.098 (2.184) data 0.000 (0.091) loss 0.4753 (0.3193) acc 93.7500 (93.7500) lr 9.5173e-05 eta 0:02:56
epoch [174/200] batch [1/3] time 2.319 (2.319) data 0.268 (0.268) loss 0.2073 (0.2073) acc 93.7500 (93.7500) lr 9.5173e-05 eta 0:03:05
epoch [174/200] batch [2/3] time 2.078 (2.199) data 0.000 (0.134) loss 0.4622 (0.3347) acc 90.6250 (92.1875) lr 9.5173e-05 eta 0:02:53
epoch [174/200] batch [3/3] time 2.055 (2.151) data 0.000 (0.089) loss 0.1532 (0.2742) acc 93.7500 (92.7083) lr 8.8597e-05 eta 0:02:47
epoch [175/200] batch [1/3] time 2.332 (2.332) data 0.263 (0.263) loss 0.1887 (0.1887) acc 96.8750 (96.8750) lr 8.8597e-05 eta 0:02:59
epoch [175/200] batch [2/3] time 2.052 (2.192) data 0.000 (0.132) loss 0.2117 (0.2002) acc 93.7500 (95.3125) lr 8.8597e-05 eta 0:02:46
epoch [175/200] batch [3/3] time 2.074 (2.153) data 0.000 (0.088) loss 0.4287 (0.2764) acc 87.5000 (92.7083) lr 8.2245e-05 eta 0:02:41
epoch [176/200] batch [1/3] time 2.314 (2.314) data 0.251 (0.251) loss 0.3430 (0.3430) acc 93.7500 (93.7500) lr 8.2245e-05 eta 0:02:51
epoch [176/200] batch [2/3] time 2.052 (2.183) data 0.000 (0.125) loss 0.3235 (0.3333) acc 87.5000 (90.6250) lr 8.2245e-05 eta 0:02:39
epoch [176/200] batch [3/3] time 2.065 (2.143) data 0.000 (0.084) loss 0.2480 (0.3049) acc 93.7500 (91.6667) lr 7.6120e-05 eta 0:02:34
epoch [177/200] batch [1/3] time 2.314 (2.314) data 0.256 (0.256) loss 0.1288 (0.1288) acc 96.8750 (96.8750) lr 7.6120e-05 eta 0:02:44
epoch [177/200] batch [2/3] time 2.048 (2.181) data 0.000 (0.128) loss 0.0764 (0.1026) acc 100.0000 (98.4375) lr 7.6120e-05 eta 0:02:32
epoch [177/200] batch [3/3] time 2.072 (2.145) data 0.000 (0.085) loss 0.3093 (0.1715) acc 90.6250 (95.8333) lr 7.0224e-05 eta 0:02:27
epoch [178/200] batch [1/3] time 2.341 (2.341) data 0.267 (0.267) loss 0.2347 (0.2347) acc 93.7500 (93.7500) lr 7.0224e-05 eta 0:02:39
epoch [178/200] batch [2/3] time 2.053 (2.197) data 0.000 (0.134) loss 0.2440 (0.2394) acc 90.6250 (92.1875) lr 7.0224e-05 eta 0:02:27
epoch [178/200] batch [3/3] time 2.066 (2.153) data 0.000 (0.089) loss 0.3423 (0.2737) acc 93.7500 (92.7083) lr 6.4556e-05 eta 0:02:22
epoch [179/200] batch [1/3] time 2.303 (2.303) data 0.248 (0.248) loss 0.1412 (0.1412) acc 96.8750 (96.8750) lr 6.4556e-05 eta 0:02:29
epoch [179/200] batch [2/3] time 2.055 (2.179) data 0.000 (0.124) loss 0.1683 (0.1548) acc 96.8750 (96.8750) lr 6.4556e-05 eta 0:02:19
epoch [179/200] batch [3/3] time 2.070 (2.143) data 0.000 (0.083) loss 0.2856 (0.1984) acc 90.6250 (94.7917) lr 5.9119e-05 eta 0:02:15
epoch [180/200] batch [1/3] time 2.315 (2.315) data 0.257 (0.257) loss 0.1284 (0.1284) acc 96.8750 (96.8750) lr 5.9119e-05 eta 0:02:23
epoch [180/200] batch [2/3] time 2.064 (2.190) data 0.000 (0.129) loss 0.2228 (0.1756) acc 96.8750 (96.8750) lr 5.9119e-05 eta 0:02:13
epoch [180/200] batch [3/3] time 2.069 (2.149) data 0.000 (0.086) loss 0.1935 (0.1816) acc 93.7500 (95.8333) lr 5.3915e-05 eta 0:02:08
epoch [181/200] batch [1/3] time 2.306 (2.306) data 0.267 (0.267) loss 0.0698 (0.0698) acc 96.8750 (96.8750) lr 5.3915e-05 eta 0:02:16
epoch [181/200] batch [2/3] time 2.066 (2.186) data 0.000 (0.134) loss 0.3274 (0.1986) acc 93.7500 (95.3125) lr 5.3915e-05 eta 0:02:06
epoch [181/200] batch [3/3] time 2.057 (2.143) data 0.000 (0.089) loss 0.2034 (0.2002) acc 96.8750 (95.8333) lr 4.8943e-05 eta 0:02:02
epoch [182/200] batch [1/3] time 2.305 (2.305) data 0.254 (0.254) loss 0.1973 (0.1973) acc 93.7500 (93.7500) lr 4.8943e-05 eta 0:02:09
epoch [182/200] batch [2/3] time 2.062 (2.184) data 0.000 (0.127) loss 0.2292 (0.2133) acc 93.7500 (93.7500) lr 4.8943e-05 eta 0:02:00
epoch [182/200] batch [3/3] time 2.067 (2.145) data 0.000 (0.085) loss 0.3020 (0.2428) acc 90.6250 (92.7083) lr 4.4207e-05 eta 0:01:55
epoch [183/200] batch [1/3] time 2.315 (2.315) data 0.256 (0.256) loss 0.1348 (0.1348) acc 96.8750 (96.8750) lr 4.4207e-05 eta 0:02:02
epoch [183/200] batch [2/3] time 2.067 (2.191) data 0.000 (0.128) loss 0.5298 (0.3323) acc 90.6250 (93.7500) lr 4.4207e-05 eta 0:01:53
epoch [183/200] batch [3/3] time 2.030 (2.137) data 0.000 (0.085) loss 0.0597 (0.2414) acc 100.0000 (95.8333) lr 3.9706e-05 eta 0:01:49
epoch [184/200] batch [1/3] time 2.333 (2.333) data 0.259 (0.259) loss 0.2739 (0.2739) acc 90.6250 (90.6250) lr 3.9706e-05 eta 0:01:56
epoch [184/200] batch [2/3] time 2.029 (2.181) data 0.000 (0.129) loss 0.0782 (0.1761) acc 100.0000 (95.3125) lr 3.9706e-05 eta 0:01:46
epoch [184/200] batch [3/3] time 2.057 (2.140) data 0.000 (0.086) loss 0.1969 (0.1830) acc 93.7500 (94.7917) lr 3.5443e-05 eta 0:01:42
epoch [185/200] batch [1/3] time 2.312 (2.312) data 0.254 (0.254) loss 0.1031 (0.1031) acc 100.0000 (100.0000) lr 3.5443e-05 eta 0:01:48
epoch [185/200] batch [2/3] time 2.058 (2.185) data 0.000 (0.127) loss 0.2061 (0.1546) acc 96.8750 (98.4375) lr 3.5443e-05 eta 0:01:40
epoch [185/200] batch [3/3] time 2.064 (2.144) data 0.000 (0.085) loss 0.2664 (0.1919) acc 93.7500 (96.8750) lr 3.1417e-05 eta 0:01:36
epoch [186/200] batch [1/3] time 2.321 (2.321) data 0.268 (0.268) loss 0.2273 (0.2273) acc 90.6250 (90.6250) lr 3.1417e-05 eta 0:01:42
epoch [186/200] batch [2/3] time 2.069 (2.195) data 0.000 (0.134) loss 0.3962 (0.3118) acc 90.6250 (90.6250) lr 3.1417e-05 eta 0:01:34
epoch [186/200] batch [3/3] time 2.060 (2.150) data 0.000 (0.089) loss 0.2893 (0.3043) acc 96.8750 (92.7083) lr 2.7630e-05 eta 0:01:30
epoch [187/200] batch [1/3] time 2.317 (2.317) data 0.260 (0.260) loss 0.1975 (0.1975) acc 93.7500 (93.7500) lr 2.7630e-05 eta 0:01:34
epoch [187/200] batch [2/3] time 2.042 (2.180) data 0.000 (0.130) loss 0.2441 (0.2208) acc 96.8750 (95.3125) lr 2.7630e-05 eta 0:01:27
epoch [187/200] batch [3/3] time 2.042 (2.134) data 0.000 (0.087) loss 0.0934 (0.1783) acc 96.8750 (95.8333) lr 2.4083e-05 eta 0:01:23
epoch [188/200] batch [1/3] time 2.303 (2.303) data 0.249 (0.249) loss 0.1128 (0.1128) acc 100.0000 (100.0000) lr 2.4083e-05 eta 0:01:27
epoch [188/200] batch [2/3] time 2.053 (2.178) data 0.000 (0.125) loss 0.1290 (0.1209) acc 96.8750 (98.4375) lr 2.4083e-05 eta 0:01:20
epoch [188/200] batch [3/3] time 2.066 (2.141) data 0.000 (0.083) loss 0.2605 (0.1674) acc 90.6250 (95.8333) lr 2.0777e-05 eta 0:01:17
epoch [189/200] batch [1/3] time 2.308 (2.308) data 0.250 (0.250) loss 0.3503 (0.3503) acc 96.8750 (96.8750) lr 2.0777e-05 eta 0:01:20
epoch [189/200] batch [2/3] time 2.063 (2.186) data 0.000 (0.125) loss 0.1713 (0.2608) acc 96.8750 (96.8750) lr 2.0777e-05 eta 0:01:14
epoch [189/200] batch [3/3] time 2.066 (2.146) data 0.000 (0.083) loss 0.2668 (0.2628) acc 93.7500 (95.8333) lr 1.7713e-05 eta 0:01:10
epoch [190/200] batch [1/3] time 2.327 (2.327) data 0.260 (0.260) loss 0.2454 (0.2454) acc 93.7500 (93.7500) lr 1.7713e-05 eta 0:01:14
epoch [190/200] batch [2/3] time 2.055 (2.191) data 0.000 (0.130) loss 0.1523 (0.1989) acc 100.0000 (96.8750) lr 1.7713e-05 eta 0:01:07
epoch [190/200] batch [3/3] time 2.071 (2.151) data 0.000 (0.087) loss 0.2386 (0.2121) acc 96.8750 (96.8750) lr 1.4891e-05 eta 0:01:04
epoch [191/200] batch [1/3] time 2.321 (2.321) data 0.273 (0.273) loss 0.1153 (0.1153) acc 96.8750 (96.8750) lr 1.4891e-05 eta 0:01:07
epoch [191/200] batch [2/3] time 2.068 (2.195) data 0.000 (0.137) loss 0.2267 (0.1710) acc 93.7500 (95.3125) lr 1.4891e-05 eta 0:01:01
epoch [191/200] batch [3/3] time 2.065 (2.152) data 0.000 (0.091) loss 0.1891 (0.1770) acc 96.8750 (95.8333) lr 1.2312e-05 eta 0:00:58
epoch [192/200] batch [1/3] time 2.311 (2.311) data 0.258 (0.258) loss 0.2386 (0.2386) acc 93.7500 (93.7500) lr 1.2312e-05 eta 0:01:00
epoch [192/200] batch [2/3] time 2.062 (2.187) data 0.000 (0.129) loss 0.3911 (0.3149) acc 84.3750 (89.0625) lr 1.2312e-05 eta 0:00:54
epoch [192/200] batch [3/3] time 2.073 (2.149) data 0.000 (0.086) loss 0.2306 (0.2868) acc 96.8750 (91.6667) lr 9.9763e-06 eta 0:00:51
epoch [193/200] batch [1/3] time 2.321 (2.321) data 0.269 (0.269) loss 0.1953 (0.1953) acc 96.8750 (96.8750) lr 9.9763e-06 eta 0:00:53
epoch [193/200] batch [2/3] time 2.038 (2.179) data 0.000 (0.135) loss 0.1992 (0.1973) acc 96.8750 (96.8750) lr 9.9763e-06 eta 0:00:47
epoch [193/200] batch [3/3] time 2.070 (2.143) data 0.000 (0.090) loss 0.3721 (0.2555) acc 90.6250 (94.7917) lr 7.8853e-06 eta 0:00:45
epoch [194/200] batch [1/3] time 2.313 (2.313) data 0.253 (0.253) loss 0.2656 (0.2656) acc 90.6250 (90.6250) lr 7.8853e-06 eta 0:00:46
epoch [194/200] batch [2/3] time 2.077 (2.195) data 0.000 (0.127) loss 0.4363 (0.3510) acc 90.6250 (90.6250) lr 7.8853e-06 eta 0:00:41
epoch [194/200] batch [3/3] time 2.062 (2.150) data 0.000 (0.084) loss 0.1877 (0.2965) acc 96.8750 (92.7083) lr 6.0390e-06 eta 0:00:38
epoch [195/200] batch [1/3] time 2.320 (2.320) data 0.259 (0.259) loss 0.4714 (0.4714) acc 90.6250 (90.6250) lr 6.0390e-06 eta 0:00:39
epoch [195/200] batch [2/3] time 2.058 (2.189) data 0.000 (0.130) loss 0.2125 (0.3420) acc 96.8750 (93.7500) lr 6.0390e-06 eta 0:00:35
epoch [195/200] batch [3/3] time 2.077 (2.152) data 0.000 (0.086) loss 0.4648 (0.3829) acc 87.5000 (91.6667) lr 4.4380e-06 eta 0:00:32
epoch [196/200] batch [1/3] time 2.326 (2.326) data 0.249 (0.249) loss 0.4153 (0.4153) acc 87.5000 (87.5000) lr 4.4380e-06 eta 0:00:32
epoch [196/200] batch [2/3] time 2.052 (2.189) data 0.000 (0.125) loss 0.2607 (0.3380) acc 93.7500 (90.6250) lr 4.4380e-06 eta 0:00:28
epoch [196/200] batch [3/3] time 2.063 (2.147) data 0.000 (0.083) loss 0.2484 (0.3081) acc 90.6250 (90.6250) lr 3.0827e-06 eta 0:00:25
epoch [197/200] batch [1/3] time 2.328 (2.328) data 0.266 (0.266) loss 0.1863 (0.1863) acc 96.8750 (96.8750) lr 3.0827e-06 eta 0:00:25
epoch [197/200] batch [2/3] time 2.064 (2.196) data 0.000 (0.133) loss 0.3716 (0.2789) acc 90.6250 (93.7500) lr 3.0827e-06 eta 0:00:21
epoch [197/200] batch [3/3] time 2.075 (2.156) data 0.000 (0.089) loss 0.5420 (0.3666) acc 90.6250 (92.7083) lr 1.9733e-06 eta 0:00:19
epoch [198/200] batch [1/3] time 2.308 (2.308) data 0.259 (0.259) loss 0.0981 (0.0981) acc 100.0000 (100.0000) lr 1.9733e-06 eta 0:00:18
epoch [198/200] batch [2/3] time 2.062 (2.185) data 0.000 (0.130) loss 0.3977 (0.2479) acc 90.6250 (95.3125) lr 1.9733e-06 eta 0:00:15
epoch [198/200] batch [3/3] time 2.063 (2.144) data 0.000 (0.087) loss 0.3105 (0.2688) acc 93.7500 (94.7917) lr 1.1101e-06 eta 0:00:12
epoch [199/200] batch [1/3] time 2.329 (2.329) data 0.268 (0.268) loss 0.1163 (0.1163) acc 100.0000 (100.0000) lr 1.1101e-06 eta 0:00:11
epoch [199/200] batch [2/3] time 2.048 (2.188) data 0.000 (0.134) loss 0.0765 (0.0964) acc 100.0000 (100.0000) lr 1.1101e-06 eta 0:00:08
epoch [199/200] batch [3/3] time 2.055 (2.144) data 0.000 (0.089) loss 0.0834 (0.0921) acc 100.0000 (100.0000) lr 4.9344e-07 eta 0:00:06
epoch [200/200] batch [1/3] time 2.313 (2.313) data 0.259 (0.259) loss 0.1506 (0.1506) acc 96.8750 (96.8750) lr 4.9344e-07 eta 0:00:04
epoch [200/200] batch [2/3] time 2.062 (2.187) data 0.000 (0.129) loss 0.1648 (0.1577) acc 100.0000 (98.4375) lr 4.9344e-07 eta 0:00:02
epoch [200/200] batch [3/3] time 2.069 (2.148) data 0.000 (0.086) loss 0.3042 (0.2065) acc 90.6250 (95.8333) lr 1.2337e-07 eta 0:00:00
Checkpoint saved to output/Caltech/1/2/3/prompt_learner/model.pth.tar-200
Finish training
Deploy the last-epoch model
Evaluate on the *test* set
=> result
* total: 2,465
* correct: 1,997
* accuracy: 81.0%
* error: 19.0%
* macro_f1: 73.9%
Elapsed: 0:22:15
