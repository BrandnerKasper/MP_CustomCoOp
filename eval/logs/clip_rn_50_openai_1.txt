args2: backbone=, config_file=configs/trainers/CoOp/rn50.yaml, dataset_config_file=configs/datasets/caltech101.yaml, eval_only=False, head=, load_epoch=None, model_dir=, no_train=False,  opts: ['TRAINER.COOP.N_CTX', '16', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '1'], output_dir=output/Caltech/1, resume=, root=/home/brandnerkasper/Uni/MP/MP_CustomCoOp/data, seed=1, source_domains=None, target_domains=None, trainer=CoOp, transforms=None
Setting fixed seed: 1
***************
** Arguments **
***************
config_file: configs/trainers/CoOp/rn50.yaml
csc: False
ctp: end
dataset_config_file: configs/datasets/caltech101.yaml
n_ctx: 16
opts: ['TRAINER.COOP.N_CTX', '16', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '1']
output_dir: output/Caltech/1
root: /home/brandnerkasper/Uni/MP/MP_CustomCoOp/data
seed: 1
shots: 1
trainer: CoOp
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 4
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: Caltech101
  NUM_LABELED: -1
  NUM_SHOTS: 1
  ROOT: /home/brandnerkasper/Uni/MP/MP_CustomCoOp/data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 200
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/Caltech/1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: 
    N_CTX: 16
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: 
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: CoOp
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 2.0.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 22.04.3 LTS (x86_64)
GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.35

Python version: 3.10.12 (main, Jul  5 2023, 18:54:27) [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-6.2.0-32-generic-x86_64-with-glibc2.35
Is CUDA available: True
CUDA runtime version: 11.5.119
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA GeForce GTX 970
Nvidia driver version: 525.125.06
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Architecture:                       x86_64
CPU op-mode(s):                     32-bit, 64-bit
Address sizes:                      39 bits physical, 48 bits virtual
Byte Order:                         Little Endian
CPU(s):                             4
On-line CPU(s) list:                0-3
Vendor ID:                          GenuineIntel
Model name:                         Intel(R) Xeon(R) CPU E3-1225 v3 @ 3.20GHz
CPU family:                         6
Model:                              60
Thread(s) per core:                 1
Core(s) per socket:                 4
Socket(s):                          1
Stepping:                           3
CPU max MHz:                        3600,0000
CPU min MHz:                        800,0000
BogoMIPS:                           6397.95
Flags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm cpuid_fault invpcid_single pti ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid xsaveopt dtherm ida arat pln pts md_clear flush_l1d
Virtualization:                     VT-x
L1d cache:                          128 KiB (4 instances)
L1i cache:                          128 KiB (4 instances)
L2 cache:                           1 MiB (4 instances)
L3 cache:                           8 MiB (1 instance)
NUMA node(s):                       1
NUMA node0 CPU(s):                  0-3
Vulnerability Gather data sampling: Not affected
Vulnerability Itlb multihit:        KVM: Mitigation: VMX disabled
Vulnerability L1tf:                 Mitigation; PTE Inversion; VMX conditional cache flushes, SMT disabled
Vulnerability Mds:                  Mitigation; Clear CPU buffers; SMT disabled
Vulnerability Meltdown:             Mitigation; PTI
Vulnerability Mmio stale data:      Unknown: No mitigations
Vulnerability Retbleed:             Not affected
Vulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl
Vulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization
Vulnerability Spectre v2:           Mitigation; Retpolines, IBPB conditional, IBRS_FW, STIBP disabled, RSB filling, PBRSB-eIBRS Not affected
Vulnerability Srbds:                Mitigation; Microcode
Vulnerability Tsx async abort:      Not affected

Versions of relevant libraries:
[pip3] numpy==1.25.2
[pip3] open-clip-torch==2.20.0
[pip3] torch==2.0.1
[pip3] torchaudio==2.0.2
[pip3] torchvision==0.15.2
[conda] blas                      1.0                         mkl  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] mkl                       2023.1.0         h213fc3f_46343  
[conda] mkl-service               2.4.0           py310h5eee18b_1  
[conda] mkl_fft                   1.3.6           py310h1128e8f_1  
[conda] mkl_random                1.2.2           py310h1128e8f_1  
[conda] numpy                     1.25.2          py310h5f9d8c6_0  
[conda] numpy-base                1.25.2          py310hb5e798b_0  
[conda] open-clip-torch           2.20.0                   pypi_0    pypi
[conda] pytorch                   2.0.1           py3.10_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchaudio                2.0.2               py310_cu117    pytorch
[conda] torchtriton               2.0.0                     py310    pytorch
[conda] torchvision               0.15.2              py310_cu117    pytorch
        Pillow (9.4.0)

Loading trainer: CoOp
Loading dataset: Caltech101
Reading split from /home/brandnerkasper/Uni/MP/MP_CustomCoOp/data/caltech-101/split_zhou_Caltech101.json
Creating a 1-shot dataset
Creating a 1-shot dataset
Saving preprocessed few-shot data to /home/brandnerkasper/Uni/MP/MP_CustomCoOp/data/caltech-101/split_fewshot/shot_1-seed_1.pkl
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ----------
Dataset    Caltech101
# classes  100
# train_x  100
# val      100
# test     2,465
---------  ----------
Loading CLIP (backbone: RN50)
CLIP(
  (visual): ModifiedResNet(
    (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu1): ReLU(inplace=True)
    (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu2): ReLU(inplace=True)
    (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu3): ReLU(inplace=True)
    (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (layer1): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
        (downsample): Sequential(
          (-1): AvgPool2d(kernel_size=1, stride=1, padding=0)
          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
    )
    (layer2): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
        (downsample): Sequential(
          (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
    )
    (layer3): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
        (downsample): Sequential(
          (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
    )
    (layer4): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
        (downsample): Sequential(
          (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
    )
    (attnpool): AttentionPool2d(
      (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
      (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
      (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
      (c_proj): Linear(in_features=2048, out_features=1024, bias=True)
    )
  )
  (transformer): Transformer(
    (resblocks): Sequential(
      (0): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (6): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (7): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (8): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (9): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (10): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (11): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (token_embedding): Embedding(49408, 512)
  (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
Turning off gradients in both the image and the text encoder
Loading evaluator: Classification
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/Caltech/1/tensorboard)
epoch [1/200] batch [1/3] time 4.663 (4.663) data 0.292 (0.292) loss 3.3965 (3.3965) acc 40.6250 (40.6250) lr 1.0000e-05 eta 0:46:32
epoch [1/200] batch [2/3] time 0.847 (2.755) data 0.000 (0.146) loss 3.4980 (3.4473) acc 40.6250 (40.6250) lr 1.0000e-05 eta 0:27:27
epoch [1/200] batch [3/3] time 0.857 (2.122) data 0.000 (0.098) loss 3.3320 (3.4089) acc 37.5000 (39.5833) lr 2.0000e-03 eta 0:21:06
epoch [2/200] batch [1/3] time 1.111 (1.111) data 0.249 (0.249) loss 3.2949 (3.2949) acc 40.6250 (40.6250) lr 2.0000e-03 eta 0:11:01
epoch [2/200] batch [2/3] time 0.885 (0.998) data 0.000 (0.125) loss 2.1719 (2.7334) acc 43.7500 (42.1875) lr 2.0000e-03 eta 0:09:53
epoch [2/200] batch [3/3] time 0.855 (0.950) data 0.000 (0.083) loss 1.4873 (2.3180) acc 56.2500 (46.8750) lr 1.9999e-03 eta 0:09:24
epoch [3/200] batch [1/3] time 1.097 (1.097) data 0.237 (0.237) loss 1.6875 (1.6875) acc 59.3750 (59.3750) lr 1.9999e-03 eta 0:10:50
epoch [3/200] batch [2/3] time 0.852 (0.975) data 0.000 (0.119) loss 1.1240 (1.4058) acc 71.8750 (65.6250) lr 1.9999e-03 eta 0:09:36
epoch [3/200] batch [3/3] time 0.853 (0.934) data 0.000 (0.079) loss 1.3496 (1.3870) acc 62.5000 (64.5833) lr 1.9995e-03 eta 0:09:12
epoch [4/200] batch [1/3] time 1.096 (1.096) data 0.239 (0.239) loss 1.2979 (1.2979) acc 62.5000 (62.5000) lr 1.9995e-03 eta 0:10:46
epoch [4/200] batch [2/3] time 0.856 (0.976) data 0.000 (0.120) loss 1.3633 (1.3306) acc 53.1250 (57.8125) lr 1.9995e-03 eta 0:09:35
epoch [4/200] batch [3/3] time 0.856 (0.936) data 0.000 (0.080) loss 1.0654 (1.2422) acc 71.8750 (62.5000) lr 1.9989e-03 eta 0:09:10
epoch [5/200] batch [1/3] time 1.098 (1.098) data 0.239 (0.239) loss 1.4141 (1.4141) acc 56.2500 (56.2500) lr 1.9989e-03 eta 0:10:44
epoch [5/200] batch [2/3] time 0.860 (0.979) data 0.000 (0.119) loss 0.9712 (1.1926) acc 75.0000 (65.6250) lr 1.9989e-03 eta 0:09:33
epoch [5/200] batch [3/3] time 0.857 (0.939) data 0.000 (0.080) loss 1.5166 (1.3006) acc 65.6250 (65.6250) lr 1.9980e-03 eta 0:09:09
epoch [6/200] batch [1/3] time 1.112 (1.112) data 0.254 (0.254) loss 1.0117 (1.0117) acc 84.3750 (84.3750) lr 1.9980e-03 eta 0:10:49
epoch [6/200] batch [2/3] time 0.846 (0.979) data 0.000 (0.127) loss 1.0957 (1.0537) acc 68.7500 (76.5625) lr 1.9980e-03 eta 0:09:30
epoch [6/200] batch [3/3] time 0.866 (0.941) data 0.000 (0.085) loss 1.4258 (1.1777) acc 59.3750 (70.8333) lr 1.9969e-03 eta 0:09:07
epoch [7/200] batch [1/3] time 1.111 (1.111) data 0.259 (0.259) loss 1.3135 (1.3135) acc 59.3750 (59.3750) lr 1.9969e-03 eta 0:10:45
epoch [7/200] batch [2/3] time 0.863 (0.987) data 0.000 (0.129) loss 0.8481 (1.0808) acc 75.0000 (67.1875) lr 1.9969e-03 eta 0:09:32
epoch [7/200] batch [3/3] time 0.859 (0.944) data 0.000 (0.086) loss 0.6753 (0.9456) acc 78.1250 (70.8333) lr 1.9956e-03 eta 0:09:06
epoch [8/200] batch [1/3] time 1.118 (1.118) data 0.259 (0.259) loss 1.4287 (1.4287) acc 56.2500 (56.2500) lr 1.9956e-03 eta 0:10:46
epoch [8/200] batch [2/3] time 0.860 (0.989) data 0.000 (0.129) loss 1.0381 (1.2334) acc 75.0000 (65.6250) lr 1.9956e-03 eta 0:09:30
epoch [8/200] batch [3/3] time 0.861 (0.946) data 0.000 (0.086) loss 1.1338 (1.2002) acc 68.7500 (66.6667) lr 1.9940e-03 eta 0:09:05
epoch [9/200] batch [1/3] time 1.120 (1.120) data 0.258 (0.258) loss 0.8550 (0.8550) acc 75.0000 (75.0000) lr 1.9940e-03 eta 0:10:43
epoch [9/200] batch [2/3] time 0.855 (0.987) data 0.000 (0.129) loss 0.8618 (0.8584) acc 84.3750 (79.6875) lr 1.9940e-03 eta 0:09:26
epoch [9/200] batch [3/3] time 0.852 (0.942) data 0.000 (0.086) loss 1.3457 (1.0208) acc 65.6250 (75.0000) lr 1.9921e-03 eta 0:08:59
epoch [10/200] batch [1/3] time 1.107 (1.107) data 0.247 (0.247) loss 0.7949 (0.7949) acc 84.3750 (84.3750) lr 1.9921e-03 eta 0:10:33
epoch [10/200] batch [2/3] time 0.852 (0.979) data 0.000 (0.124) loss 1.4072 (1.1011) acc 65.6250 (75.0000) lr 1.9921e-03 eta 0:09:19
epoch [10/200] batch [3/3] time 0.855 (0.938) data 0.000 (0.083) loss 1.2197 (1.1406) acc 71.8750 (73.9583) lr 1.9900e-03 eta 0:08:54
epoch [11/200] batch [1/3] time 1.108 (1.108) data 0.254 (0.254) loss 1.2725 (1.2725) acc 65.6250 (65.6250) lr 1.9900e-03 eta 0:10:30
epoch [11/200] batch [2/3] time 0.860 (0.984) data 0.000 (0.127) loss 1.1689 (1.2207) acc 62.5000 (64.0625) lr 1.9900e-03 eta 0:09:18
epoch [11/200] batch [3/3] time 0.849 (0.939) data 0.000 (0.085) loss 1.0352 (1.1589) acc 75.0000 (67.7083) lr 1.9877e-03 eta 0:08:52
epoch [12/200] batch [1/3] time 1.106 (1.106) data 0.246 (0.246) loss 1.1230 (1.1230) acc 68.7500 (68.7500) lr 1.9877e-03 eta 0:10:25
epoch [12/200] batch [2/3] time 0.858 (0.982) data 0.000 (0.123) loss 0.7402 (0.9316) acc 81.2500 (75.0000) lr 1.9877e-03 eta 0:09:14
epoch [12/200] batch [3/3] time 0.852 (0.938) data 0.000 (0.082) loss 0.9438 (0.9357) acc 75.0000 (75.0000) lr 1.9851e-03 eta 0:08:49
epoch [13/200] batch [1/3] time 1.114 (1.114) data 0.256 (0.256) loss 0.5498 (0.5498) acc 81.2500 (81.2500) lr 1.9851e-03 eta 0:10:26
epoch [13/200] batch [2/3] time 0.859 (0.986) data 0.000 (0.128) loss 1.1992 (0.8745) acc 62.5000 (71.8750) lr 1.9851e-03 eta 0:09:14
epoch [13/200] batch [3/3] time 0.864 (0.946) data 0.000 (0.085) loss 0.9438 (0.8976) acc 68.7500 (70.8333) lr 1.9823e-03 eta 0:08:50
epoch [14/200] batch [1/3] time 1.108 (1.108) data 0.249 (0.249) loss 0.6997 (0.6997) acc 90.6250 (90.6250) lr 1.9823e-03 eta 0:10:20
epoch [14/200] batch [2/3] time 0.860 (0.984) data 0.000 (0.124) loss 0.9312 (0.8154) acc 78.1250 (84.3750) lr 1.9823e-03 eta 0:09:10
epoch [14/200] batch [3/3] time 0.863 (0.944) data 0.000 (0.083) loss 0.8560 (0.8289) acc 78.1250 (82.2917) lr 1.9792e-03 eta 0:08:46
epoch [15/200] batch [1/3] time 1.106 (1.106) data 0.245 (0.245) loss 0.9653 (0.9653) acc 71.8750 (71.8750) lr 1.9792e-03 eta 0:10:15
epoch [15/200] batch [2/3] time 0.861 (0.983) data 0.000 (0.123) loss 1.3936 (1.1794) acc 71.8750 (71.8750) lr 1.9792e-03 eta 0:09:06
epoch [15/200] batch [3/3] time 0.863 (0.943) data 0.000 (0.082) loss 0.3704 (0.9097) acc 87.5000 (77.0833) lr 1.9759e-03 eta 0:08:43
epoch [16/200] batch [1/3] time 1.108 (1.108) data 0.249 (0.249) loss 1.2256 (1.2256) acc 75.0000 (75.0000) lr 1.9759e-03 eta 0:10:13
epoch [16/200] batch [2/3] time 0.857 (0.983) data 0.000 (0.125) loss 0.7280 (0.9768) acc 78.1250 (76.5625) lr 1.9759e-03 eta 0:09:03
epoch [16/200] batch [3/3] time 0.861 (0.942) data 0.000 (0.083) loss 1.1455 (1.0330) acc 71.8750 (75.0000) lr 1.9724e-03 eta 0:08:39
epoch [17/200] batch [1/3] time 1.113 (1.113) data 0.252 (0.252) loss 0.8491 (0.8491) acc 78.1250 (78.1250) lr 1.9724e-03 eta 0:10:13
epoch [17/200] batch [2/3] time 0.856 (0.985) data 0.000 (0.126) loss 1.0557 (0.9524) acc 81.2500 (79.6875) lr 1.9724e-03 eta 0:09:01
epoch [17/200] batch [3/3] time 0.858 (0.943) data 0.000 (0.084) loss 0.7603 (0.8883) acc 78.1250 (79.1667) lr 1.9686e-03 eta 0:08:37
epoch [18/200] batch [1/3] time 1.117 (1.117) data 0.255 (0.255) loss 0.9805 (0.9805) acc 71.8750 (71.8750) lr 1.9686e-03 eta 0:10:12
epoch [18/200] batch [2/3] time 0.861 (0.989) data 0.000 (0.128) loss 0.5078 (0.7441) acc 90.6250 (81.2500) lr 1.9686e-03 eta 0:09:01
epoch [18/200] batch [3/3] time 0.857 (0.945) data 0.000 (0.085) loss 0.8638 (0.7840) acc 71.8750 (78.1250) lr 1.9646e-03 eta 0:08:36
epoch [19/200] batch [1/3] time 1.105 (1.105) data 0.241 (0.241) loss 0.7954 (0.7954) acc 78.1250 (78.1250) lr 1.9646e-03 eta 0:10:02
epoch [19/200] batch [2/3] time 0.856 (0.981) data 0.000 (0.121) loss 0.3157 (0.5555) acc 93.7500 (85.9375) lr 1.9646e-03 eta 0:08:53
epoch [19/200] batch [3/3] time 0.863 (0.941) data 0.000 (0.080) loss 0.6763 (0.5958) acc 81.2500 (84.3750) lr 1.9603e-03 eta 0:08:31
epoch [20/200] batch [1/3] time 1.103 (1.103) data 0.242 (0.242) loss 0.5825 (0.5825) acc 90.6250 (90.6250) lr 1.9603e-03 eta 0:09:57
epoch [20/200] batch [2/3] time 0.860 (0.981) data 0.000 (0.121) loss 1.2383 (0.9104) acc 65.6250 (78.1250) lr 1.9603e-03 eta 0:08:50
epoch [20/200] batch [3/3] time 0.857 (0.940) data 0.000 (0.081) loss 0.7852 (0.8687) acc 78.1250 (78.1250) lr 1.9558e-03 eta 0:08:27
epoch [21/200] batch [1/3] time 1.120 (1.120) data 0.257 (0.257) loss 0.5781 (0.5781) acc 81.2500 (81.2500) lr 1.9558e-03 eta 0:10:03
epoch [21/200] batch [2/3] time 0.861 (0.991) data 0.000 (0.129) loss 1.0674 (0.8228) acc 78.1250 (79.6875) lr 1.9558e-03 eta 0:08:52
epoch [21/200] batch [3/3] time 0.864 (0.948) data 0.000 (0.086) loss 1.1211 (0.9222) acc 78.1250 (79.1667) lr 1.9511e-03 eta 0:08:29
epoch [22/200] batch [1/3] time 1.107 (1.107) data 0.246 (0.246) loss 0.5947 (0.5947) acc 84.3750 (84.3750) lr 1.9511e-03 eta 0:09:53
epoch [22/200] batch [2/3] time 0.858 (0.982) data 0.000 (0.123) loss 0.4653 (0.5300) acc 87.5000 (85.9375) lr 1.9511e-03 eta 0:08:45
epoch [22/200] batch [3/3] time 0.860 (0.941) data 0.000 (0.082) loss 1.3887 (0.8162) acc 68.7500 (80.2083) lr 1.9461e-03 eta 0:08:22
epoch [23/200] batch [1/3] time 1.117 (1.117) data 0.256 (0.256) loss 0.6611 (0.6611) acc 81.2500 (81.2500) lr 1.9461e-03 eta 0:09:55
epoch [23/200] batch [2/3] time 0.856 (0.986) data 0.000 (0.128) loss 0.9028 (0.7820) acc 78.1250 (79.6875) lr 1.9461e-03 eta 0:08:44
epoch [23/200] batch [3/3] time 0.860 (0.944) data 0.000 (0.085) loss 0.6919 (0.7520) acc 84.3750 (81.2500) lr 1.9409e-03 eta 0:08:21
epoch [24/200] batch [1/3] time 1.110 (1.110) data 0.256 (0.256) loss 0.4592 (0.4592) acc 93.7500 (93.7500) lr 1.9409e-03 eta 0:09:48
epoch [24/200] batch [2/3] time 0.864 (0.987) data 0.000 (0.128) loss 0.6470 (0.5531) acc 87.5000 (90.6250) lr 1.9409e-03 eta 0:08:42
epoch [24/200] batch [3/3] time 0.863 (0.946) data 0.000 (0.085) loss 1.1084 (0.7382) acc 75.0000 (85.4167) lr 1.9354e-03 eta 0:08:19
epoch [25/200] batch [1/3] time 1.102 (1.102) data 0.239 (0.239) loss 1.0684 (1.0684) acc 75.0000 (75.0000) lr 1.9354e-03 eta 0:09:40
epoch [25/200] batch [2/3] time 0.861 (0.981) data 0.000 (0.120) loss 0.4746 (0.7715) acc 87.5000 (81.2500) lr 1.9354e-03 eta 0:08:36
epoch [25/200] batch [3/3] time 0.856 (0.939) data 0.000 (0.080) loss 0.6904 (0.7445) acc 87.5000 (83.3333) lr 1.9298e-03 eta 0:08:13
epoch [26/200] batch [1/3] time 1.098 (1.098) data 0.240 (0.240) loss 0.6431 (0.6431) acc 87.5000 (87.5000) lr 1.9298e-03 eta 0:09:35
epoch [26/200] batch [2/3] time 0.855 (0.976) data 0.000 (0.120) loss 0.7603 (0.7017) acc 81.2500 (84.3750) lr 1.9298e-03 eta 0:08:30
epoch [26/200] batch [3/3] time 0.861 (0.938) data 0.000 (0.080) loss 0.8081 (0.7371) acc 81.2500 (83.3333) lr 1.9239e-03 eta 0:08:09
epoch [27/200] batch [1/3] time 1.103 (1.103) data 0.242 (0.242) loss 0.8452 (0.8452) acc 78.1250 (78.1250) lr 1.9239e-03 eta 0:09:34
epoch [27/200] batch [2/3] time 0.861 (0.982) data 0.000 (0.121) loss 0.5610 (0.7031) acc 84.3750 (81.2500) lr 1.9239e-03 eta 0:08:30
epoch [27/200] batch [3/3] time 0.858 (0.941) data 0.000 (0.081) loss 0.4414 (0.6159) acc 87.5000 (83.3333) lr 1.9178e-03 eta 0:08:08
epoch [28/200] batch [1/3] time 1.118 (1.118) data 0.256 (0.256) loss 0.8843 (0.8843) acc 71.8750 (71.8750) lr 1.9178e-03 eta 0:09:38
epoch [28/200] batch [2/3] time 0.861 (0.989) data 0.000 (0.128) loss 0.4800 (0.6821) acc 90.6250 (81.2500) lr 1.9178e-03 eta 0:08:31
epoch [28/200] batch [3/3] time 0.861 (0.947) data 0.000 (0.086) loss 0.8560 (0.7401) acc 84.3750 (82.2917) lr 1.9114e-03 eta 0:08:08
epoch [29/200] batch [1/3] time 1.102 (1.102) data 0.238 (0.238) loss 0.5190 (0.5190) acc 90.6250 (90.6250) lr 1.9114e-03 eta 0:09:27
epoch [29/200] batch [2/3] time 0.855 (0.978) data 0.000 (0.119) loss 0.5181 (0.5186) acc 87.5000 (89.0625) lr 1.9114e-03 eta 0:08:22
epoch [29/200] batch [3/3] time 0.865 (0.941) data 0.000 (0.079) loss 0.9194 (0.6522) acc 81.2500 (86.4583) lr 1.9048e-03 eta 0:08:02
epoch [30/200] batch [1/3] time 1.111 (1.111) data 0.249 (0.249) loss 0.3738 (0.3738) acc 87.5000 (87.5000) lr 1.9048e-03 eta 0:09:28
epoch [30/200] batch [2/3] time 0.859 (0.985) data 0.000 (0.125) loss 0.5068 (0.4403) acc 90.6250 (89.0625) lr 1.9048e-03 eta 0:08:23
epoch [30/200] batch [3/3] time 0.861 (0.943) data 0.000 (0.083) loss 0.4602 (0.4469) acc 87.5000 (88.5417) lr 1.8980e-03 eta 0:08:01
epoch [31/200] batch [1/3] time 1.099 (1.099) data 0.238 (0.238) loss 0.7627 (0.7627) acc 87.5000 (87.5000) lr 1.8980e-03 eta 0:09:19
epoch [31/200] batch [2/3] time 0.858 (0.978) data 0.000 (0.119) loss 0.6343 (0.6985) acc 84.3750 (85.9375) lr 1.8980e-03 eta 0:08:17
epoch [31/200] batch [3/3] time 0.859 (0.939) data 0.000 (0.079) loss 0.4255 (0.6075) acc 90.6250 (87.5000) lr 1.8910e-03 eta 0:07:55
epoch [32/200] batch [1/3] time 1.121 (1.121) data 0.260 (0.260) loss 0.5435 (0.5435) acc 84.3750 (84.3750) lr 1.8910e-03 eta 0:09:27
epoch [32/200] batch [2/3] time 0.859 (0.990) data 0.000 (0.130) loss 0.8755 (0.7095) acc 81.2500 (82.8125) lr 1.8910e-03 eta 0:08:19
epoch [32/200] batch [3/3] time 0.854 (0.944) data 0.000 (0.087) loss 0.5938 (0.6709) acc 84.3750 (83.3333) lr 1.8838e-03 eta 0:07:56
epoch [33/200] batch [1/3] time 1.117 (1.117) data 0.255 (0.255) loss 0.4248 (0.4248) acc 84.3750 (84.3750) lr 1.8838e-03 eta 0:09:22
epoch [33/200] batch [2/3] time 0.853 (0.985) data 0.000 (0.128) loss 1.2959 (0.8604) acc 75.0000 (79.6875) lr 1.8838e-03 eta 0:08:14
epoch [33/200] batch [3/3] time 0.859 (0.943) data 0.000 (0.085) loss 1.0068 (0.9092) acc 81.2500 (80.2083) lr 1.8763e-03 eta 0:07:52
epoch [34/200] batch [1/3] time 1.093 (1.093) data 0.240 (0.240) loss 0.6929 (0.6929) acc 84.3750 (84.3750) lr 1.8763e-03 eta 0:09:06
epoch [34/200] batch [2/3] time 0.858 (0.976) data 0.000 (0.120) loss 0.4634 (0.5781) acc 84.3750 (84.3750) lr 1.8763e-03 eta 0:08:06
epoch [34/200] batch [3/3] time 0.855 (0.935) data 0.000 (0.080) loss 0.7476 (0.6346) acc 84.3750 (84.3750) lr 1.8686e-03 eta 0:07:45
epoch [35/200] batch [1/3] time 1.124 (1.124) data 0.264 (0.264) loss 0.6748 (0.6748) acc 81.2500 (81.2500) lr 1.8686e-03 eta 0:09:18
epoch [35/200] batch [2/3] time 0.864 (0.994) data 0.000 (0.132) loss 1.0186 (0.8467) acc 75.0000 (78.1250) lr 1.8686e-03 eta 0:08:12
epoch [35/200] batch [3/3] time 0.861 (0.949) data 0.000 (0.088) loss 0.7515 (0.8149) acc 84.3750 (80.2083) lr 1.8607e-03 eta 0:07:49
epoch [36/200] batch [1/3] time 1.103 (1.103) data 0.244 (0.244) loss 0.2101 (0.2101) acc 96.8750 (96.8750) lr 1.8607e-03 eta 0:09:04
epoch [36/200] batch [2/3] time 0.861 (0.982) data 0.000 (0.122) loss 0.5596 (0.3848) acc 81.2500 (89.0625) lr 1.8607e-03 eta 0:08:04
epoch [36/200] batch [3/3] time 0.866 (0.943) data 0.000 (0.081) loss 0.1536 (0.3077) acc 96.8750 (91.6667) lr 1.8526e-03 eta 0:07:43
epoch [37/200] batch [1/3] time 1.099 (1.099) data 0.236 (0.236) loss 0.2961 (0.2961) acc 93.7500 (93.7500) lr 1.8526e-03 eta 0:08:59
epoch [37/200] batch [2/3] time 0.854 (0.976) data 0.000 (0.118) loss 0.3579 (0.3270) acc 90.6250 (92.1875) lr 1.8526e-03 eta 0:07:58
epoch [37/200] batch [3/3] time 0.865 (0.939) data 0.000 (0.079) loss 0.5337 (0.3959) acc 87.5000 (90.6250) lr 1.8443e-03 eta 0:07:39
epoch [38/200] batch [1/3] time 1.097 (1.097) data 0.245 (0.245) loss 0.6133 (0.6133) acc 87.5000 (87.5000) lr 1.8443e-03 eta 0:08:55
epoch [38/200] batch [2/3] time 0.857 (0.977) data 0.000 (0.122) loss 0.6123 (0.6128) acc 87.5000 (87.5000) lr 1.8443e-03 eta 0:07:55
epoch [38/200] batch [3/3] time 0.857 (0.937) data 0.000 (0.082) loss 0.4622 (0.5626) acc 90.6250 (88.5417) lr 1.8358e-03 eta 0:07:35
epoch [39/200] batch [1/3] time 1.122 (1.122) data 0.259 (0.259) loss 0.5405 (0.5405) acc 90.6250 (90.6250) lr 1.8358e-03 eta 0:09:04
epoch [39/200] batch [2/3] time 0.866 (0.994) data 0.000 (0.130) loss 0.3740 (0.4573) acc 90.6250 (90.6250) lr 1.8358e-03 eta 0:08:01
epoch [39/200] batch [3/3] time 0.860 (0.949) data 0.000 (0.087) loss 0.8457 (0.5868) acc 87.5000 (89.5833) lr 1.8271e-03 eta 0:07:38
epoch [40/200] batch [1/3] time 1.121 (1.121) data 0.259 (0.259) loss 0.4150 (0.4150) acc 90.6250 (90.6250) lr 1.8271e-03 eta 0:09:00
epoch [40/200] batch [2/3] time 0.859 (0.990) data 0.000 (0.129) loss 0.4402 (0.4276) acc 87.5000 (89.0625) lr 1.8271e-03 eta 0:07:55
epoch [40/200] batch [3/3] time 0.853 (0.944) data 0.000 (0.086) loss 0.6221 (0.4924) acc 90.6250 (89.5833) lr 1.8181e-03 eta 0:07:33
epoch [41/200] batch [1/3] time 1.110 (1.110) data 0.248 (0.248) loss 0.4346 (0.4346) acc 87.5000 (87.5000) lr 1.8181e-03 eta 0:08:51
epoch [41/200] batch [2/3] time 0.850 (0.980) data 0.000 (0.124) loss 0.7241 (0.5793) acc 81.2500 (84.3750) lr 1.8181e-03 eta 0:07:48
epoch [41/200] batch [3/3] time 0.858 (0.939) data 0.000 (0.083) loss 0.4690 (0.5426) acc 90.6250 (86.4583) lr 1.8090e-03 eta 0:07:27
epoch [42/200] batch [1/3] time 1.090 (1.090) data 0.240 (0.240) loss 0.6797 (0.6797) acc 87.5000 (87.5000) lr 1.8090e-03 eta 0:08:38
epoch [42/200] batch [2/3] time 0.861 (0.976) data 0.000 (0.120) loss 0.6294 (0.6545) acc 87.5000 (87.5000) lr 1.8090e-03 eta 0:07:43
epoch [42/200] batch [3/3] time 0.862 (0.938) data 0.000 (0.080) loss 0.4834 (0.5975) acc 93.7500 (89.5833) lr 1.7997e-03 eta 0:07:24
epoch [43/200] batch [1/3] time 1.109 (1.109) data 0.250 (0.250) loss 0.5688 (0.5688) acc 81.2500 (81.2500) lr 1.7997e-03 eta 0:08:44
epoch [43/200] batch [2/3] time 0.856 (0.982) data 0.000 (0.125) loss 0.8823 (0.7256) acc 75.0000 (78.1250) lr 1.7997e-03 eta 0:07:43
epoch [43/200] batch [3/3] time 0.862 (0.942) data 0.000 (0.083) loss 0.5874 (0.6795) acc 87.5000 (81.2500) lr 1.7902e-03 eta 0:07:23
epoch [44/200] batch [1/3] time 1.112 (1.112) data 0.251 (0.251) loss 0.2671 (0.2671) acc 96.8750 (96.8750) lr 1.7902e-03 eta 0:08:42
epoch [44/200] batch [2/3] time 0.863 (0.987) data 0.000 (0.126) loss 0.1835 (0.2253) acc 100.0000 (98.4375) lr 1.7902e-03 eta 0:07:43
epoch [44/200] batch [3/3] time 0.849 (0.941) data 0.000 (0.084) loss 0.3787 (0.2764) acc 93.7500 (96.8750) lr 1.7804e-03 eta 0:07:20
epoch [45/200] batch [1/3] time 1.110 (1.110) data 0.247 (0.247) loss 0.7544 (0.7544) acc 87.5000 (87.5000) lr 1.7804e-03 eta 0:08:38
epoch [45/200] batch [2/3] time 0.860 (0.985) data 0.000 (0.123) loss 0.3608 (0.5576) acc 93.7500 (90.6250) lr 1.7804e-03 eta 0:07:38
epoch [45/200] batch [3/3] time 0.861 (0.943) data 0.000 (0.082) loss 0.4626 (0.5260) acc 96.8750 (92.7083) lr 1.7705e-03 eta 0:07:18
epoch [46/200] batch [1/3] time 1.103 (1.103) data 0.243 (0.243) loss 0.6289 (0.6289) acc 81.2500 (81.2500) lr 1.7705e-03 eta 0:08:31
epoch [46/200] batch [2/3] time 0.860 (0.981) data 0.000 (0.121) loss 0.2888 (0.4589) acc 93.7500 (87.5000) lr 1.7705e-03 eta 0:07:34
epoch [46/200] batch [3/3] time 0.857 (0.940) data 0.000 (0.081) loss 0.3777 (0.4318) acc 93.7500 (89.5833) lr 1.7604e-03 eta 0:07:14
epoch [47/200] batch [1/3] time 1.103 (1.103) data 0.242 (0.242) loss 0.5503 (0.5503) acc 90.6250 (90.6250) lr 1.7604e-03 eta 0:08:28
epoch [47/200] batch [2/3] time 0.861 (0.982) data 0.000 (0.121) loss 0.4043 (0.4773) acc 90.6250 (90.6250) lr 1.7604e-03 eta 0:07:31
epoch [47/200] batch [3/3] time 0.858 (0.941) data 0.000 (0.081) loss 0.2830 (0.4125) acc 93.7500 (91.6667) lr 1.7501e-03 eta 0:07:11
epoch [48/200] batch [1/3] time 1.114 (1.114) data 0.255 (0.255) loss 0.4856 (0.4856) acc 87.5000 (87.5000) lr 1.7501e-03 eta 0:08:30
epoch [48/200] batch [2/3] time 0.863 (0.988) data 0.000 (0.128) loss 0.7480 (0.6168) acc 81.2500 (84.3750) lr 1.7501e-03 eta 0:07:31
epoch [48/200] batch [3/3] time 0.862 (0.946) data 0.000 (0.085) loss 0.5396 (0.5911) acc 90.6250 (86.4583) lr 1.7396e-03 eta 0:07:11
epoch [49/200] batch [1/3] time 1.101 (1.101) data 0.240 (0.240) loss 0.1758 (0.1758) acc 100.0000 (100.0000) lr 1.7396e-03 eta 0:08:21
epoch [49/200] batch [2/3] time 0.856 (0.979) data 0.000 (0.120) loss 0.3621 (0.2689) acc 90.6250 (95.3125) lr 1.7396e-03 eta 0:07:24
epoch [49/200] batch [3/3] time 0.855 (0.937) data 0.000 (0.080) loss 0.5664 (0.3681) acc 87.5000 (92.7083) lr 1.7290e-03 eta 0:07:04
epoch [50/200] batch [1/3] time 1.111 (1.111) data 0.250 (0.250) loss 0.2805 (0.2805) acc 93.7500 (93.7500) lr 1.7290e-03 eta 0:08:22
epoch [50/200] batch [2/3] time 0.859 (0.985) data 0.000 (0.125) loss 0.3247 (0.3026) acc 93.7500 (93.7500) lr 1.7290e-03 eta 0:07:24
epoch [50/200] batch [3/3] time 0.863 (0.944) data 0.000 (0.083) loss 0.5322 (0.3792) acc 84.3750 (90.6250) lr 1.7181e-03 eta 0:07:04
epoch [51/200] batch [1/3] time 1.116 (1.116) data 0.255 (0.255) loss 0.2375 (0.2375) acc 93.7500 (93.7500) lr 1.7181e-03 eta 0:08:21
epoch [51/200] batch [2/3] time 0.862 (0.989) data 0.000 (0.128) loss 0.3447 (0.2911) acc 93.7500 (93.7500) lr 1.7181e-03 eta 0:07:23
epoch [51/200] batch [3/3] time 0.852 (0.943) data 0.000 (0.085) loss 0.5688 (0.3837) acc 90.6250 (92.7083) lr 1.7071e-03 eta 0:07:01
epoch [52/200] batch [1/3] time 1.108 (1.108) data 0.246 (0.246) loss 0.2063 (0.2063) acc 96.8750 (96.8750) lr 1.7071e-03 eta 0:08:14
epoch [52/200] batch [2/3] time 0.859 (0.983) data 0.000 (0.123) loss 0.4106 (0.3085) acc 84.3750 (90.6250) lr 1.7071e-03 eta 0:07:17
epoch [52/200] batch [3/3] time 0.858 (0.942) data 0.000 (0.082) loss 0.3071 (0.3080) acc 87.5000 (89.5833) lr 1.6959e-03 eta 0:06:58
epoch [53/200] batch [1/3] time 1.100 (1.100) data 0.241 (0.241) loss 0.2954 (0.2954) acc 93.7500 (93.7500) lr 1.6959e-03 eta 0:08:07
epoch [53/200] batch [2/3] time 0.862 (0.981) data 0.000 (0.121) loss 0.5225 (0.4089) acc 87.5000 (90.6250) lr 1.6959e-03 eta 0:07:13
epoch [53/200] batch [3/3] time 0.859 (0.941) data 0.000 (0.080) loss 0.4458 (0.4212) acc 87.5000 (89.5833) lr 1.6845e-03 eta 0:06:54
epoch [54/200] batch [1/3] time 1.120 (1.120) data 0.258 (0.258) loss 0.3447 (0.3447) acc 93.7500 (93.7500) lr 1.6845e-03 eta 0:08:12
epoch [54/200] batch [2/3] time 0.865 (0.993) data 0.000 (0.129) loss 0.5688 (0.4568) acc 84.3750 (89.0625) lr 1.6845e-03 eta 0:07:15
epoch [54/200] batch [3/3] time 0.861 (0.949) data 0.000 (0.086) loss 0.4497 (0.4544) acc 87.5000 (88.5417) lr 1.6730e-03 eta 0:06:55
epoch [55/200] batch [1/3] time 1.108 (1.108) data 0.248 (0.248) loss 0.2703 (0.2703) acc 93.7500 (93.7500) lr 1.6730e-03 eta 0:08:04
epoch [55/200] batch [2/3] time 0.861 (0.985) data 0.000 (0.124) loss 0.3413 (0.3058) acc 93.7500 (93.7500) lr 1.6730e-03 eta 0:07:09
epoch [55/200] batch [3/3] time 0.861 (0.943) data 0.000 (0.083) loss 0.7134 (0.4417) acc 81.2500 (89.5833) lr 1.6613e-03 eta 0:06:50
epoch [56/200] batch [1/3] time 1.100 (1.100) data 0.239 (0.239) loss 0.1013 (0.1013) acc 100.0000 (100.0000) lr 1.6613e-03 eta 0:07:57
epoch [56/200] batch [2/3] time 0.865 (0.983) data 0.000 (0.120) loss 0.4934 (0.2973) acc 84.3750 (92.1875) lr 1.6613e-03 eta 0:07:05
epoch [56/200] batch [3/3] time 0.854 (0.940) data 0.000 (0.080) loss 0.3474 (0.3140) acc 87.5000 (90.6250) lr 1.6494e-03 eta 0:06:45
epoch [57/200] batch [1/3] time 1.098 (1.098) data 0.239 (0.239) loss 0.5693 (0.5693) acc 87.5000 (87.5000) lr 1.6494e-03 eta 0:07:53
epoch [57/200] batch [2/3] time 0.853 (0.975) data 0.000 (0.120) loss 0.3706 (0.4700) acc 93.7500 (90.6250) lr 1.6494e-03 eta 0:06:59
epoch [57/200] batch [3/3] time 0.861 (0.937) data 0.000 (0.080) loss 0.4209 (0.4536) acc 90.6250 (90.6250) lr 1.6374e-03 eta 0:06:42
epoch [58/200] batch [1/3] time 1.118 (1.118) data 0.255 (0.255) loss 0.3000 (0.3000) acc 93.7500 (93.7500) lr 1.6374e-03 eta 0:07:58
epoch [58/200] batch [2/3] time 0.860 (0.989) data 0.000 (0.128) loss 0.3799 (0.3400) acc 90.6250 (92.1875) lr 1.6374e-03 eta 0:07:02
epoch [58/200] batch [3/3] time 0.865 (0.948) data 0.000 (0.085) loss 0.5332 (0.4044) acc 87.5000 (90.6250) lr 1.6252e-03 eta 0:06:43
epoch [59/200] batch [1/3] time 1.109 (1.109) data 0.248 (0.248) loss 0.2234 (0.2234) acc 90.6250 (90.6250) lr 1.6252e-03 eta 0:07:51
epoch [59/200] batch [2/3] time 0.861 (0.985) data 0.000 (0.124) loss 0.5996 (0.4115) acc 87.5000 (89.0625) lr 1.6252e-03 eta 0:06:57
epoch [59/200] batch [3/3] time 0.861 (0.944) data 0.000 (0.083) loss 0.7969 (0.5400) acc 75.0000 (84.3750) lr 1.6129e-03 eta 0:06:39
epoch [60/200] batch [1/3] time 1.104 (1.104) data 0.242 (0.242) loss 0.2747 (0.2747) acc 93.7500 (93.7500) lr 1.6129e-03 eta 0:07:45
epoch [60/200] batch [2/3] time 0.856 (0.980) data 0.000 (0.121) loss 0.6304 (0.4525) acc 84.3750 (89.0625) lr 1.6129e-03 eta 0:06:52
epoch [60/200] batch [3/3] time 0.859 (0.940) data 0.000 (0.081) loss 0.7031 (0.5361) acc 81.2500 (86.4583) lr 1.6004e-03 eta 0:06:34
epoch [61/200] batch [1/3] time 1.116 (1.116) data 0.257 (0.257) loss 0.4875 (0.4875) acc 90.6250 (90.6250) lr 1.6004e-03 eta 0:07:47
epoch [61/200] batch [2/3] time 0.863 (0.989) data 0.000 (0.129) loss 0.2183 (0.3529) acc 93.7500 (92.1875) lr 1.6004e-03 eta 0:06:53
epoch [61/200] batch [3/3] time 0.867 (0.948) data 0.000 (0.086) loss 0.4868 (0.3975) acc 90.6250 (91.6667) lr 1.5878e-03 eta 0:06:35
epoch [62/200] batch [1/3] time 1.118 (1.118) data 0.257 (0.257) loss 0.1742 (0.1742) acc 93.7500 (93.7500) lr 1.5878e-03 eta 0:07:45
epoch [62/200] batch [2/3] time 0.861 (0.990) data 0.000 (0.129) loss 0.0925 (0.1334) acc 100.0000 (96.8750) lr 1.5878e-03 eta 0:06:50
epoch [62/200] batch [3/3] time 0.859 (0.946) data 0.000 (0.086) loss 0.5347 (0.2671) acc 90.6250 (94.7917) lr 1.5750e-03 eta 0:06:31
epoch [63/200] batch [1/3] time 1.117 (1.117) data 0.257 (0.257) loss 0.4592 (0.4592) acc 90.6250 (90.6250) lr 1.5750e-03 eta 0:07:41
epoch [63/200] batch [2/3] time 0.857 (0.987) data 0.000 (0.129) loss 0.4241 (0.4417) acc 87.5000 (89.0625) lr 1.5750e-03 eta 0:06:46
epoch [63/200] batch [3/3] time 0.861 (0.945) data 0.000 (0.086) loss 0.5347 (0.4727) acc 87.5000 (88.5417) lr 1.5621e-03 eta 0:06:28
epoch [64/200] batch [1/3] time 1.119 (1.119) data 0.256 (0.256) loss 0.2852 (0.2852) acc 93.7500 (93.7500) lr 1.5621e-03 eta 0:07:38
epoch [64/200] batch [2/3] time 0.858 (0.988) data 0.000 (0.128) loss 0.2690 (0.2771) acc 93.7500 (93.7500) lr 1.5621e-03 eta 0:06:44
epoch [64/200] batch [3/3] time 0.859 (0.945) data 0.000 (0.086) loss 0.7661 (0.4401) acc 84.3750 (90.6250) lr 1.5490e-03 eta 0:06:25
epoch [65/200] batch [1/3] time 1.117 (1.117) data 0.255 (0.255) loss 0.5195 (0.5195) acc 84.3750 (84.3750) lr 1.5490e-03 eta 0:07:34
epoch [65/200] batch [2/3] time 0.854 (0.986) data 0.000 (0.127) loss 0.4802 (0.4999) acc 87.5000 (85.9375) lr 1.5490e-03 eta 0:06:40
epoch [65/200] batch [3/3] time 0.862 (0.945) data 0.000 (0.085) loss 0.7539 (0.5846) acc 75.0000 (82.2917) lr 1.5358e-03 eta 0:06:22
epoch [66/200] batch [1/3] time 1.110 (1.110) data 0.256 (0.256) loss 0.5430 (0.5430) acc 93.7500 (93.7500) lr 1.5358e-03 eta 0:07:28
epoch [66/200] batch [2/3] time 0.850 (0.980) data 0.000 (0.128) loss 0.5132 (0.5281) acc 84.3750 (89.0625) lr 1.5358e-03 eta 0:06:34
epoch [66/200] batch [3/3] time 0.862 (0.941) data 0.000 (0.085) loss 0.1984 (0.4182) acc 93.7500 (90.6250) lr 1.5225e-03 eta 0:06:18
epoch [67/200] batch [1/3] time 1.115 (1.115) data 0.255 (0.255) loss 0.4128 (0.4128) acc 90.6250 (90.6250) lr 1.5225e-03 eta 0:07:27
epoch [67/200] batch [2/3] time 0.863 (0.989) data 0.000 (0.128) loss 0.6025 (0.5077) acc 84.3750 (87.5000) lr 1.5225e-03 eta 0:06:35
epoch [67/200] batch [3/3] time 0.854 (0.944) data 0.000 (0.085) loss 0.9756 (0.6637) acc 68.7500 (81.2500) lr 1.5090e-03 eta 0:06:16
epoch [68/200] batch [1/3] time 1.110 (1.110) data 0.249 (0.249) loss 0.4714 (0.4714) acc 90.6250 (90.6250) lr 1.5090e-03 eta 0:07:21
epoch [68/200] batch [2/3] time 0.858 (0.984) data 0.000 (0.125) loss 0.0782 (0.2748) acc 100.0000 (95.3125) lr 1.5090e-03 eta 0:06:30
epoch [68/200] batch [3/3] time 0.862 (0.944) data 0.000 (0.083) loss 0.2273 (0.2590) acc 93.7500 (94.7917) lr 1.4955e-03 eta 0:06:13
epoch [69/200] batch [1/3] time 1.108 (1.108) data 0.246 (0.246) loss 0.1750 (0.1750) acc 96.8750 (96.8750) lr 1.4955e-03 eta 0:07:17
epoch [69/200] batch [2/3] time 0.852 (0.980) data 0.000 (0.123) loss 0.2474 (0.2112) acc 93.7500 (95.3125) lr 1.4955e-03 eta 0:06:26
epoch [69/200] batch [3/3] time 0.861 (0.940) data 0.000 (0.082) loss 0.5488 (0.3238) acc 90.6250 (93.7500) lr 1.4818e-03 eta 0:06:09
epoch [70/200] batch [1/3] time 1.095 (1.095) data 0.243 (0.243) loss 0.6372 (0.6372) acc 81.2500 (81.2500) lr 1.4818e-03 eta 0:07:09
epoch [70/200] batch [2/3] time 0.863 (0.979) data 0.000 (0.122) loss 0.3340 (0.4856) acc 90.6250 (85.9375) lr 1.4818e-03 eta 0:06:22
epoch [70/200] batch [3/3] time 0.862 (0.940) data 0.000 (0.081) loss 0.4509 (0.4740) acc 93.7500 (88.5417) lr 1.4679e-03 eta 0:06:06
epoch [71/200] batch [1/3] time 1.117 (1.117) data 0.255 (0.255) loss 0.2423 (0.2423) acc 90.6250 (90.6250) lr 1.4679e-03 eta 0:07:14
epoch [71/200] batch [2/3] time 0.858 (0.987) data 0.000 (0.128) loss 0.3960 (0.3192) acc 87.5000 (89.0625) lr 1.4679e-03 eta 0:06:23
epoch [71/200] batch [3/3] time 0.866 (0.947) data 0.000 (0.085) loss 0.4609 (0.3664) acc 87.5000 (88.5417) lr 1.4540e-03 eta 0:06:06
epoch [72/200] batch [1/3] time 1.120 (1.120) data 0.259 (0.259) loss 0.5317 (0.5317) acc 90.6250 (90.6250) lr 1.4540e-03 eta 0:07:12
epoch [72/200] batch [2/3] time 0.859 (0.989) data 0.000 (0.130) loss 0.1969 (0.3643) acc 96.8750 (93.7500) lr 1.4540e-03 eta 0:06:20
epoch [72/200] batch [3/3] time 0.853 (0.944) data 0.000 (0.087) loss 0.5527 (0.4271) acc 87.5000 (91.6667) lr 1.4399e-03 eta 0:06:02
epoch [73/200] batch [1/3] time 1.102 (1.102) data 0.239 (0.239) loss 0.3330 (0.3330) acc 90.6250 (90.6250) lr 1.4399e-03 eta 0:07:02
epoch [73/200] batch [2/3] time 0.855 (0.979) data 0.000 (0.120) loss 0.2930 (0.3130) acc 93.7500 (92.1875) lr 1.4399e-03 eta 0:06:13
epoch [73/200] batch [3/3] time 0.861 (0.939) data 0.000 (0.080) loss 0.2378 (0.2879) acc 93.7500 (92.7083) lr 1.4258e-03 eta 0:05:57
epoch [74/200] batch [1/3] time 1.103 (1.103) data 0.248 (0.248) loss 0.3694 (0.3694) acc 90.6250 (90.6250) lr 1.4258e-03 eta 0:06:59
epoch [74/200] batch [2/3] time 0.858 (0.981) data 0.000 (0.124) loss 0.1738 (0.2716) acc 100.0000 (95.3125) lr 1.4258e-03 eta 0:06:11
epoch [74/200] batch [3/3] time 0.854 (0.938) data 0.000 (0.083) loss 0.4377 (0.3270) acc 84.3750 (91.6667) lr 1.4115e-03 eta 0:05:54
epoch [75/200] batch [1/3] time 1.109 (1.109) data 0.248 (0.248) loss 0.7031 (0.7031) acc 87.5000 (87.5000) lr 1.4115e-03 eta 0:06:58
epoch [75/200] batch [2/3] time 0.862 (0.985) data 0.000 (0.124) loss 0.4595 (0.5813) acc 90.6250 (89.0625) lr 1.4115e-03 eta 0:06:10
epoch [75/200] batch [3/3] time 0.858 (0.943) data 0.000 (0.083) loss 0.3755 (0.5127) acc 87.5000 (88.5417) lr 1.3971e-03 eta 0:05:53
epoch [76/200] batch [1/3] time 1.114 (1.114) data 0.249 (0.249) loss 0.1991 (0.1991) acc 96.8750 (96.8750) lr 1.3971e-03 eta 0:06:56
epoch [76/200] batch [2/3] time 0.859 (0.987) data 0.000 (0.125) loss 0.3796 (0.2894) acc 90.6250 (93.7500) lr 1.3971e-03 eta 0:06:08
epoch [76/200] batch [3/3] time 0.862 (0.945) data 0.000 (0.083) loss 0.5508 (0.3765) acc 81.2500 (89.5833) lr 1.3827e-03 eta 0:05:51
epoch [77/200] batch [1/3] time 1.110 (1.110) data 0.247 (0.247) loss 0.4871 (0.4871) acc 90.6250 (90.6250) lr 1.3827e-03 eta 0:06:51
epoch [77/200] batch [2/3] time 0.856 (0.983) data 0.000 (0.124) loss 0.3772 (0.4321) acc 93.7500 (92.1875) lr 1.3827e-03 eta 0:06:03
epoch [77/200] batch [3/3] time 0.861 (0.943) data 0.000 (0.082) loss 0.5796 (0.4813) acc 87.5000 (90.6250) lr 1.3681e-03 eta 0:05:47
epoch [78/200] batch [1/3] time 1.103 (1.103) data 0.249 (0.249) loss 0.4146 (0.4146) acc 93.7500 (93.7500) lr 1.3681e-03 eta 0:06:45
epoch [78/200] batch [2/3] time 0.863 (0.983) data 0.000 (0.125) loss 0.4944 (0.4545) acc 90.6250 (92.1875) lr 1.3681e-03 eta 0:06:00
epoch [78/200] batch [3/3] time 0.857 (0.941) data 0.000 (0.083) loss 0.6187 (0.5092) acc 81.2500 (88.5417) lr 1.3535e-03 eta 0:05:44
epoch [79/200] batch [1/3] time 1.119 (1.119) data 0.258 (0.258) loss 0.3711 (0.3711) acc 93.7500 (93.7500) lr 1.3535e-03 eta 0:06:48
epoch [79/200] batch [2/3] time 0.860 (0.989) data 0.000 (0.129) loss 0.4380 (0.4045) acc 90.6250 (92.1875) lr 1.3535e-03 eta 0:06:00
epoch [79/200] batch [3/3] time 0.858 (0.946) data 0.000 (0.086) loss 0.4080 (0.4057) acc 93.7500 (92.7083) lr 1.3387e-03 eta 0:05:43
epoch [80/200] batch [1/3] time 1.119 (1.119) data 0.257 (0.257) loss 0.2668 (0.2668) acc 96.8750 (96.8750) lr 1.3387e-03 eta 0:06:44
epoch [80/200] batch [2/3] time 0.864 (0.991) data 0.000 (0.129) loss 0.3647 (0.3158) acc 90.6250 (93.7500) lr 1.3387e-03 eta 0:05:57
epoch [80/200] batch [3/3] time 0.855 (0.946) data 0.000 (0.086) loss 0.2433 (0.2916) acc 93.7500 (93.7500) lr 1.3239e-03 eta 0:05:40
epoch [81/200] batch [1/3] time 1.118 (1.118) data 0.255 (0.255) loss 0.5518 (0.5518) acc 87.5000 (87.5000) lr 1.3239e-03 eta 0:06:41
epoch [81/200] batch [2/3] time 0.853 (0.986) data 0.000 (0.128) loss 0.3525 (0.4521) acc 93.7500 (90.6250) lr 1.3239e-03 eta 0:05:52
epoch [81/200] batch [3/3] time 0.863 (0.945) data 0.000 (0.085) loss 0.5166 (0.4736) acc 87.5000 (89.5833) lr 1.3090e-03 eta 0:05:37
epoch [82/200] batch [1/3] time 1.092 (1.092) data 0.239 (0.239) loss 0.3533 (0.3533) acc 87.5000 (87.5000) lr 1.3090e-03 eta 0:06:28
epoch [82/200] batch [2/3] time 0.863 (0.978) data 0.000 (0.119) loss 0.3757 (0.3645) acc 90.6250 (89.0625) lr 1.3090e-03 eta 0:05:47
epoch [82/200] batch [3/3] time 0.863 (0.940) data 0.000 (0.080) loss 0.1498 (0.2929) acc 96.8750 (91.6667) lr 1.2940e-03 eta 0:05:32
epoch [83/200] batch [1/3] time 1.111 (1.111) data 0.250 (0.250) loss 0.6421 (0.6421) acc 81.2500 (81.2500) lr 1.2940e-03 eta 0:06:32
epoch [83/200] batch [2/3] time 0.860 (0.985) data 0.000 (0.125) loss 0.2737 (0.4579) acc 90.6250 (85.9375) lr 1.2940e-03 eta 0:05:46
epoch [83/200] batch [3/3] time 0.859 (0.943) data 0.000 (0.083) loss 0.2778 (0.3979) acc 93.7500 (88.5417) lr 1.2790e-03 eta 0:05:31
epoch [84/200] batch [1/3] time 1.101 (1.101) data 0.239 (0.239) loss 0.3477 (0.3477) acc 93.7500 (93.7500) lr 1.2790e-03 eta 0:06:25
epoch [84/200] batch [2/3] time 0.859 (0.980) data 0.000 (0.119) loss 0.4185 (0.3831) acc 87.5000 (90.6250) lr 1.2790e-03 eta 0:05:42
epoch [84/200] batch [3/3] time 0.855 (0.938) data 0.000 (0.080) loss 0.6714 (0.4792) acc 84.3750 (88.5417) lr 1.2639e-03 eta 0:05:26
epoch [85/200] batch [1/3] time 1.111 (1.111) data 0.249 (0.249) loss 0.2639 (0.2639) acc 93.7500 (93.7500) lr 1.2639e-03 eta 0:06:25
epoch [85/200] batch [2/3] time 0.852 (0.982) data 0.000 (0.124) loss 0.5522 (0.4081) acc 87.5000 (90.6250) lr 1.2639e-03 eta 0:05:39
epoch [85/200] batch [3/3] time 0.860 (0.941) data 0.000 (0.083) loss 0.9360 (0.5841) acc 78.1250 (86.4583) lr 1.2487e-03 eta 0:05:24
epoch [86/200] batch [1/3] time 1.100 (1.100) data 0.247 (0.247) loss 0.4993 (0.4993) acc 84.3750 (84.3750) lr 1.2487e-03 eta 0:06:18
epoch [86/200] batch [2/3] time 0.860 (0.980) data 0.000 (0.123) loss 0.5034 (0.5013) acc 90.6250 (87.5000) lr 1.2487e-03 eta 0:05:36
epoch [86/200] batch [3/3] time 0.860 (0.940) data 0.000 (0.082) loss 0.2040 (0.4022) acc 90.6250 (88.5417) lr 1.2334e-03 eta 0:05:21
epoch [87/200] batch [1/3] time 1.103 (1.103) data 0.241 (0.241) loss 0.4114 (0.4114) acc 90.6250 (90.6250) lr 1.2334e-03 eta 0:06:15
epoch [87/200] batch [2/3] time 0.864 (0.983) data 0.000 (0.120) loss 0.2294 (0.3204) acc 93.7500 (92.1875) lr 1.2334e-03 eta 0:05:34
epoch [87/200] batch [3/3] time 0.864 (0.943) data 0.000 (0.080) loss 0.2039 (0.2815) acc 96.8750 (93.7500) lr 1.2181e-03 eta 0:05:19
epoch [88/200] batch [1/3] time 1.118 (1.118) data 0.258 (0.258) loss 0.6304 (0.6304) acc 84.3750 (84.3750) lr 1.2181e-03 eta 0:06:17
epoch [88/200] batch [2/3] time 0.862 (0.990) data 0.000 (0.129) loss 0.2739 (0.4521) acc 93.7500 (89.0625) lr 1.2181e-03 eta 0:05:33
epoch [88/200] batch [3/3] time 0.854 (0.945) data 0.000 (0.086) loss 0.3752 (0.4265) acc 90.6250 (89.5833) lr 1.2028e-03 eta 0:05:17
epoch [89/200] batch [1/3] time 1.121 (1.121) data 0.259 (0.259) loss 0.3281 (0.3281) acc 93.7500 (93.7500) lr 1.2028e-03 eta 0:06:15
epoch [89/200] batch [2/3] time 0.855 (0.988) data 0.000 (0.130) loss 0.1912 (0.2596) acc 96.8750 (95.3125) lr 1.2028e-03 eta 0:05:30
epoch [89/200] batch [3/3] time 0.860 (0.946) data 0.000 (0.086) loss 0.2585 (0.2593) acc 93.7500 (94.7917) lr 1.1874e-03 eta 0:05:14
epoch [90/200] batch [1/3] time 1.112 (1.112) data 0.255 (0.255) loss 0.6167 (0.6167) acc 87.5000 (87.5000) lr 1.1874e-03 eta 0:06:09
epoch [90/200] batch [2/3] time 0.863 (0.987) data 0.000 (0.128) loss 0.3318 (0.4742) acc 93.7500 (90.6250) lr 1.1874e-03 eta 0:05:26
epoch [90/200] batch [3/3] time 0.862 (0.946) data 0.000 (0.085) loss 0.4395 (0.4626) acc 87.5000 (89.5833) lr 1.1719e-03 eta 0:05:12
epoch [91/200] batch [1/3] time 1.104 (1.104) data 0.240 (0.240) loss 0.3433 (0.3433) acc 93.7500 (93.7500) lr 1.1719e-03 eta 0:06:03
epoch [91/200] batch [2/3] time 0.861 (0.982) data 0.000 (0.120) loss 0.7563 (0.5498) acc 87.5000 (90.6250) lr 1.1719e-03 eta 0:05:22
epoch [91/200] batch [3/3] time 0.863 (0.942) data 0.000 (0.080) loss 0.4844 (0.5280) acc 90.6250 (90.6250) lr 1.1564e-03 eta 0:05:08
epoch [92/200] batch [1/3] time 1.108 (1.108) data 0.246 (0.246) loss 0.3308 (0.3308) acc 93.7500 (93.7500) lr 1.1564e-03 eta 0:06:01
epoch [92/200] batch [2/3] time 0.858 (0.983) data 0.000 (0.123) loss 0.5127 (0.4218) acc 90.6250 (92.1875) lr 1.1564e-03 eta 0:05:19
epoch [92/200] batch [3/3] time 0.847 (0.938) data 0.000 (0.082) loss 0.5537 (0.4657) acc 81.2500 (88.5417) lr 1.1409e-03 eta 0:05:03
epoch [93/200] batch [1/3] time 1.113 (1.113) data 0.249 (0.249) loss 0.3054 (0.3054) acc 93.7500 (93.7500) lr 1.1409e-03 eta 0:05:59
epoch [93/200] batch [2/3] time 0.853 (0.983) data 0.000 (0.124) loss 0.2239 (0.2646) acc 96.8750 (95.3125) lr 1.1409e-03 eta 0:05:16
epoch [93/200] batch [3/3] time 0.861 (0.942) data 0.000 (0.083) loss 0.4146 (0.3146) acc 87.5000 (92.7083) lr 1.1253e-03 eta 0:05:02
epoch [94/200] batch [1/3] time 1.108 (1.108) data 0.255 (0.255) loss 0.2812 (0.2812) acc 96.8750 (96.8750) lr 1.1253e-03 eta 0:05:54
epoch [94/200] batch [2/3] time 0.860 (0.984) data 0.000 (0.128) loss 0.4434 (0.3623) acc 87.5000 (92.1875) lr 1.1253e-03 eta 0:05:13
epoch [94/200] batch [3/3] time 0.870 (0.946) data 0.000 (0.085) loss 0.3169 (0.3472) acc 90.6250 (91.6667) lr 1.1097e-03 eta 0:05:00
epoch [95/200] batch [1/3] time 1.120 (1.120) data 0.258 (0.258) loss 0.2180 (0.2180) acc 96.8750 (96.8750) lr 1.1097e-03 eta 0:05:55
epoch [95/200] batch [2/3] time 0.858 (0.989) data 0.000 (0.129) loss 0.2063 (0.2122) acc 93.7500 (95.3125) lr 1.1097e-03 eta 0:05:12
epoch [95/200] batch [3/3] time 0.864 (0.947) data 0.000 (0.086) loss 0.2935 (0.2393) acc 87.5000 (92.7083) lr 1.0941e-03 eta 0:04:58
epoch [96/200] batch [1/3] time 1.112 (1.112) data 0.251 (0.251) loss 0.2732 (0.2732) acc 93.7500 (93.7500) lr 1.0941e-03 eta 0:05:49
epoch [96/200] batch [2/3] time 0.856 (0.984) data 0.000 (0.125) loss 0.5137 (0.3934) acc 81.2500 (87.5000) lr 1.0941e-03 eta 0:05:07
epoch [96/200] batch [3/3] time 0.856 (0.941) data 0.000 (0.084) loss 0.1725 (0.3198) acc 100.0000 (91.6667) lr 1.0785e-03 eta 0:04:53
epoch [97/200] batch [1/3] time 1.118 (1.118) data 0.257 (0.257) loss 0.3833 (0.3833) acc 87.5000 (87.5000) lr 1.0785e-03 eta 0:05:47
epoch [97/200] batch [2/3] time 0.854 (0.986) data 0.000 (0.129) loss 0.3181 (0.3507) acc 90.6250 (89.0625) lr 1.0785e-03 eta 0:05:05
epoch [97/200] batch [3/3] time 0.861 (0.944) data 0.000 (0.086) loss 0.5122 (0.4045) acc 84.3750 (87.5000) lr 1.0628e-03 eta 0:04:51
epoch [98/200] batch [1/3] time 1.113 (1.113) data 0.255 (0.255) loss 0.2598 (0.2598) acc 93.7500 (93.7500) lr 1.0628e-03 eta 0:05:42
epoch [98/200] batch [2/3] time 0.863 (0.988) data 0.000 (0.128) loss 0.2886 (0.2742) acc 93.7500 (93.7500) lr 1.0628e-03 eta 0:05:03
epoch [98/200] batch [3/3] time 0.860 (0.945) data 0.000 (0.085) loss 0.2891 (0.2791) acc 93.7500 (93.7500) lr 1.0471e-03 eta 0:04:49
epoch [99/200] batch [1/3] time 1.117 (1.117) data 0.256 (0.256) loss 0.0731 (0.0731) acc 100.0000 (100.0000) lr 1.0471e-03 eta 0:05:40
epoch [99/200] batch [2/3] time 0.855 (0.986) data 0.000 (0.128) loss 0.5146 (0.2939) acc 87.5000 (93.7500) lr 1.0471e-03 eta 0:04:59
epoch [99/200] batch [3/3] time 0.862 (0.944) data 0.000 (0.085) loss 0.3662 (0.3180) acc 90.6250 (92.7083) lr 1.0314e-03 eta 0:04:46
epoch [100/200] batch [1/3] time 1.105 (1.105) data 0.243 (0.243) loss 0.4075 (0.4075) acc 90.6250 (90.6250) lr 1.0314e-03 eta 0:05:33
epoch [100/200] batch [2/3] time 0.863 (0.984) data 0.000 (0.121) loss 0.3792 (0.3933) acc 87.5000 (89.0625) lr 1.0314e-03 eta 0:04:56
epoch [100/200] batch [3/3] time 0.854 (0.941) data 0.000 (0.081) loss 0.1820 (0.3229) acc 100.0000 (92.7083) lr 1.0157e-03 eta 0:04:42
epoch [101/200] batch [1/3] time 1.105 (1.105) data 0.241 (0.241) loss 0.3306 (0.3306) acc 90.6250 (90.6250) lr 1.0157e-03 eta 0:05:30
epoch [101/200] batch [2/3] time 0.854 (0.979) data 0.000 (0.121) loss 0.1794 (0.2550) acc 96.8750 (93.7500) lr 1.0157e-03 eta 0:04:51
epoch [101/200] batch [3/3] time 0.861 (0.940) data 0.000 (0.080) loss 0.4749 (0.3283) acc 87.5000 (91.6667) lr 1.0000e-03 eta 0:04:39
epoch [102/200] batch [1/3] time 1.104 (1.104) data 0.249 (0.249) loss 0.1904 (0.1904) acc 93.7500 (93.7500) lr 1.0000e-03 eta 0:05:26
epoch [102/200] batch [2/3] time 0.866 (0.985) data 0.000 (0.125) loss 0.1652 (0.1778) acc 96.8750 (95.3125) lr 1.0000e-03 eta 0:04:50
epoch [102/200] batch [3/3] time 0.858 (0.943) data 0.000 (0.083) loss 0.2595 (0.2050) acc 93.7500 (94.7917) lr 9.8429e-04 eta 0:04:37
epoch [103/200] batch [1/3] time 1.119 (1.119) data 0.256 (0.256) loss 0.1974 (0.1974) acc 96.8750 (96.8750) lr 9.8429e-04 eta 0:05:27
epoch [103/200] batch [2/3] time 0.861 (0.990) data 0.000 (0.128) loss 0.1289 (0.1631) acc 100.0000 (98.4375) lr 9.8429e-04 eta 0:04:49
epoch [103/200] batch [3/3] time 0.861 (0.947) data 0.000 (0.086) loss 0.2966 (0.2076) acc 96.8750 (97.9167) lr 9.6859e-04 eta 0:04:35
epoch [104/200] batch [1/3] time 1.116 (1.116) data 0.257 (0.257) loss 0.4302 (0.4302) acc 87.5000 (87.5000) lr 9.6859e-04 eta 0:05:23
epoch [104/200] batch [2/3] time 0.862 (0.989) data 0.000 (0.128) loss 0.2622 (0.3462) acc 93.7500 (90.6250) lr 9.6859e-04 eta 0:04:45
epoch [104/200] batch [3/3] time 0.857 (0.945) data 0.000 (0.086) loss 0.4785 (0.3903) acc 87.5000 (89.5833) lr 9.5289e-04 eta 0:04:32
epoch [105/200] batch [1/3] time 1.106 (1.106) data 0.244 (0.244) loss 0.2355 (0.2355) acc 96.8750 (96.8750) lr 9.5289e-04 eta 0:05:17
epoch [105/200] batch [2/3] time 0.848 (0.977) data 0.000 (0.122) loss 0.3875 (0.3115) acc 90.6250 (93.7500) lr 9.5289e-04 eta 0:04:39
epoch [105/200] batch [3/3] time 0.861 (0.938) data 0.000 (0.081) loss 0.2861 (0.3030) acc 90.6250 (92.7083) lr 9.3721e-04 eta 0:04:27
epoch [106/200] batch [1/3] time 1.110 (1.110) data 0.256 (0.256) loss 0.5063 (0.5063) acc 90.6250 (90.6250) lr 9.3721e-04 eta 0:05:15
epoch [106/200] batch [2/3] time 0.863 (0.986) data 0.000 (0.128) loss 0.4551 (0.4807) acc 87.5000 (89.0625) lr 9.3721e-04 eta 0:04:39
epoch [106/200] batch [3/3] time 0.858 (0.944) data 0.000 (0.086) loss 0.2433 (0.4016) acc 93.7500 (90.6250) lr 9.2154e-04 eta 0:04:26
epoch [107/200] batch [1/3] time 1.109 (1.109) data 0.249 (0.249) loss 0.2405 (0.2405) acc 93.7500 (93.7500) lr 9.2154e-04 eta 0:05:11
epoch [107/200] batch [2/3] time 0.859 (0.984) data 0.000 (0.125) loss 0.4448 (0.3427) acc 93.7500 (93.7500) lr 9.2154e-04 eta 0:04:35
epoch [107/200] batch [3/3] time 0.859 (0.942) data 0.000 (0.083) loss 0.2993 (0.3282) acc 93.7500 (93.7500) lr 9.0589e-04 eta 0:04:22
epoch [108/200] batch [1/3] time 1.104 (1.104) data 0.243 (0.243) loss 0.4026 (0.4026) acc 87.5000 (87.5000) lr 9.0589e-04 eta 0:05:06
epoch [108/200] batch [2/3] time 0.862 (0.983) data 0.000 (0.122) loss 0.2515 (0.3270) acc 90.6250 (89.0625) lr 9.0589e-04 eta 0:04:32
epoch [108/200] batch [3/3] time 0.857 (0.941) data 0.000 (0.081) loss 0.1969 (0.2837) acc 93.7500 (90.6250) lr 8.9027e-04 eta 0:04:19
epoch [109/200] batch [1/3] time 1.117 (1.117) data 0.255 (0.255) loss 0.3713 (0.3713) acc 87.5000 (87.5000) lr 8.9027e-04 eta 0:05:07
epoch [109/200] batch [2/3] time 0.857 (0.987) data 0.000 (0.128) loss 0.3564 (0.3639) acc 87.5000 (87.5000) lr 8.9027e-04 eta 0:04:30
epoch [109/200] batch [3/3] time 0.863 (0.946) data 0.000 (0.085) loss 0.1345 (0.2874) acc 96.8750 (90.6250) lr 8.7467e-04 eta 0:04:18
epoch [110/200] batch [1/3] time 1.092 (1.092) data 0.239 (0.239) loss 0.2974 (0.2974) acc 93.7500 (93.7500) lr 8.7467e-04 eta 0:04:56
epoch [110/200] batch [2/3] time 0.857 (0.974) data 0.000 (0.119) loss 0.1202 (0.2088) acc 100.0000 (96.8750) lr 8.7467e-04 eta 0:04:24
epoch [110/200] batch [3/3] time 0.860 (0.936) data 0.000 (0.080) loss 0.3545 (0.2574) acc 90.6250 (94.7917) lr 8.5910e-04 eta 0:04:12
epoch [111/200] batch [1/3] time 1.115 (1.115) data 0.254 (0.254) loss 0.2203 (0.2203) acc 96.8750 (96.8750) lr 8.5910e-04 eta 0:04:59
epoch [111/200] batch [2/3] time 0.859 (0.987) data 0.000 (0.127) loss 0.2993 (0.2598) acc 96.8750 (96.8750) lr 8.5910e-04 eta 0:04:24
epoch [111/200] batch [3/3] time 0.855 (0.943) data 0.000 (0.085) loss 0.1337 (0.2178) acc 100.0000 (97.9167) lr 8.4357e-04 eta 0:04:11
epoch [112/200] batch [1/3] time 1.117 (1.117) data 0.257 (0.257) loss 0.4050 (0.4050) acc 87.5000 (87.5000) lr 8.4357e-04 eta 0:04:57
epoch [112/200] batch [2/3] time 0.858 (0.988) data 0.000 (0.129) loss 0.3806 (0.3928) acc 93.7500 (90.6250) lr 8.4357e-04 eta 0:04:21
epoch [112/200] batch [3/3] time 0.859 (0.945) data 0.000 (0.086) loss 0.1251 (0.3036) acc 96.8750 (92.7083) lr 8.2807e-04 eta 0:04:09
epoch [113/200] batch [1/3] time 1.109 (1.109) data 0.246 (0.246) loss 0.3789 (0.3789) acc 90.6250 (90.6250) lr 8.2807e-04 eta 0:04:51
epoch [113/200] batch [2/3] time 0.856 (0.982) data 0.000 (0.123) loss 0.3447 (0.3618) acc 93.7500 (92.1875) lr 8.2807e-04 eta 0:04:17
epoch [113/200] batch [3/3] time 0.860 (0.942) data 0.000 (0.082) loss 0.2568 (0.3268) acc 96.8750 (93.7500) lr 8.1262e-04 eta 0:04:05
epoch [114/200] batch [1/3] time 1.116 (1.116) data 0.257 (0.257) loss 0.4302 (0.4302) acc 87.5000 (87.5000) lr 8.1262e-04 eta 0:04:50
epoch [114/200] batch [2/3] time 0.861 (0.988) data 0.000 (0.128) loss 0.3179 (0.3740) acc 96.8750 (92.1875) lr 8.1262e-04 eta 0:04:16
epoch [114/200] batch [3/3] time 0.863 (0.947) data 0.000 (0.086) loss 0.3091 (0.3524) acc 93.7500 (92.7083) lr 7.9721e-04 eta 0:04:04
epoch [115/200] batch [1/3] time 1.118 (1.118) data 0.257 (0.257) loss 0.0700 (0.0700) acc 96.8750 (96.8750) lr 7.9721e-04 eta 0:04:47
epoch [115/200] batch [2/3] time 0.858 (0.988) data 0.000 (0.129) loss 0.4006 (0.2353) acc 84.3750 (90.6250) lr 7.9721e-04 eta 0:04:12
epoch [115/200] batch [3/3] time 0.856 (0.944) data 0.000 (0.086) loss 0.3340 (0.2682) acc 93.7500 (91.6667) lr 7.8186e-04 eta 0:04:00
epoch [116/200] batch [1/3] time 1.102 (1.102) data 0.240 (0.240) loss 0.2285 (0.2285) acc 96.8750 (96.8750) lr 7.8186e-04 eta 0:04:39
epoch [116/200] batch [2/3] time 0.862 (0.982) data 0.000 (0.120) loss 0.3696 (0.2991) acc 90.6250 (93.7500) lr 7.8186e-04 eta 0:04:08
epoch [116/200] batch [3/3] time 0.849 (0.938) data 0.000 (0.080) loss 0.3743 (0.3241) acc 90.6250 (92.7083) lr 7.6655e-04 eta 0:03:56
epoch [117/200] batch [1/3] time 1.098 (1.098) data 0.240 (0.240) loss 0.2742 (0.2742) acc 93.7500 (93.7500) lr 7.6655e-04 eta 0:04:35
epoch [117/200] batch [2/3] time 0.855 (0.977) data 0.000 (0.120) loss 0.3267 (0.3004) acc 90.6250 (92.1875) lr 7.6655e-04 eta 0:04:04
epoch [117/200] batch [3/3] time 0.862 (0.939) data 0.000 (0.080) loss 0.1387 (0.2465) acc 100.0000 (94.7917) lr 7.5131e-04 eta 0:03:53
epoch [118/200] batch [1/3] time 1.101 (1.101) data 0.248 (0.248) loss 0.2285 (0.2285) acc 90.6250 (90.6250) lr 7.5131e-04 eta 0:04:33
epoch [118/200] batch [2/3] time 0.861 (0.981) data 0.000 (0.124) loss 0.1687 (0.1986) acc 96.8750 (93.7500) lr 7.5131e-04 eta 0:04:02
epoch [118/200] batch [3/3] time 0.859 (0.940) data 0.000 (0.083) loss 0.2856 (0.2276) acc 90.6250 (92.7083) lr 7.3613e-04 eta 0:03:51
epoch [119/200] batch [1/3] time 1.117 (1.117) data 0.258 (0.258) loss 0.3027 (0.3027) acc 90.6250 (90.6250) lr 7.3613e-04 eta 0:04:33
epoch [119/200] batch [2/3] time 0.864 (0.991) data 0.000 (0.129) loss 0.0846 (0.1937) acc 100.0000 (95.3125) lr 7.3613e-04 eta 0:04:01
epoch [119/200] batch [3/3] time 0.854 (0.945) data 0.000 (0.086) loss 0.2083 (0.1985) acc 96.8750 (95.8333) lr 7.2101e-04 eta 0:03:49
epoch [120/200] batch [1/3] time 1.109 (1.109) data 0.248 (0.248) loss 0.1852 (0.1852) acc 96.8750 (96.8750) lr 7.2101e-04 eta 0:04:28
epoch [120/200] batch [2/3] time 0.859 (0.984) data 0.000 (0.124) loss 0.1793 (0.1823) acc 90.6250 (93.7500) lr 7.2101e-04 eta 0:03:57
epoch [120/200] batch [3/3] time 0.855 (0.941) data 0.000 (0.083) loss 0.1127 (0.1591) acc 100.0000 (95.8333) lr 7.0596e-04 eta 0:03:45
epoch [121/200] batch [1/3] time 1.101 (1.101) data 0.239 (0.239) loss 0.1560 (0.1560) acc 93.7500 (93.7500) lr 7.0596e-04 eta 0:04:23
epoch [121/200] batch [2/3] time 0.859 (0.980) data 0.000 (0.119) loss 0.2040 (0.1800) acc 96.8750 (95.3125) lr 7.0596e-04 eta 0:03:53
epoch [121/200] batch [3/3] time 0.862 (0.941) data 0.000 (0.080) loss 0.0563 (0.1388) acc 100.0000 (96.8750) lr 6.9098e-04 eta 0:03:42
epoch [122/200] batch [1/3] time 1.106 (1.106) data 0.255 (0.255) loss 0.0621 (0.0621) acc 100.0000 (100.0000) lr 6.9098e-04 eta 0:04:21
epoch [122/200] batch [2/3] time 0.887 (0.997) data 0.000 (0.128) loss 0.3337 (0.1979) acc 90.6250 (95.3125) lr 6.9098e-04 eta 0:03:54
epoch [122/200] batch [3/3] time 0.868 (0.954) data 0.000 (0.085) loss 0.3149 (0.2369) acc 93.7500 (94.7917) lr 6.7608e-04 eta 0:03:43
epoch [123/200] batch [1/3] time 1.129 (1.129) data 0.259 (0.259) loss 0.1129 (0.1129) acc 96.8750 (96.8750) lr 6.7608e-04 eta 0:04:23
epoch [123/200] batch [2/3] time 0.883 (1.006) data 0.000 (0.129) loss 0.2231 (0.1680) acc 100.0000 (98.4375) lr 6.7608e-04 eta 0:03:53
epoch [123/200] batch [3/3] time 0.885 (0.966) data 0.000 (0.086) loss 0.1584 (0.1648) acc 96.8750 (97.9167) lr 6.6126e-04 eta 0:03:43
epoch [124/200] batch [1/3] time 1.104 (1.104) data 0.243 (0.243) loss 0.3987 (0.3987) acc 87.5000 (87.5000) lr 6.6126e-04 eta 0:04:13
epoch [124/200] batch [2/3] time 0.857 (0.981) data 0.000 (0.122) loss 0.4246 (0.4116) acc 84.3750 (85.9375) lr 6.6126e-04 eta 0:03:44
epoch [124/200] batch [3/3] time 0.862 (0.941) data 0.000 (0.081) loss 0.0747 (0.2993) acc 100.0000 (90.6250) lr 6.4653e-04 eta 0:03:34
epoch [125/200] batch [1/3] time 1.101 (1.101) data 0.240 (0.240) loss 0.1500 (0.1500) acc 100.0000 (100.0000) lr 6.4653e-04 eta 0:04:09
epoch [125/200] batch [2/3] time 0.857 (0.979) data 0.000 (0.120) loss 0.0835 (0.1168) acc 100.0000 (100.0000) lr 6.4653e-04 eta 0:03:41
epoch [125/200] batch [3/3] time 0.861 (0.940) data 0.000 (0.080) loss 0.2463 (0.1600) acc 93.7500 (97.9167) lr 6.3188e-04 eta 0:03:31
epoch [126/200] batch [1/3] time 1.120 (1.120) data 0.262 (0.262) loss 0.4653 (0.4653) acc 87.5000 (87.5000) lr 6.3188e-04 eta 0:04:10
epoch [126/200] batch [2/3] time 0.859 (0.990) data 0.000 (0.131) loss 0.3091 (0.3872) acc 93.7500 (90.6250) lr 6.3188e-04 eta 0:03:40
epoch [126/200] batch [3/3] time 0.859 (0.946) data 0.000 (0.087) loss 0.5420 (0.4388) acc 84.3750 (88.5417) lr 6.1732e-04 eta 0:03:30
epoch [127/200] batch [1/3] time 1.095 (1.095) data 0.237 (0.237) loss 0.3354 (0.3354) acc 87.5000 (87.5000) lr 6.1732e-04 eta 0:04:01
epoch [127/200] batch [2/3] time 0.856 (0.975) data 0.000 (0.118) loss 0.1827 (0.2591) acc 96.8750 (92.1875) lr 6.1732e-04 eta 0:03:34
epoch [127/200] batch [3/3] time 0.862 (0.938) data 0.000 (0.079) loss 0.2051 (0.2411) acc 90.6250 (91.6667) lr 6.0285e-04 eta 0:03:25
epoch [128/200] batch [1/3] time 1.103 (1.103) data 0.240 (0.240) loss 0.1262 (0.1262) acc 96.8750 (96.8750) lr 6.0285e-04 eta 0:04:00
epoch [128/200] batch [2/3] time 0.864 (0.983) data 0.000 (0.120) loss 0.1902 (0.1582) acc 96.8750 (96.8750) lr 6.0285e-04 eta 0:03:33
epoch [128/200] batch [3/3] time 0.857 (0.941) data 0.000 (0.080) loss 0.1128 (0.1431) acc 96.8750 (96.8750) lr 5.8849e-04 eta 0:03:23
epoch [129/200] batch [1/3] time 1.112 (1.112) data 0.249 (0.249) loss 0.4124 (0.4124) acc 87.5000 (87.5000) lr 5.8849e-04 eta 0:03:59
epoch [129/200] batch [2/3] time 0.863 (0.988) data 0.000 (0.125) loss 0.1081 (0.2602) acc 96.8750 (92.1875) lr 5.8849e-04 eta 0:03:31
epoch [129/200] batch [3/3] time 0.863 (0.946) data 0.000 (0.083) loss 0.2969 (0.2724) acc 93.7500 (92.7083) lr 5.7422e-04 eta 0:03:21
epoch [130/200] batch [1/3] time 1.127 (1.127) data 0.267 (0.267) loss 0.1216 (0.1216) acc 100.0000 (100.0000) lr 5.7422e-04 eta 0:03:58
epoch [130/200] batch [2/3] time 0.857 (0.992) data 0.000 (0.133) loss 0.2134 (0.1675) acc 96.8750 (98.4375) lr 5.7422e-04 eta 0:03:29
epoch [130/200] batch [3/3] time 0.862 (0.949) data 0.000 (0.089) loss 0.1289 (0.1546) acc 96.8750 (97.9167) lr 5.6006e-04 eta 0:03:19
epoch [131/200] batch [1/3] time 1.119 (1.119) data 0.255 (0.255) loss 0.4304 (0.4304) acc 81.2500 (81.2500) lr 5.6006e-04 eta 0:03:53
epoch [131/200] batch [2/3] time 0.858 (0.989) data 0.000 (0.127) loss 0.2507 (0.3406) acc 93.7500 (87.5000) lr 5.6006e-04 eta 0:03:25
epoch [131/200] batch [3/3] time 0.858 (0.945) data 0.000 (0.085) loss 0.4414 (0.3742) acc 90.6250 (88.5417) lr 5.4601e-04 eta 0:03:15
epoch [132/200] batch [1/3] time 1.098 (1.098) data 0.237 (0.237) loss 0.2764 (0.2764) acc 93.7500 (93.7500) lr 5.4601e-04 eta 0:03:46
epoch [132/200] batch [2/3] time 0.859 (0.979) data 0.000 (0.119) loss 0.1383 (0.2073) acc 96.8750 (95.3125) lr 5.4601e-04 eta 0:03:20
epoch [132/200] batch [3/3] time 0.862 (0.940) data 0.000 (0.079) loss 0.4736 (0.2961) acc 93.7500 (94.7917) lr 5.3207e-04 eta 0:03:11
epoch [133/200] batch [1/3] time 1.100 (1.100) data 0.240 (0.240) loss 0.5771 (0.5771) acc 87.5000 (87.5000) lr 5.3207e-04 eta 0:03:43
epoch [133/200] batch [2/3] time 0.843 (0.971) data 0.000 (0.120) loss 0.1694 (0.3733) acc 93.7500 (90.6250) lr 5.3207e-04 eta 0:03:16
epoch [133/200] batch [3/3] time 0.856 (0.933) data 0.000 (0.080) loss 0.3235 (0.3567) acc 90.6250 (90.6250) lr 5.1825e-04 eta 0:03:07
epoch [134/200] batch [1/3] time 1.119 (1.119) data 0.256 (0.256) loss 0.0862 (0.0862) acc 100.0000 (100.0000) lr 5.1825e-04 eta 0:03:43
epoch [134/200] batch [2/3] time 0.861 (0.990) data 0.000 (0.128) loss 0.3870 (0.2366) acc 90.6250 (95.3125) lr 5.1825e-04 eta 0:03:16
epoch [134/200] batch [3/3] time 0.862 (0.947) data 0.000 (0.085) loss 0.6382 (0.3705) acc 87.5000 (92.7083) lr 5.0454e-04 eta 0:03:07
epoch [135/200] batch [1/3] time 1.108 (1.108) data 0.248 (0.248) loss 0.4631 (0.4631) acc 87.5000 (87.5000) lr 5.0454e-04 eta 0:03:38
epoch [135/200] batch [2/3] time 0.860 (0.984) data 0.000 (0.124) loss 0.2603 (0.3617) acc 100.0000 (93.7500) lr 5.0454e-04 eta 0:03:12
epoch [135/200] batch [3/3] time 0.861 (0.943) data 0.000 (0.083) loss 0.0826 (0.2687) acc 100.0000 (95.8333) lr 4.9096e-04 eta 0:03:03
epoch [136/200] batch [1/3] time 1.114 (1.114) data 0.256 (0.256) loss 0.4961 (0.4961) acc 81.2500 (81.2500) lr 4.9096e-04 eta 0:03:36
epoch [136/200] batch [2/3] time 0.854 (0.984) data 0.000 (0.128) loss 0.4448 (0.4705) acc 87.5000 (84.3750) lr 4.9096e-04 eta 0:03:09
epoch [136/200] batch [3/3] time 0.859 (0.942) data 0.000 (0.086) loss 0.2849 (0.4086) acc 93.7500 (87.5000) lr 4.7750e-04 eta 0:03:00
epoch [137/200] batch [1/3] time 1.116 (1.116) data 0.257 (0.257) loss 0.2197 (0.2197) acc 96.8750 (96.8750) lr 4.7750e-04 eta 0:03:33
epoch [137/200] batch [2/3] time 0.858 (0.987) data 0.000 (0.128) loss 0.2698 (0.2448) acc 93.7500 (95.3125) lr 4.7750e-04 eta 0:03:07
epoch [137/200] batch [3/3] time 0.859 (0.945) data 0.000 (0.086) loss 0.3716 (0.2870) acc 90.6250 (93.7500) lr 4.6417e-04 eta 0:02:58
epoch [138/200] batch [1/3] time 1.107 (1.107) data 0.246 (0.246) loss 0.0809 (0.0809) acc 100.0000 (100.0000) lr 4.6417e-04 eta 0:03:28
epoch [138/200] batch [2/3] time 0.864 (0.986) data 0.000 (0.123) loss 0.4492 (0.2651) acc 90.6250 (95.3125) lr 4.6417e-04 eta 0:03:04
epoch [138/200] batch [3/3] time 0.861 (0.944) data 0.000 (0.082) loss 0.0709 (0.2003) acc 100.0000 (96.8750) lr 4.5098e-04 eta 0:02:55
epoch [139/200] batch [1/3] time 1.108 (1.108) data 0.245 (0.245) loss 0.1863 (0.1863) acc 100.0000 (100.0000) lr 4.5098e-04 eta 0:03:24
epoch [139/200] batch [2/3] time 0.863 (0.985) data 0.000 (0.123) loss 0.2157 (0.2010) acc 96.8750 (98.4375) lr 4.5098e-04 eta 0:03:01
epoch [139/200] batch [3/3] time 0.855 (0.942) data 0.000 (0.082) loss 0.2715 (0.2245) acc 90.6250 (95.8333) lr 4.3792e-04 eta 0:02:52
epoch [140/200] batch [1/3] time 1.139 (1.139) data 0.279 (0.279) loss 0.2864 (0.2864) acc 90.6250 (90.6250) lr 4.3792e-04 eta 0:03:27
epoch [140/200] batch [2/3] time 0.860 (1.000) data 0.000 (0.139) loss 0.0876 (0.1870) acc 100.0000 (95.3125) lr 4.3792e-04 eta 0:03:00
epoch [140/200] batch [3/3] time 0.859 (0.953) data 0.000 (0.093) loss 0.0788 (0.1509) acc 100.0000 (96.8750) lr 4.2499e-04 eta 0:02:51
epoch [141/200] batch [1/3] time 1.108 (1.108) data 0.256 (0.256) loss 0.2573 (0.2573) acc 93.7500 (93.7500) lr 4.2499e-04 eta 0:03:18
epoch [141/200] batch [2/3] time 0.858 (0.983) data 0.000 (0.128) loss 0.3354 (0.2964) acc 90.6250 (92.1875) lr 4.2499e-04 eta 0:02:54
epoch [141/200] batch [3/3] time 0.861 (0.942) data 0.000 (0.085) loss 0.3499 (0.3142) acc 93.7500 (92.7083) lr 4.1221e-04 eta 0:02:46
epoch [142/200] batch [1/3] time 1.102 (1.102) data 0.242 (0.242) loss 0.4338 (0.4338) acc 87.5000 (87.5000) lr 4.1221e-04 eta 0:03:13
epoch [142/200] batch [2/3] time 0.865 (0.983) data 0.000 (0.121) loss 0.6372 (0.5355) acc 87.5000 (87.5000) lr 4.1221e-04 eta 0:02:52
epoch [142/200] batch [3/3] time 0.855 (0.941) data 0.000 (0.081) loss 0.3691 (0.4801) acc 90.6250 (88.5417) lr 3.9958e-04 eta 0:02:43
epoch [143/200] batch [1/3] time 1.117 (1.117) data 0.256 (0.256) loss 0.2573 (0.2573) acc 96.8750 (96.8750) lr 3.9958e-04 eta 0:03:13
epoch [143/200] batch [2/3] time 0.855 (0.986) data 0.000 (0.128) loss 0.0606 (0.1590) acc 100.0000 (98.4375) lr 3.9958e-04 eta 0:02:49
epoch [143/200] batch [3/3] time 0.863 (0.945) data 0.000 (0.085) loss 0.1171 (0.1450) acc 100.0000 (98.9583) lr 3.8709e-04 eta 0:02:41
epoch [144/200] batch [1/3] time 1.111 (1.111) data 0.252 (0.252) loss 0.1226 (0.1226) acc 96.8750 (96.8750) lr 3.8709e-04 eta 0:03:08
epoch [144/200] batch [2/3] time 0.857 (0.984) data 0.000 (0.126) loss 0.2369 (0.1798) acc 90.6250 (93.7500) lr 3.8709e-04 eta 0:02:46
epoch [144/200] batch [3/3] time 0.860 (0.943) data 0.000 (0.084) loss 0.0919 (0.1505) acc 100.0000 (95.8333) lr 3.7476e-04 eta 0:02:38
epoch [145/200] batch [1/3] time 1.118 (1.118) data 0.256 (0.256) loss 0.1703 (0.1703) acc 96.8750 (96.8750) lr 3.7476e-04 eta 0:03:06
epoch [145/200] batch [2/3] time 0.867 (0.992) data 0.000 (0.128) loss 0.3027 (0.2365) acc 93.7500 (95.3125) lr 3.7476e-04 eta 0:02:44
epoch [145/200] batch [3/3] time 0.857 (0.947) data 0.000 (0.085) loss 0.5093 (0.3274) acc 90.6250 (93.7500) lr 3.6258e-04 eta 0:02:36
epoch [146/200] batch [1/3] time 1.117 (1.117) data 0.256 (0.256) loss 0.4836 (0.4836) acc 90.6250 (90.6250) lr 3.6258e-04 eta 0:03:03
epoch [146/200] batch [2/3] time 0.863 (0.990) data 0.000 (0.128) loss 0.6538 (0.5687) acc 78.1250 (84.3750) lr 3.6258e-04 eta 0:02:41
epoch [146/200] batch [3/3] time 0.864 (0.948) data 0.000 (0.085) loss 0.3098 (0.4824) acc 93.7500 (87.5000) lr 3.5055e-04 eta 0:02:33
epoch [147/200] batch [1/3] time 1.117 (1.117) data 0.255 (0.255) loss 0.0762 (0.0762) acc 100.0000 (100.0000) lr 3.5055e-04 eta 0:02:59
epoch [147/200] batch [2/3] time 0.861 (0.989) data 0.000 (0.128) loss 0.2981 (0.1872) acc 90.6250 (95.3125) lr 3.5055e-04 eta 0:02:38
epoch [147/200] batch [3/3] time 0.863 (0.947) data 0.000 (0.085) loss 0.5024 (0.2923) acc 84.3750 (91.6667) lr 3.3869e-04 eta 0:02:30
epoch [148/200] batch [1/3] time 1.110 (1.110) data 0.249 (0.249) loss 0.3870 (0.3870) acc 87.5000 (87.5000) lr 3.3869e-04 eta 0:02:55
epoch [148/200] batch [2/3] time 0.857 (0.983) data 0.000 (0.124) loss 0.0888 (0.2379) acc 96.8750 (92.1875) lr 3.3869e-04 eta 0:02:34
epoch [148/200] batch [3/3] time 0.863 (0.943) data 0.000 (0.083) loss 0.4683 (0.3147) acc 87.5000 (90.6250) lr 3.2699e-04 eta 0:02:27
epoch [149/200] batch [1/3] time 1.101 (1.101) data 0.240 (0.240) loss 0.1765 (0.1765) acc 93.7500 (93.7500) lr 3.2699e-04 eta 0:02:50
epoch [149/200] batch [2/3] time 0.860 (0.981) data 0.000 (0.120) loss 0.2883 (0.2324) acc 90.6250 (92.1875) lr 3.2699e-04 eta 0:02:31
epoch [149/200] batch [3/3] time 0.859 (0.940) data 0.000 (0.080) loss 0.2920 (0.2523) acc 90.6250 (91.6667) lr 3.1545e-04 eta 0:02:23
epoch [150/200] batch [1/3] time 1.115 (1.115) data 0.253 (0.253) loss 0.1544 (0.1544) acc 100.0000 (100.0000) lr 3.1545e-04 eta 0:02:49
epoch [150/200] batch [2/3] time 0.858 (0.987) data 0.000 (0.126) loss 0.3262 (0.2403) acc 90.6250 (95.3125) lr 3.1545e-04 eta 0:02:28
epoch [150/200] batch [3/3] time 0.861 (0.945) data 0.000 (0.084) loss 0.4580 (0.3129) acc 84.3750 (91.6667) lr 3.0409e-04 eta 0:02:21
epoch [151/200] batch [1/3] time 1.117 (1.117) data 0.254 (0.254) loss 0.1004 (0.1004) acc 100.0000 (100.0000) lr 3.0409e-04 eta 0:02:46
epoch [151/200] batch [2/3] time 0.864 (0.990) data 0.000 (0.127) loss 0.0904 (0.0954) acc 100.0000 (100.0000) lr 3.0409e-04 eta 0:02:26
epoch [151/200] batch [3/3] time 0.863 (0.948) data 0.000 (0.085) loss 0.0628 (0.0845) acc 100.0000 (100.0000) lr 2.9289e-04 eta 0:02:19
epoch [152/200] batch [1/3] time 1.117 (1.117) data 0.256 (0.256) loss 0.0974 (0.0974) acc 100.0000 (100.0000) lr 2.9289e-04 eta 0:02:43
epoch [152/200] batch [2/3] time 0.855 (0.986) data 0.000 (0.128) loss 0.3232 (0.2103) acc 90.6250 (95.3125) lr 2.9289e-04 eta 0:02:23
epoch [152/200] batch [3/3] time 0.862 (0.945) data 0.000 (0.085) loss 0.1115 (0.1774) acc 96.8750 (95.8333) lr 2.8187e-04 eta 0:02:16
epoch [153/200] batch [1/3] time 1.097 (1.097) data 0.246 (0.246) loss 0.6372 (0.6372) acc 84.3750 (84.3750) lr 2.8187e-04 eta 0:02:36
epoch [153/200] batch [2/3] time 0.858 (0.977) data 0.000 (0.123) loss 0.0780 (0.3576) acc 100.0000 (92.1875) lr 2.8187e-04 eta 0:02:18
epoch [153/200] batch [3/3] time 0.858 (0.938) data 0.000 (0.082) loss 0.1633 (0.2928) acc 90.6250 (91.6667) lr 2.7103e-04 eta 0:02:12
epoch [154/200] batch [1/3] time 1.119 (1.119) data 0.258 (0.258) loss 0.1774 (0.1774) acc 96.8750 (96.8750) lr 2.7103e-04 eta 0:02:36
epoch [154/200] batch [2/3] time 0.857 (0.988) data 0.000 (0.129) loss 0.6089 (0.3931) acc 87.5000 (92.1875) lr 2.7103e-04 eta 0:02:17
epoch [154/200] batch [3/3] time 0.857 (0.944) data 0.000 (0.086) loss 0.0997 (0.2953) acc 100.0000 (94.7917) lr 2.6037e-04 eta 0:02:10
epoch [155/200] batch [1/3] time 1.120 (1.120) data 0.258 (0.258) loss 0.1177 (0.1177) acc 100.0000 (100.0000) lr 2.6037e-04 eta 0:02:33
epoch [155/200] batch [2/3] time 0.859 (0.989) data 0.000 (0.129) loss 0.1852 (0.1514) acc 90.6250 (95.3125) lr 2.6037e-04 eta 0:02:14
epoch [155/200] batch [3/3] time 0.864 (0.948) data 0.000 (0.086) loss 0.0882 (0.1304) acc 100.0000 (96.8750) lr 2.4989e-04 eta 0:02:07
epoch [156/200] batch [1/3] time 1.112 (1.112) data 0.251 (0.251) loss 0.3696 (0.3696) acc 87.5000 (87.5000) lr 2.4989e-04 eta 0:02:28
epoch [156/200] batch [2/3] time 0.862 (0.987) data 0.000 (0.126) loss 0.1215 (0.2455) acc 100.0000 (93.7500) lr 2.4989e-04 eta 0:02:11
epoch [156/200] batch [3/3] time 0.863 (0.945) data 0.000 (0.084) loss 0.2527 (0.2479) acc 96.8750 (94.7917) lr 2.3959e-04 eta 0:02:04
epoch [157/200] batch [1/3] time 1.109 (1.109) data 0.255 (0.255) loss 0.3303 (0.3303) acc 93.7500 (93.7500) lr 2.3959e-04 eta 0:02:25
epoch [157/200] batch [2/3] time 0.864 (0.987) data 0.000 (0.128) loss 0.1625 (0.2464) acc 96.8750 (95.3125) lr 2.3959e-04 eta 0:02:08
epoch [157/200] batch [3/3] time 0.854 (0.942) data 0.000 (0.085) loss 0.2496 (0.2475) acc 90.6250 (93.7500) lr 2.2949e-04 eta 0:02:01
epoch [158/200] batch [1/3] time 1.119 (1.119) data 0.257 (0.257) loss 0.4368 (0.4368) acc 90.6250 (90.6250) lr 2.2949e-04 eta 0:02:23
epoch [158/200] batch [2/3] time 0.862 (0.990) data 0.000 (0.129) loss 0.3508 (0.3938) acc 90.6250 (90.6250) lr 2.2949e-04 eta 0:02:05
epoch [158/200] batch [3/3] time 0.860 (0.947) data 0.000 (0.086) loss 0.3728 (0.3868) acc 93.7500 (91.6667) lr 2.1957e-04 eta 0:01:59
epoch [159/200] batch [1/3] time 1.111 (1.111) data 0.250 (0.250) loss 0.3416 (0.3416) acc 96.8750 (96.8750) lr 2.1957e-04 eta 0:02:18
epoch [159/200] batch [2/3] time 0.862 (0.987) data 0.000 (0.125) loss 0.0739 (0.2077) acc 100.0000 (98.4375) lr 2.1957e-04 eta 0:02:02
epoch [159/200] batch [3/3] time 0.850 (0.941) data 0.000 (0.083) loss 0.1760 (0.1971) acc 96.8750 (97.9167) lr 2.0984e-04 eta 0:01:55
epoch [160/200] batch [1/3] time 1.119 (1.119) data 0.257 (0.257) loss 0.3376 (0.3376) acc 90.6250 (90.6250) lr 2.0984e-04 eta 0:02:16
epoch [160/200] batch [2/3] time 0.856 (0.987) data 0.000 (0.129) loss 0.1895 (0.2635) acc 96.8750 (93.7500) lr 2.0984e-04 eta 0:01:59
epoch [160/200] batch [3/3] time 0.861 (0.945) data 0.000 (0.086) loss 0.1819 (0.2363) acc 93.7500 (93.7500) lr 2.0032e-04 eta 0:01:53
epoch [161/200] batch [1/3] time 1.108 (1.108) data 0.247 (0.247) loss 0.2761 (0.2761) acc 90.6250 (90.6250) lr 2.0032e-04 eta 0:02:11
epoch [161/200] batch [2/3] time 0.863 (0.985) data 0.000 (0.123) loss 0.2261 (0.2511) acc 90.6250 (90.6250) lr 2.0032e-04 eta 0:01:56
epoch [161/200] batch [3/3] time 0.864 (0.945) data 0.000 (0.082) loss 0.3528 (0.2850) acc 87.5000 (89.5833) lr 1.9098e-04 eta 0:01:50
epoch [162/200] batch [1/3] time 1.118 (1.118) data 0.257 (0.257) loss 0.3313 (0.3313) acc 93.7500 (93.7500) lr 1.9098e-04 eta 0:02:09
epoch [162/200] batch [2/3] time 0.863 (0.990) data 0.000 (0.129) loss 0.1711 (0.2512) acc 96.8750 (95.3125) lr 1.9098e-04 eta 0:01:53
epoch [162/200] batch [3/3] time 0.862 (0.948) data 0.000 (0.086) loss 0.4185 (0.3070) acc 90.6250 (93.7500) lr 1.8185e-04 eta 0:01:48
epoch [163/200] batch [1/3] time 1.115 (1.115) data 0.254 (0.254) loss 0.1112 (0.1112) acc 100.0000 (100.0000) lr 1.8185e-04 eta 0:02:06
epoch [163/200] batch [2/3] time 0.855 (0.985) data 0.000 (0.127) loss 0.0854 (0.0983) acc 100.0000 (100.0000) lr 1.8185e-04 eta 0:01:50
epoch [163/200] batch [3/3] time 0.861 (0.944) data 0.000 (0.085) loss 0.3411 (0.1792) acc 90.6250 (96.8750) lr 1.7292e-04 eta 0:01:44
epoch [164/200] batch [1/3] time 1.100 (1.100) data 0.238 (0.238) loss 0.4219 (0.4219) acc 93.7500 (93.7500) lr 1.7292e-04 eta 0:02:01
epoch [164/200] batch [2/3] time 0.860 (0.980) data 0.000 (0.119) loss 0.3652 (0.3936) acc 90.6250 (92.1875) lr 1.7292e-04 eta 0:01:46
epoch [164/200] batch [3/3] time 0.857 (0.939) data 0.000 (0.079) loss 0.5986 (0.4619) acc 81.2500 (88.5417) lr 1.6419e-04 eta 0:01:41
epoch [165/200] batch [1/3] time 1.112 (1.112) data 0.251 (0.251) loss 0.3752 (0.3752) acc 90.6250 (90.6250) lr 1.6419e-04 eta 0:01:58
epoch [165/200] batch [2/3] time 0.861 (0.987) data 0.000 (0.125) loss 0.1053 (0.2403) acc 96.8750 (93.7500) lr 1.6419e-04 eta 0:01:44
epoch [165/200] batch [3/3] time 0.857 (0.944) data 0.000 (0.084) loss 0.0770 (0.1859) acc 100.0000 (95.8333) lr 1.5567e-04 eta 0:01:39
epoch [166/200] batch [1/3] time 1.108 (1.108) data 0.247 (0.247) loss 0.4370 (0.4370) acc 87.5000 (87.5000) lr 1.5567e-04 eta 0:01:55
epoch [166/200] batch [2/3] time 0.860 (0.984) data 0.000 (0.123) loss 0.0730 (0.2550) acc 100.0000 (93.7500) lr 1.5567e-04 eta 0:01:41
epoch [166/200] batch [3/3] time 0.862 (0.943) data 0.000 (0.082) loss 0.6597 (0.3899) acc 81.2500 (89.5833) lr 1.4736e-04 eta 0:01:36
epoch [167/200] batch [1/3] time 1.110 (1.110) data 0.249 (0.249) loss 0.2817 (0.2817) acc 90.6250 (90.6250) lr 1.4736e-04 eta 0:01:52
epoch [167/200] batch [2/3] time 0.860 (0.985) data 0.000 (0.125) loss 0.0999 (0.1908) acc 100.0000 (95.3125) lr 1.4736e-04 eta 0:01:38
epoch [167/200] batch [3/3] time 0.856 (0.942) data 0.000 (0.083) loss 0.3269 (0.2362) acc 90.6250 (93.7500) lr 1.3926e-04 eta 0:01:33
epoch [168/200] batch [1/3] time 1.102 (1.102) data 0.240 (0.240) loss 0.1041 (0.1041) acc 100.0000 (100.0000) lr 1.3926e-04 eta 0:01:48
epoch [168/200] batch [2/3] time 0.859 (0.981) data 0.000 (0.120) loss 0.4304 (0.2672) acc 87.5000 (93.7500) lr 1.3926e-04 eta 0:01:35
epoch [168/200] batch [3/3] time 0.864 (0.942) data 0.000 (0.080) loss 0.0616 (0.1987) acc 100.0000 (95.8333) lr 1.3137e-04 eta 0:01:30
epoch [169/200] batch [1/3] time 1.102 (1.102) data 0.241 (0.241) loss 0.0981 (0.0981) acc 96.8750 (96.8750) lr 1.3137e-04 eta 0:01:44
epoch [169/200] batch [2/3] time 0.858 (0.980) data 0.000 (0.121) loss 0.1252 (0.1117) acc 96.8750 (96.8750) lr 1.3137e-04 eta 0:01:32
epoch [169/200] batch [3/3] time 0.856 (0.939) data 0.000 (0.080) loss 0.0216 (0.0816) acc 100.0000 (97.9167) lr 1.2369e-04 eta 0:01:27
epoch [170/200] batch [1/3] time 1.109 (1.109) data 0.249 (0.249) loss 0.1423 (0.1423) acc 96.8750 (96.8750) lr 1.2369e-04 eta 0:01:42
epoch [170/200] batch [2/3] time 0.855 (0.982) data 0.000 (0.124) loss 0.5488 (0.3456) acc 93.7500 (95.3125) lr 1.2369e-04 eta 0:01:29
epoch [170/200] batch [3/3] time 0.864 (0.943) data 0.000 (0.083) loss 0.2079 (0.2997) acc 93.7500 (94.7917) lr 1.1623e-04 eta 0:01:24
epoch [171/200] batch [1/3] time 1.119 (1.119) data 0.257 (0.257) loss 0.2043 (0.2043) acc 96.8750 (96.8750) lr 1.1623e-04 eta 0:01:39
epoch [171/200] batch [2/3] time 0.864 (0.992) data 0.000 (0.129) loss 0.2529 (0.2286) acc 96.8750 (96.8750) lr 1.1623e-04 eta 0:01:27
epoch [171/200] batch [3/3] time 0.855 (0.946) data 0.000 (0.086) loss 0.1119 (0.1897) acc 100.0000 (97.9167) lr 1.0899e-04 eta 0:01:22
epoch [172/200] batch [1/3] time 1.101 (1.101) data 0.239 (0.239) loss 0.4744 (0.4744) acc 87.5000 (87.5000) lr 1.0899e-04 eta 0:01:34
epoch [172/200] batch [2/3] time 0.862 (0.981) data 0.000 (0.120) loss 0.1490 (0.3117) acc 93.7500 (90.6250) lr 1.0899e-04 eta 0:01:23
epoch [172/200] batch [3/3] time 0.861 (0.941) data 0.000 (0.080) loss 0.2001 (0.2745) acc 96.8750 (92.7083) lr 1.0197e-04 eta 0:01:19
epoch [173/200] batch [1/3] time 1.119 (1.119) data 0.256 (0.256) loss 0.0847 (0.0847) acc 100.0000 (100.0000) lr 1.0197e-04 eta 0:01:32
epoch [173/200] batch [2/3] time 0.862 (0.990) data 0.000 (0.128) loss 0.2413 (0.1630) acc 93.7500 (96.8750) lr 1.0197e-04 eta 0:01:21
epoch [173/200] batch [3/3] time 0.862 (0.947) data 0.000 (0.085) loss 0.3486 (0.2249) acc 93.7500 (95.8333) lr 9.5173e-05 eta 0:01:16
epoch [174/200] batch [1/3] time 1.103 (1.103) data 0.241 (0.241) loss 0.1869 (0.1869) acc 93.7500 (93.7500) lr 9.5173e-05 eta 0:01:28
epoch [174/200] batch [2/3] time 0.857 (0.980) data 0.000 (0.121) loss 0.3325 (0.2597) acc 93.7500 (93.7500) lr 9.5173e-05 eta 0:01:17
epoch [174/200] batch [3/3] time 0.856 (0.939) data 0.000 (0.081) loss 0.4463 (0.3219) acc 90.6250 (92.7083) lr 8.8597e-05 eta 0:01:13
epoch [175/200] batch [1/3] time 1.109 (1.109) data 0.249 (0.249) loss 0.1704 (0.1704) acc 93.7500 (93.7500) lr 8.8597e-05 eta 0:01:25
epoch [175/200] batch [2/3] time 0.857 (0.983) data 0.000 (0.124) loss 0.1766 (0.1735) acc 96.8750 (95.3125) lr 8.8597e-05 eta 0:01:14
epoch [175/200] batch [3/3] time 0.853 (0.939) data 0.000 (0.083) loss 0.2457 (0.1976) acc 93.7500 (94.7917) lr 8.2245e-05 eta 0:01:10
epoch [176/200] batch [1/3] time 1.101 (1.101) data 0.240 (0.240) loss 0.1740 (0.1740) acc 90.6250 (90.6250) lr 8.2245e-05 eta 0:01:21
epoch [176/200] batch [2/3] time 0.854 (0.978) data 0.000 (0.120) loss 0.2690 (0.2215) acc 93.7500 (92.1875) lr 8.2245e-05 eta 0:01:11
epoch [176/200] batch [3/3] time 0.864 (0.940) data 0.000 (0.080) loss 0.3035 (0.2488) acc 90.6250 (91.6667) lr 7.6120e-05 eta 0:01:07
epoch [177/200] batch [1/3] time 1.110 (1.110) data 0.257 (0.257) loss 0.2120 (0.2120) acc 93.7500 (93.7500) lr 7.6120e-05 eta 0:01:18
epoch [177/200] batch [2/3] time 0.859 (0.985) data 0.000 (0.129) loss 0.3118 (0.2619) acc 96.8750 (95.3125) lr 7.6120e-05 eta 0:01:08
epoch [177/200] batch [3/3] time 0.861 (0.944) data 0.000 (0.086) loss 0.2563 (0.2601) acc 96.8750 (95.8333) lr 7.0224e-05 eta 0:01:05
epoch [178/200] batch [1/3] time 1.121 (1.121) data 0.260 (0.260) loss 0.2336 (0.2336) acc 93.7500 (93.7500) lr 7.0224e-05 eta 0:01:16
epoch [178/200] batch [2/3] time 0.856 (0.989) data 0.000 (0.130) loss 0.3525 (0.2931) acc 90.6250 (92.1875) lr 7.0224e-05 eta 0:01:06
epoch [178/200] batch [3/3] time 0.863 (0.947) data 0.000 (0.087) loss 0.0517 (0.2126) acc 100.0000 (94.7917) lr 6.4556e-05 eta 0:01:02
epoch [179/200] batch [1/3] time 1.102 (1.102) data 0.241 (0.241) loss 0.2426 (0.2426) acc 93.7500 (93.7500) lr 6.4556e-05 eta 0:01:11
epoch [179/200] batch [2/3] time 0.857 (0.980) data 0.000 (0.121) loss 0.1829 (0.2127) acc 96.8750 (95.3125) lr 6.4556e-05 eta 0:01:02
epoch [179/200] batch [3/3] time 0.856 (0.939) data 0.000 (0.081) loss 0.2247 (0.2167) acc 96.8750 (95.8333) lr 5.9119e-05 eta 0:00:59
epoch [180/200] batch [1/3] time 1.105 (1.105) data 0.242 (0.242) loss 0.1483 (0.1483) acc 96.8750 (96.8750) lr 5.9119e-05 eta 0:01:08
epoch [180/200] batch [2/3] time 0.855 (0.980) data 0.000 (0.121) loss 0.3459 (0.2471) acc 96.8750 (96.8750) lr 5.9119e-05 eta 0:00:59
epoch [180/200] batch [3/3] time 0.859 (0.940) data 0.000 (0.081) loss 0.1683 (0.2209) acc 96.8750 (96.8750) lr 5.3915e-05 eta 0:00:56
epoch [181/200] batch [1/3] time 1.094 (1.094) data 0.241 (0.241) loss 0.4688 (0.4688) acc 87.5000 (87.5000) lr 5.3915e-05 eta 0:01:04
epoch [181/200] batch [2/3] time 0.861 (0.978) data 0.000 (0.121) loss 0.2458 (0.3573) acc 96.8750 (92.1875) lr 5.3915e-05 eta 0:00:56
epoch [181/200] batch [3/3] time 0.866 (0.941) data 0.000 (0.081) loss 0.1064 (0.2737) acc 96.8750 (93.7500) lr 4.8943e-05 eta 0:00:53
epoch [182/200] batch [1/3] time 1.115 (1.115) data 0.254 (0.254) loss 0.3992 (0.3992) acc 87.5000 (87.5000) lr 4.8943e-05 eta 0:01:02
epoch [182/200] batch [2/3] time 0.856 (0.985) data 0.000 (0.127) loss 0.0812 (0.2402) acc 100.0000 (93.7500) lr 4.8943e-05 eta 0:00:54
epoch [182/200] batch [3/3] time 0.860 (0.943) data 0.000 (0.085) loss 0.3252 (0.2685) acc 93.7500 (93.7500) lr 4.4207e-05 eta 0:00:50
epoch [183/200] batch [1/3] time 1.102 (1.102) data 0.241 (0.241) loss 0.5327 (0.5327) acc 87.5000 (87.5000) lr 4.4207e-05 eta 0:00:58
epoch [183/200] batch [2/3] time 0.858 (0.980) data 0.000 (0.121) loss 0.2612 (0.3970) acc 90.6250 (89.0625) lr 4.4207e-05 eta 0:00:50
epoch [183/200] batch [3/3] time 0.858 (0.940) data 0.000 (0.080) loss 0.1223 (0.3054) acc 100.0000 (92.7083) lr 3.9706e-05 eta 0:00:47
epoch [184/200] batch [1/3] time 1.100 (1.100) data 0.238 (0.238) loss 0.1960 (0.1960) acc 93.7500 (93.7500) lr 3.9706e-05 eta 0:00:55
epoch [184/200] batch [2/3] time 0.855 (0.978) data 0.000 (0.119) loss 0.4578 (0.3269) acc 90.6250 (92.1875) lr 3.9706e-05 eta 0:00:47
epoch [184/200] batch [3/3] time 0.863 (0.940) data 0.000 (0.080) loss 0.1715 (0.2751) acc 96.8750 (93.7500) lr 3.5443e-05 eta 0:00:45
epoch [185/200] batch [1/3] time 1.094 (1.094) data 0.240 (0.240) loss 0.4834 (0.4834) acc 87.5000 (87.5000) lr 3.5443e-05 eta 0:00:51
epoch [185/200] batch [2/3] time 0.861 (0.977) data 0.000 (0.120) loss 0.3416 (0.4125) acc 93.7500 (90.6250) lr 3.5443e-05 eta 0:00:44
epoch [185/200] batch [3/3] time 0.864 (0.940) data 0.000 (0.080) loss 0.1107 (0.3119) acc 96.8750 (92.7083) lr 3.1417e-05 eta 0:00:42
epoch [186/200] batch [1/3] time 1.118 (1.118) data 0.257 (0.257) loss 0.3745 (0.3745) acc 90.6250 (90.6250) lr 3.1417e-05 eta 0:00:49
epoch [186/200] batch [2/3] time 0.858 (0.988) data 0.000 (0.128) loss 0.0828 (0.2286) acc 100.0000 (95.3125) lr 3.1417e-05 eta 0:00:42
epoch [186/200] batch [3/3] time 0.856 (0.944) data 0.000 (0.086) loss 0.1864 (0.2146) acc 96.8750 (95.8333) lr 2.7630e-05 eta 0:00:39
epoch [187/200] batch [1/3] time 1.105 (1.105) data 0.244 (0.244) loss 0.3757 (0.3757) acc 93.7500 (93.7500) lr 2.7630e-05 eta 0:00:45
epoch [187/200] batch [2/3] time 0.864 (0.985) data 0.000 (0.122) loss 0.5513 (0.4635) acc 87.5000 (90.6250) lr 2.7630e-05 eta 0:00:39
epoch [187/200] batch [3/3] time 0.851 (0.940) data 0.000 (0.081) loss 0.3481 (0.4250) acc 90.6250 (90.6250) lr 2.4083e-05 eta 0:00:36
epoch [188/200] batch [1/3] time 1.109 (1.109) data 0.249 (0.249) loss 0.1107 (0.1107) acc 100.0000 (100.0000) lr 2.4083e-05 eta 0:00:42
epoch [188/200] batch [2/3] time 0.852 (0.981) data 0.000 (0.124) loss 0.2142 (0.1625) acc 100.0000 (100.0000) lr 2.4083e-05 eta 0:00:36
epoch [188/200] batch [3/3] time 0.860 (0.940) data 0.000 (0.083) loss 0.3640 (0.2297) acc 93.7500 (97.9167) lr 2.0777e-05 eta 0:00:33
epoch [189/200] batch [1/3] time 1.092 (1.092) data 0.240 (0.240) loss 0.1958 (0.1958) acc 96.8750 (96.8750) lr 2.0777e-05 eta 0:00:38
epoch [189/200] batch [2/3] time 0.863 (0.978) data 0.000 (0.120) loss 0.1609 (0.1783) acc 96.8750 (96.8750) lr 2.0777e-05 eta 0:00:33
epoch [189/200] batch [3/3] time 0.864 (0.940) data 0.000 (0.080) loss 0.2515 (0.2027) acc 93.7500 (95.8333) lr 1.7713e-05 eta 0:00:31
epoch [190/200] batch [1/3] time 1.116 (1.116) data 0.254 (0.254) loss 0.3569 (0.3569) acc 93.7500 (93.7500) lr 1.7713e-05 eta 0:00:35
epoch [190/200] batch [2/3] time 0.867 (0.992) data 0.000 (0.127) loss 0.3035 (0.3302) acc 93.7500 (93.7500) lr 1.7713e-05 eta 0:00:30
epoch [190/200] batch [3/3] time 0.860 (0.948) data 0.000 (0.085) loss 0.1078 (0.2561) acc 96.8750 (94.7917) lr 1.4891e-05 eta 0:00:28
epoch [191/200] batch [1/3] time 1.105 (1.105) data 0.242 (0.242) loss 0.1505 (0.1505) acc 96.8750 (96.8750) lr 1.4891e-05 eta 0:00:32
epoch [191/200] batch [2/3] time 0.857 (0.981) data 0.000 (0.121) loss 0.2737 (0.2121) acc 93.7500 (95.3125) lr 1.4891e-05 eta 0:00:27
epoch [191/200] batch [3/3] time 0.861 (0.941) data 0.000 (0.081) loss 0.3987 (0.2743) acc 90.6250 (93.7500) lr 1.2312e-05 eta 0:00:25
epoch [192/200] batch [1/3] time 1.102 (1.102) data 0.241 (0.241) loss 0.3816 (0.3816) acc 90.6250 (90.6250) lr 1.2312e-05 eta 0:00:28
epoch [192/200] batch [2/3] time 0.857 (0.980) data 0.000 (0.121) loss 0.1186 (0.2501) acc 100.0000 (95.3125) lr 1.2312e-05 eta 0:00:24
epoch [192/200] batch [3/3] time 0.856 (0.938) data 0.000 (0.080) loss 0.1321 (0.2108) acc 96.8750 (95.8333) lr 9.9763e-06 eta 0:00:22
epoch [193/200] batch [1/3] time 1.102 (1.102) data 0.240 (0.240) loss 0.1379 (0.1379) acc 100.0000 (100.0000) lr 9.9763e-06 eta 0:00:25
epoch [193/200] batch [2/3] time 0.862 (0.982) data 0.000 (0.120) loss 0.4985 (0.3182) acc 93.7500 (96.8750) lr 9.9763e-06 eta 0:00:21
epoch [193/200] batch [3/3] time 0.858 (0.940) data 0.000 (0.080) loss 0.1840 (0.2735) acc 96.8750 (96.8750) lr 7.8853e-06 eta 0:00:19
epoch [194/200] batch [1/3] time 1.111 (1.111) data 0.250 (0.250) loss 0.1371 (0.1371) acc 100.0000 (100.0000) lr 7.8853e-06 eta 0:00:22
epoch [194/200] batch [2/3] time 0.857 (0.984) data 0.000 (0.125) loss 0.2710 (0.2040) acc 90.6250 (95.3125) lr 7.8853e-06 eta 0:00:18
epoch [194/200] batch [3/3] time 0.862 (0.943) data 0.000 (0.083) loss 0.2500 (0.2194) acc 96.8750 (95.8333) lr 6.0390e-06 eta 0:00:16
epoch [195/200] batch [1/3] time 1.116 (1.116) data 0.253 (0.253) loss 0.3018 (0.3018) acc 93.7500 (93.7500) lr 6.0390e-06 eta 0:00:18
epoch [195/200] batch [2/3] time 0.854 (0.985) data 0.000 (0.126) loss 0.2783 (0.2900) acc 93.7500 (93.7500) lr 6.0390e-06 eta 0:00:15
epoch [195/200] batch [3/3] time 0.859 (0.943) data 0.000 (0.084) loss 0.1396 (0.2399) acc 100.0000 (95.8333) lr 4.4380e-06 eta 0:00:14
epoch [196/200] batch [1/3] time 1.116 (1.116) data 0.256 (0.256) loss 0.4204 (0.4204) acc 87.5000 (87.5000) lr 4.4380e-06 eta 0:00:15
epoch [196/200] batch [2/3] time 0.864 (0.990) data 0.000 (0.128) loss 0.4792 (0.4498) acc 93.7500 (90.6250) lr 4.4380e-06 eta 0:00:12
epoch [196/200] batch [3/3] time 0.867 (0.949) data 0.000 (0.085) loss 0.2563 (0.3853) acc 90.6250 (90.6250) lr 3.0827e-06 eta 0:00:11
epoch [197/200] batch [1/3] time 1.115 (1.115) data 0.251 (0.251) loss 0.3486 (0.3486) acc 87.5000 (87.5000) lr 3.0827e-06 eta 0:00:12
epoch [197/200] batch [2/3] time 0.860 (0.988) data 0.000 (0.126) loss 0.2729 (0.3108) acc 93.7500 (90.6250) lr 3.0827e-06 eta 0:00:09
epoch [197/200] batch [3/3] time 0.857 (0.944) data 0.000 (0.084) loss 0.1216 (0.2477) acc 100.0000 (93.7500) lr 1.9733e-06 eta 0:00:08
epoch [198/200] batch [1/3] time 1.101 (1.101) data 0.242 (0.242) loss 0.3127 (0.3127) acc 93.7500 (93.7500) lr 1.9733e-06 eta 0:00:08
epoch [198/200] batch [2/3] time 0.863 (0.982) data 0.000 (0.121) loss 0.1478 (0.2303) acc 96.8750 (95.3125) lr 1.9733e-06 eta 0:00:06
epoch [198/200] batch [3/3] time 0.863 (0.942) data 0.000 (0.081) loss 0.1262 (0.1956) acc 100.0000 (96.8750) lr 1.1101e-06 eta 0:00:05
epoch [199/200] batch [1/3] time 1.116 (1.116) data 0.257 (0.257) loss 0.1201 (0.1201) acc 96.8750 (96.8750) lr 1.1101e-06 eta 0:00:05
epoch [199/200] batch [2/3] time 0.866 (0.991) data 0.000 (0.128) loss 0.1823 (0.1512) acc 93.7500 (95.3125) lr 1.1101e-06 eta 0:00:03
epoch [199/200] batch [3/3] time 0.850 (0.944) data 0.000 (0.086) loss 0.2094 (0.1706) acc 93.7500 (94.7917) lr 4.9344e-07 eta 0:00:02
epoch [200/200] batch [1/3] time 1.106 (1.106) data 0.244 (0.244) loss 0.3518 (0.3518) acc 93.7500 (93.7500) lr 4.9344e-07 eta 0:00:02
epoch [200/200] batch [2/3] time 0.854 (0.980) data 0.000 (0.122) loss 0.2500 (0.3009) acc 93.7500 (93.7500) lr 4.9344e-07 eta 0:00:00
epoch [200/200] batch [3/3] time 0.857 (0.939) data 0.000 (0.082) loss 0.1809 (0.2609) acc 96.8750 (94.7917) lr 1.2337e-07 eta 0:00:00
Checkpoint saved to output/Caltech/1/prompt_learner/model.pth.tar-200
Finish training
Deploy the last-epoch model
Evaluate on the *test* set
=> result
* total: 2,465
* correct: 2,001
* accuracy: 81.2%
* error: 18.8%
* macro_f1: 73.7%
Elapsed: 0:10:07
args2: backbone=, config_file=configs/trainers/CoOp/rn50.yaml, dataset_config_file=configs/datasets/caltech101.yaml, eval_only=False, head=, load_epoch=None, model_dir=, no_train=False,  opts: ['TRAINER.COOP.N_CTX', '16', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '1'], output_dir=output/Caltech/1/2, resume=, root=/home/brandnerkasper/Uni/MP/MP_CustomCoOp/data, seed=2, source_domains=None, target_domains=None, trainer=CoOp, transforms=None
Setting fixed seed: 2
***************
** Arguments **
***************
config_file: configs/trainers/CoOp/rn50.yaml
csc: False
ctp: end
dataset_config_file: configs/datasets/caltech101.yaml
n_ctx: 16
opts: ['TRAINER.COOP.N_CTX', '16', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '1']
output_dir: output/Caltech/1/2
root: /home/brandnerkasper/Uni/MP/MP_CustomCoOp/data
seed: 2
shots: 1
trainer: CoOp
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 4
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: Caltech101
  NUM_LABELED: -1
  NUM_SHOTS: 1
  ROOT: /home/brandnerkasper/Uni/MP/MP_CustomCoOp/data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 200
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/Caltech/1/2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: 
    N_CTX: 16
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: 
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: CoOp
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 2.0.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 22.04.3 LTS (x86_64)
GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.35

Python version: 3.10.12 (main, Jul  5 2023, 18:54:27) [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-6.2.0-32-generic-x86_64-with-glibc2.35
Is CUDA available: True
CUDA runtime version: 11.5.119
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA GeForce GTX 970
Nvidia driver version: 525.125.06
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Architecture:                       x86_64
CPU op-mode(s):                     32-bit, 64-bit
Address sizes:                      39 bits physical, 48 bits virtual
Byte Order:                         Little Endian
CPU(s):                             4
On-line CPU(s) list:                0-3
Vendor ID:                          GenuineIntel
Model name:                         Intel(R) Xeon(R) CPU E3-1225 v3 @ 3.20GHz
CPU family:                         6
Model:                              60
Thread(s) per core:                 1
Core(s) per socket:                 4
Socket(s):                          1
Stepping:                           3
CPU max MHz:                        3600,0000
CPU min MHz:                        800,0000
BogoMIPS:                           6397.95
Flags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm cpuid_fault invpcid_single pti ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid xsaveopt dtherm ida arat pln pts md_clear flush_l1d
Virtualization:                     VT-x
L1d cache:                          128 KiB (4 instances)
L1i cache:                          128 KiB (4 instances)
L2 cache:                           1 MiB (4 instances)
L3 cache:                           8 MiB (1 instance)
NUMA node(s):                       1
NUMA node0 CPU(s):                  0-3
Vulnerability Gather data sampling: Not affected
Vulnerability Itlb multihit:        KVM: Mitigation: VMX disabled
Vulnerability L1tf:                 Mitigation; PTE Inversion; VMX conditional cache flushes, SMT disabled
Vulnerability Mds:                  Mitigation; Clear CPU buffers; SMT disabled
Vulnerability Meltdown:             Mitigation; PTI
Vulnerability Mmio stale data:      Unknown: No mitigations
Vulnerability Retbleed:             Not affected
Vulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl
Vulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization
Vulnerability Spectre v2:           Mitigation; Retpolines, IBPB conditional, IBRS_FW, STIBP disabled, RSB filling, PBRSB-eIBRS Not affected
Vulnerability Srbds:                Mitigation; Microcode
Vulnerability Tsx async abort:      Not affected

Versions of relevant libraries:
[pip3] numpy==1.25.2
[pip3] open-clip-torch==2.20.0
[pip3] torch==2.0.1
[pip3] torchaudio==2.0.2
[pip3] torchvision==0.15.2
[conda] blas                      1.0                         mkl  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] mkl                       2023.1.0         h213fc3f_46343  
[conda] mkl-service               2.4.0           py310h5eee18b_1  
[conda] mkl_fft                   1.3.6           py310h1128e8f_1  
[conda] mkl_random                1.2.2           py310h1128e8f_1  
[conda] numpy                     1.25.2          py310h5f9d8c6_0  
[conda] numpy-base                1.25.2          py310hb5e798b_0  
[conda] open-clip-torch           2.20.0                   pypi_0    pypi
[conda] pytorch                   2.0.1           py3.10_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchaudio                2.0.2               py310_cu117    pytorch
[conda] torchtriton               2.0.0                     py310    pytorch
[conda] torchvision               0.15.2              py310_cu117    pytorch
        Pillow (9.4.0)

Loading trainer: CoOp
Loading dataset: Caltech101
Reading split from /home/brandnerkasper/Uni/MP/MP_CustomCoOp/data/caltech-101/split_zhou_Caltech101.json
Creating a 1-shot dataset
Creating a 1-shot dataset
Saving preprocessed few-shot data to /home/brandnerkasper/Uni/MP/MP_CustomCoOp/data/caltech-101/split_fewshot/shot_1-seed_2.pkl
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ----------
Dataset    Caltech101
# classes  100
# train_x  100
# val      100
# test     2,465
---------  ----------
Loading CLIP (backbone: RN50)
CLIP(
  (visual): ModifiedResNet(
    (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu1): ReLU(inplace=True)
    (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu2): ReLU(inplace=True)
    (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu3): ReLU(inplace=True)
    (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (layer1): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
        (downsample): Sequential(
          (-1): AvgPool2d(kernel_size=1, stride=1, padding=0)
          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
    )
    (layer2): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
        (downsample): Sequential(
          (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
    )
    (layer3): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
        (downsample): Sequential(
          (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
    )
    (layer4): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
        (downsample): Sequential(
          (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
    )
    (attnpool): AttentionPool2d(
      (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
      (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
      (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
      (c_proj): Linear(in_features=2048, out_features=1024, bias=True)
    )
  )
  (transformer): Transformer(
    (resblocks): Sequential(
      (0): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (6): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (7): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (8): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (9): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (10): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (11): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (token_embedding): Embedding(49408, 512)
  (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
Turning off gradients in both the image and the text encoder
Loading evaluator: Classification
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/Caltech/1/2/tensorboard)
epoch [1/200] batch [1/3] time 1.025 (1.025) data 0.288 (0.288) loss 3.2539 (3.2539) acc 46.8750 (46.8750) lr 1.0000e-05 eta 0:10:13
epoch [1/200] batch [2/3] time 0.717 (0.871) data 0.000 (0.144) loss 3.1738 (3.2139) acc 34.3750 (40.6250) lr 1.0000e-05 eta 0:08:40
epoch [1/200] batch [3/3] time 0.717 (0.819) data 0.000 (0.096) loss 2.6719 (3.0332) acc 56.2500 (45.8333) lr 2.0000e-03 eta 0:08:09
epoch [2/200] batch [1/3] time 0.973 (0.973) data 0.258 (0.258) loss 2.9824 (2.9824) acc 50.0000 (50.0000) lr 2.0000e-03 eta 0:09:39
epoch [2/200] batch [2/3] time 0.717 (0.845) data 0.000 (0.129) loss 2.0098 (2.4961) acc 53.1250 (51.5625) lr 2.0000e-03 eta 0:08:22
epoch [2/200] batch [3/3] time 0.718 (0.803) data 0.000 (0.086) loss 1.6016 (2.1979) acc 62.5000 (55.2083) lr 1.9999e-03 eta 0:07:56
epoch [3/200] batch [1/3] time 0.973 (0.973) data 0.258 (0.258) loss 2.5176 (2.5176) acc 46.8750 (46.8750) lr 1.9999e-03 eta 0:09:36
epoch [3/200] batch [2/3] time 0.718 (0.845) data 0.000 (0.129) loss 1.3408 (1.9292) acc 68.7500 (57.8125) lr 1.9999e-03 eta 0:08:20
epoch [3/200] batch [3/3] time 0.718 (0.803) data 0.000 (0.086) loss 1.8848 (1.9144) acc 53.1250 (56.2500) lr 1.9995e-03 eta 0:07:54
epoch [4/200] batch [1/3] time 0.973 (0.973) data 0.259 (0.259) loss 1.5508 (1.5508) acc 68.7500 (68.7500) lr 1.9995e-03 eta 0:09:34
epoch [4/200] batch [2/3] time 0.718 (0.846) data 0.000 (0.130) loss 1.6201 (1.5854) acc 59.3750 (64.0625) lr 1.9995e-03 eta 0:08:18
epoch [4/200] batch [3/3] time 0.717 (0.803) data 0.000 (0.086) loss 1.7578 (1.6429) acc 53.1250 (60.4167) lr 1.9989e-03 eta 0:07:52
epoch [5/200] batch [1/3] time 0.989 (0.989) data 0.274 (0.274) loss 1.2246 (1.2246) acc 65.6250 (65.6250) lr 1.9989e-03 eta 0:09:40
epoch [5/200] batch [2/3] time 0.718 (0.853) data 0.000 (0.137) loss 1.2373 (1.2310) acc 59.3750 (62.5000) lr 1.9989e-03 eta 0:08:20
epoch [5/200] batch [3/3] time 0.718 (0.808) data 0.000 (0.091) loss 1.0244 (1.1621) acc 78.1250 (67.7083) lr 1.9980e-03 eta 0:07:52
epoch [6/200] batch [1/3] time 0.992 (0.992) data 0.278 (0.278) loss 1.1006 (1.1006) acc 75.0000 (75.0000) lr 1.9980e-03 eta 0:09:39
epoch [6/200] batch [2/3] time 0.723 (0.857) data 0.000 (0.139) loss 1.2725 (1.1865) acc 65.6250 (70.3125) lr 1.9980e-03 eta 0:08:19
epoch [6/200] batch [3/3] time 0.719 (0.811) data 0.000 (0.093) loss 0.9111 (1.0947) acc 78.1250 (72.9167) lr 1.9969e-03 eta 0:07:52
epoch [7/200] batch [1/3] time 0.990 (0.990) data 0.276 (0.276) loss 1.0088 (1.0088) acc 71.8750 (71.8750) lr 1.9969e-03 eta 0:09:35
epoch [7/200] batch [2/3] time 0.720 (0.855) data 0.000 (0.138) loss 0.8325 (0.9207) acc 78.1250 (75.0000) lr 1.9969e-03 eta 0:08:15
epoch [7/200] batch [3/3] time 0.719 (0.810) data 0.000 (0.092) loss 1.5732 (1.1382) acc 59.3750 (69.7917) lr 1.9956e-03 eta 0:07:48
epoch [8/200] batch [1/3] time 0.992 (0.992) data 0.278 (0.278) loss 1.1172 (1.1172) acc 71.8750 (71.8750) lr 1.9956e-03 eta 0:09:33
epoch [8/200] batch [2/3] time 0.720 (0.856) data 0.000 (0.139) loss 0.7666 (0.9419) acc 71.8750 (71.8750) lr 1.9956e-03 eta 0:08:13
epoch [8/200] batch [3/3] time 0.719 (0.810) data 0.000 (0.093) loss 1.3018 (1.0618) acc 71.8750 (71.8750) lr 1.9940e-03 eta 0:07:46
epoch [9/200] batch [1/3] time 0.980 (0.980) data 0.266 (0.266) loss 0.9688 (0.9688) acc 75.0000 (75.0000) lr 1.9940e-03 eta 0:09:23
epoch [9/200] batch [2/3] time 0.720 (0.850) data 0.000 (0.133) loss 0.9072 (0.9380) acc 78.1250 (76.5625) lr 1.9940e-03 eta 0:08:07
epoch [9/200] batch [3/3] time 0.719 (0.806) data 0.000 (0.089) loss 1.2871 (1.0544) acc 59.3750 (70.8333) lr 1.9921e-03 eta 0:07:42
epoch [10/200] batch [1/3] time 0.985 (0.985) data 0.269 (0.269) loss 0.6431 (0.6431) acc 78.1250 (78.1250) lr 1.9921e-03 eta 0:09:23
epoch [10/200] batch [2/3] time 0.720 (0.852) data 0.000 (0.135) loss 1.0010 (0.8220) acc 71.8750 (75.0000) lr 1.9921e-03 eta 0:08:06
epoch [10/200] batch [3/3] time 0.719 (0.808) data 0.000 (0.090) loss 1.2666 (0.9702) acc 71.8750 (73.9583) lr 1.9900e-03 eta 0:07:40
epoch [11/200] batch [1/3] time 0.974 (0.974) data 0.259 (0.259) loss 1.2646 (1.2646) acc 65.6250 (65.6250) lr 1.9900e-03 eta 0:09:14
epoch [11/200] batch [2/3] time 0.720 (0.847) data 0.000 (0.129) loss 0.8486 (1.0566) acc 81.2500 (73.4375) lr 1.9900e-03 eta 0:08:01
epoch [11/200] batch [3/3] time 0.719 (0.804) data 0.000 (0.086) loss 0.8164 (0.9766) acc 78.1250 (75.0000) lr 1.9877e-03 eta 0:07:36
epoch [12/200] batch [1/3] time 0.991 (0.991) data 0.275 (0.275) loss 1.1299 (1.1299) acc 71.8750 (71.8750) lr 1.9877e-03 eta 0:09:20
epoch [12/200] batch [2/3] time 0.720 (0.855) data 0.000 (0.138) loss 1.2822 (1.2061) acc 71.8750 (71.8750) lr 1.9877e-03 eta 0:08:03
epoch [12/200] batch [3/3] time 0.719 (0.810) data 0.000 (0.092) loss 1.0947 (1.1689) acc 65.6250 (69.7917) lr 1.9851e-03 eta 0:07:36
epoch [13/200] batch [1/3] time 0.974 (0.974) data 0.258 (0.258) loss 1.0107 (1.0107) acc 71.8750 (71.8750) lr 1.9851e-03 eta 0:09:08
epoch [13/200] batch [2/3] time 0.719 (0.846) data 0.000 (0.129) loss 0.9800 (0.9954) acc 75.0000 (73.4375) lr 1.9851e-03 eta 0:07:55
epoch [13/200] batch [3/3] time 0.719 (0.804) data 0.000 (0.086) loss 0.9375 (0.9761) acc 71.8750 (72.9167) lr 1.9823e-03 eta 0:07:30
epoch [14/200] batch [1/3] time 0.977 (0.977) data 0.262 (0.262) loss 1.2285 (1.2285) acc 68.7500 (68.7500) lr 1.9823e-03 eta 0:09:07
epoch [14/200] batch [2/3] time 0.718 (0.848) data 0.000 (0.131) loss 0.8003 (1.0144) acc 71.8750 (70.3125) lr 1.9823e-03 eta 0:07:53
epoch [14/200] batch [3/3] time 0.720 (0.805) data 0.000 (0.087) loss 0.6821 (0.9036) acc 84.3750 (75.0000) lr 1.9792e-03 eta 0:07:29
epoch [15/200] batch [1/3] time 0.991 (0.991) data 0.276 (0.276) loss 1.0625 (1.0625) acc 68.7500 (68.7500) lr 1.9792e-03 eta 0:09:12
epoch [15/200] batch [2/3] time 0.718 (0.855) data 0.000 (0.138) loss 0.5586 (0.8105) acc 84.3750 (76.5625) lr 1.9792e-03 eta 0:07:55
epoch [15/200] batch [3/3] time 0.720 (0.810) data 0.000 (0.092) loss 1.0381 (0.8864) acc 78.1250 (77.0833) lr 1.9759e-03 eta 0:07:29
epoch [16/200] batch [1/3] time 0.984 (0.984) data 0.266 (0.266) loss 0.7422 (0.7422) acc 87.5000 (87.5000) lr 1.9759e-03 eta 0:09:05
epoch [16/200] batch [2/3] time 0.720 (0.852) data 0.001 (0.133) loss 0.7354 (0.7388) acc 75.0000 (81.2500) lr 1.9759e-03 eta 0:07:51
epoch [16/200] batch [3/3] time 0.718 (0.808) data 0.000 (0.089) loss 0.5254 (0.6676) acc 87.5000 (83.3333) lr 1.9724e-03 eta 0:07:25
epoch [17/200] batch [1/3] time 0.984 (0.984) data 0.268 (0.268) loss 0.9106 (0.9106) acc 71.8750 (71.8750) lr 1.9724e-03 eta 0:09:02
epoch [17/200] batch [2/3] time 0.717 (0.851) data 0.000 (0.134) loss 0.9492 (0.9299) acc 68.7500 (70.3125) lr 1.9724e-03 eta 0:07:48
epoch [17/200] batch [3/3] time 0.720 (0.807) data 0.000 (0.090) loss 1.0635 (0.9744) acc 71.8750 (70.8333) lr 1.9686e-03 eta 0:07:23
epoch [18/200] batch [1/3] time 0.976 (0.976) data 0.260 (0.260) loss 0.9141 (0.9141) acc 75.0000 (75.0000) lr 1.9686e-03 eta 0:08:54
epoch [18/200] batch [2/3] time 0.719 (0.848) data 0.000 (0.130) loss 0.9858 (0.9500) acc 68.7500 (71.8750) lr 1.9686e-03 eta 0:07:43
epoch [18/200] batch [3/3] time 0.723 (0.806) data 0.000 (0.087) loss 0.7402 (0.8800) acc 81.2500 (75.0000) lr 1.9646e-03 eta 0:07:20
epoch [19/200] batch [1/3] time 1.004 (1.004) data 0.266 (0.266) loss 0.8843 (0.8843) acc 71.8750 (71.8750) lr 1.9646e-03 eta 0:09:07
epoch [19/200] batch [2/3] time 0.738 (0.871) data 0.000 (0.133) loss 0.8379 (0.8611) acc 84.3750 (78.1250) lr 1.9646e-03 eta 0:07:53
epoch [19/200] batch [3/3] time 0.736 (0.826) data 0.000 (0.089) loss 0.6348 (0.7856) acc 87.5000 (81.2500) lr 1.9603e-03 eta 0:07:28
epoch [20/200] batch [1/3] time 0.990 (0.990) data 0.261 (0.261) loss 1.2422 (1.2422) acc 65.6250 (65.6250) lr 1.9603e-03 eta 0:08:56
epoch [20/200] batch [2/3] time 0.731 (0.861) data 0.000 (0.131) loss 0.8740 (1.0581) acc 81.2500 (73.4375) lr 1.9603e-03 eta 0:07:45
epoch [20/200] batch [3/3] time 0.732 (0.818) data 0.000 (0.087) loss 0.4187 (0.8450) acc 84.3750 (77.0833) lr 1.9558e-03 eta 0:07:21
epoch [21/200] batch [1/3] time 1.001 (1.001) data 0.274 (0.274) loss 0.3762 (0.3762) acc 90.6250 (90.6250) lr 1.9558e-03 eta 0:08:59
epoch [21/200] batch [2/3] time 0.726 (0.864) data 0.000 (0.137) loss 0.8774 (0.6268) acc 78.1250 (84.3750) lr 1.9558e-03 eta 0:07:44
epoch [21/200] batch [3/3] time 0.725 (0.817) data 0.000 (0.092) loss 0.7891 (0.6809) acc 84.3750 (84.3750) lr 1.9511e-03 eta 0:07:18
epoch [22/200] batch [1/3] time 0.992 (0.992) data 0.276 (0.276) loss 0.6704 (0.6704) acc 78.1250 (78.1250) lr 1.9511e-03 eta 0:08:51
epoch [22/200] batch [2/3] time 0.718 (0.855) data 0.000 (0.138) loss 0.5479 (0.6091) acc 81.2500 (79.6875) lr 1.9511e-03 eta 0:07:37
epoch [22/200] batch [3/3] time 0.719 (0.810) data 0.000 (0.092) loss 0.9478 (0.7220) acc 75.0000 (78.1250) lr 1.9461e-03 eta 0:07:12
epoch [23/200] batch [1/3] time 0.976 (0.976) data 0.267 (0.267) loss 1.1201 (1.1201) acc 75.0000 (75.0000) lr 1.9461e-03 eta 0:08:40
epoch [23/200] batch [2/3] time 0.713 (0.845) data 0.000 (0.134) loss 0.5581 (0.8391) acc 81.2500 (78.1250) lr 1.9461e-03 eta 0:07:29
epoch [23/200] batch [3/3] time 0.712 (0.801) data 0.000 (0.089) loss 0.8477 (0.8420) acc 75.0000 (77.0833) lr 1.9409e-03 eta 0:07:05
epoch [24/200] batch [1/3] time 0.985 (0.985) data 0.273 (0.273) loss 1.1699 (1.1699) acc 68.7500 (68.7500) lr 1.9409e-03 eta 0:08:42
epoch [24/200] batch [2/3] time 0.712 (0.848) data 0.000 (0.137) loss 1.1699 (1.1699) acc 71.8750 (70.3125) lr 1.9409e-03 eta 0:07:28
epoch [24/200] batch [3/3] time 0.712 (0.803) data 0.000 (0.091) loss 0.7734 (1.0378) acc 81.2500 (73.9583) lr 1.9354e-03 eta 0:07:04
epoch [25/200] batch [1/3] time 0.988 (0.988) data 0.275 (0.275) loss 0.9922 (0.9922) acc 71.8750 (71.8750) lr 1.9354e-03 eta 0:08:40
epoch [25/200] batch [2/3] time 0.712 (0.850) data 0.000 (0.138) loss 0.9229 (0.9575) acc 68.7500 (70.3125) lr 1.9354e-03 eta 0:07:27
epoch [25/200] batch [3/3] time 0.712 (0.804) data 0.000 (0.092) loss 0.9526 (0.9559) acc 71.8750 (70.8333) lr 1.9298e-03 eta 0:07:02
epoch [26/200] batch [1/3] time 0.971 (0.971) data 0.260 (0.260) loss 0.4744 (0.4744) acc 87.5000 (87.5000) lr 1.9298e-03 eta 0:08:28
epoch [26/200] batch [2/3] time 0.710 (0.841) data 0.000 (0.130) loss 0.7148 (0.5946) acc 84.3750 (85.9375) lr 1.9298e-03 eta 0:07:19
epoch [26/200] batch [3/3] time 0.712 (0.798) data 0.000 (0.087) loss 0.8740 (0.6877) acc 75.0000 (82.2917) lr 1.9239e-03 eta 0:06:56
epoch [27/200] batch [1/3] time 0.980 (0.980) data 0.268 (0.268) loss 1.0791 (1.0791) acc 81.2500 (81.2500) lr 1.9239e-03 eta 0:08:30
epoch [27/200] batch [2/3] time 0.711 (0.846) data 0.000 (0.134) loss 0.9204 (0.9998) acc 78.1250 (79.6875) lr 1.9239e-03 eta 0:07:19
epoch [27/200] batch [3/3] time 0.712 (0.801) data 0.000 (0.089) loss 0.9814 (0.9937) acc 75.0000 (78.1250) lr 1.9178e-03 eta 0:06:55
epoch [28/200] batch [1/3] time 0.987 (0.987) data 0.277 (0.277) loss 1.0293 (1.0293) acc 68.7500 (68.7500) lr 1.9178e-03 eta 0:08:31
epoch [28/200] batch [2/3] time 0.712 (0.850) data 0.000 (0.138) loss 0.4250 (0.7272) acc 93.7500 (81.2500) lr 1.9178e-03 eta 0:07:19
epoch [28/200] batch [3/3] time 0.712 (0.804) data 0.000 (0.092) loss 0.8418 (0.7654) acc 78.1250 (80.2083) lr 1.9114e-03 eta 0:06:54
epoch [29/200] batch [1/3] time 0.988 (0.988) data 0.276 (0.276) loss 0.6279 (0.6279) acc 81.2500 (81.2500) lr 1.9114e-03 eta 0:08:28
epoch [29/200] batch [2/3] time 0.711 (0.850) data 0.000 (0.138) loss 0.9102 (0.7690) acc 68.7500 (75.0000) lr 1.9114e-03 eta 0:07:16
epoch [29/200] batch [3/3] time 0.712 (0.804) data 0.000 (0.092) loss 0.7959 (0.7780) acc 81.2500 (77.0833) lr 1.9048e-03 eta 0:06:52
epoch [30/200] batch [1/3] time 0.986 (0.986) data 0.275 (0.275) loss 0.9419 (0.9419) acc 78.1250 (78.1250) lr 1.9048e-03 eta 0:08:25
epoch [30/200] batch [2/3] time 0.712 (0.849) data 0.000 (0.137) loss 1.1865 (1.0642) acc 68.7500 (73.4375) lr 1.9048e-03 eta 0:07:13
epoch [30/200] batch [3/3] time 0.712 (0.803) data 0.000 (0.092) loss 0.9419 (1.0234) acc 75.0000 (73.9583) lr 1.8980e-03 eta 0:06:49
epoch [31/200] batch [1/3] time 0.984 (0.984) data 0.275 (0.275) loss 0.5962 (0.5962) acc 84.3750 (84.3750) lr 1.8980e-03 eta 0:08:20
epoch [31/200] batch [2/3] time 0.712 (0.848) data 0.000 (0.138) loss 0.6157 (0.6060) acc 81.2500 (82.8125) lr 1.8980e-03 eta 0:07:10
epoch [31/200] batch [3/3] time 0.712 (0.803) data 0.000 (0.092) loss 0.6392 (0.6170) acc 87.5000 (84.3750) lr 1.8910e-03 eta 0:06:47
epoch [32/200] batch [1/3] time 0.980 (0.980) data 0.269 (0.269) loss 0.9355 (0.9355) acc 81.2500 (81.2500) lr 1.8910e-03 eta 0:08:15
epoch [32/200] batch [2/3] time 0.712 (0.846) data 0.000 (0.135) loss 0.6621 (0.7988) acc 78.1250 (79.6875) lr 1.8910e-03 eta 0:07:07
epoch [32/200] batch [3/3] time 0.712 (0.801) data 0.000 (0.090) loss 0.8950 (0.8309) acc 81.2500 (80.2083) lr 1.8838e-03 eta 0:06:43
epoch [33/200] batch [1/3] time 0.983 (0.983) data 0.273 (0.273) loss 0.4819 (0.4819) acc 87.5000 (87.5000) lr 1.8838e-03 eta 0:08:14
epoch [33/200] batch [2/3] time 0.711 (0.847) data 0.000 (0.137) loss 0.5518 (0.5168) acc 81.2500 (84.3750) lr 1.8838e-03 eta 0:07:05
epoch [33/200] batch [3/3] time 0.714 (0.803) data 0.000 (0.091) loss 1.2207 (0.7515) acc 71.8750 (80.2083) lr 1.8763e-03 eta 0:06:42
epoch [34/200] batch [1/3] time 0.984 (0.984) data 0.275 (0.275) loss 0.8999 (0.8999) acc 68.7500 (68.7500) lr 1.8763e-03 eta 0:08:12
epoch [34/200] batch [2/3] time 0.710 (0.847) data 0.000 (0.138) loss 0.8472 (0.8735) acc 78.1250 (73.4375) lr 1.8763e-03 eta 0:07:02
epoch [34/200] batch [3/3] time 0.712 (0.802) data 0.000 (0.092) loss 0.5713 (0.7728) acc 81.2500 (76.0417) lr 1.8686e-03 eta 0:06:39
epoch [35/200] batch [1/3] time 0.966 (0.966) data 0.257 (0.257) loss 1.1514 (1.1514) acc 78.1250 (78.1250) lr 1.8686e-03 eta 0:08:00
epoch [35/200] batch [2/3] time 0.712 (0.839) data 0.000 (0.128) loss 0.6602 (0.9058) acc 84.3750 (81.2500) lr 1.8686e-03 eta 0:06:56
epoch [35/200] batch [3/3] time 0.711 (0.796) data 0.000 (0.086) loss 0.5117 (0.7744) acc 81.2500 (81.2500) lr 1.8607e-03 eta 0:06:34
epoch [36/200] batch [1/3] time 0.971 (0.971) data 0.258 (0.258) loss 1.0049 (1.0049) acc 68.7500 (68.7500) lr 1.8607e-03 eta 0:07:59
epoch [36/200] batch [2/3] time 0.712 (0.841) data 0.000 (0.129) loss 0.7026 (0.8538) acc 81.2500 (75.0000) lr 1.8607e-03 eta 0:06:54
epoch [36/200] batch [3/3] time 0.712 (0.798) data 0.000 (0.086) loss 0.5049 (0.7375) acc 81.2500 (77.0833) lr 1.8526e-03 eta 0:06:32
epoch [37/200] batch [1/3] time 0.970 (0.970) data 0.257 (0.257) loss 0.6689 (0.6689) acc 84.3750 (84.3750) lr 1.8526e-03 eta 0:07:56
epoch [37/200] batch [2/3] time 0.712 (0.841) data 0.000 (0.128) loss 1.0693 (0.8691) acc 75.0000 (79.6875) lr 1.8526e-03 eta 0:06:51
epoch [37/200] batch [3/3] time 0.712 (0.798) data 0.000 (0.086) loss 0.7222 (0.8201) acc 81.2500 (80.2083) lr 1.8443e-03 eta 0:06:30
epoch [38/200] batch [1/3] time 0.970 (0.970) data 0.258 (0.258) loss 0.9697 (0.9697) acc 71.8750 (71.8750) lr 1.8443e-03 eta 0:07:53
epoch [38/200] batch [2/3] time 0.712 (0.841) data 0.000 (0.129) loss 1.0244 (0.9971) acc 68.7500 (70.3125) lr 1.8443e-03 eta 0:06:49
epoch [38/200] batch [3/3] time 0.712 (0.798) data 0.000 (0.086) loss 0.5459 (0.8467) acc 84.3750 (75.0000) lr 1.8358e-03 eta 0:06:27
epoch [39/200] batch [1/3] time 0.986 (0.986) data 0.274 (0.274) loss 0.6069 (0.6069) acc 87.5000 (87.5000) lr 1.8358e-03 eta 0:07:58
epoch [39/200] batch [2/3] time 0.714 (0.850) data 0.000 (0.137) loss 0.4900 (0.5485) acc 87.5000 (87.5000) lr 1.8358e-03 eta 0:06:51
epoch [39/200] batch [3/3] time 0.712 (0.804) data 0.000 (0.092) loss 0.2869 (0.4613) acc 87.5000 (87.5000) lr 1.8271e-03 eta 0:06:28
epoch [40/200] batch [1/3] time 0.969 (0.969) data 0.258 (0.258) loss 0.4050 (0.4050) acc 87.5000 (87.5000) lr 1.8271e-03 eta 0:07:47
epoch [40/200] batch [2/3] time 0.712 (0.841) data 0.000 (0.129) loss 0.7773 (0.5912) acc 81.2500 (84.3750) lr 1.8271e-03 eta 0:06:44
epoch [40/200] batch [3/3] time 0.712 (0.798) data 0.000 (0.086) loss 0.5449 (0.5758) acc 81.2500 (83.3333) lr 1.8181e-03 eta 0:06:22
epoch [41/200] batch [1/3] time 0.985 (0.985) data 0.274 (0.274) loss 0.7344 (0.7344) acc 84.3750 (84.3750) lr 1.8181e-03 eta 0:07:52
epoch [41/200] batch [2/3] time 0.712 (0.849) data 0.000 (0.137) loss 0.3992 (0.5668) acc 90.6250 (87.5000) lr 1.8181e-03 eta 0:06:45
epoch [41/200] batch [3/3] time 0.712 (0.803) data 0.000 (0.091) loss 0.8945 (0.6760) acc 75.0000 (83.3333) lr 1.8090e-03 eta 0:06:23
epoch [42/200] batch [1/3] time 0.985 (0.985) data 0.274 (0.274) loss 0.3564 (0.3564) acc 93.7500 (93.7500) lr 1.8090e-03 eta 0:07:48
epoch [42/200] batch [2/3] time 0.712 (0.848) data 0.000 (0.137) loss 0.8652 (0.6108) acc 81.2500 (87.5000) lr 1.8090e-03 eta 0:06:42
epoch [42/200] batch [3/3] time 0.713 (0.803) data 0.000 (0.091) loss 0.5942 (0.6053) acc 87.5000 (87.5000) lr 1.7997e-03 eta 0:06:20
epoch [43/200] batch [1/3] time 0.976 (0.976) data 0.265 (0.265) loss 0.9673 (0.9673) acc 81.2500 (81.2500) lr 1.7997e-03 eta 0:07:41
epoch [43/200] batch [2/3] time 0.712 (0.844) data 0.000 (0.132) loss 0.9971 (0.9822) acc 65.6250 (73.4375) lr 1.7997e-03 eta 0:06:38
epoch [43/200] batch [3/3] time 0.716 (0.802) data 0.000 (0.088) loss 0.4290 (0.7978) acc 93.7500 (80.2083) lr 1.7902e-03 eta 0:06:17
epoch [44/200] batch [1/3] time 0.978 (0.978) data 0.264 (0.264) loss 1.0586 (1.0586) acc 75.0000 (75.0000) lr 1.7902e-03 eta 0:07:39
epoch [44/200] batch [2/3] time 0.712 (0.845) data 0.000 (0.132) loss 0.5493 (0.8040) acc 87.5000 (81.2500) lr 1.7902e-03 eta 0:06:36
epoch [44/200] batch [3/3] time 0.712 (0.801) data 0.000 (0.088) loss 0.3928 (0.6669) acc 84.3750 (82.2917) lr 1.7804e-03 eta 0:06:14
epoch [45/200] batch [1/3] time 0.967 (0.967) data 0.257 (0.257) loss 0.5508 (0.5508) acc 81.2500 (81.2500) lr 1.7804e-03 eta 0:07:31
epoch [45/200] batch [2/3] time 0.712 (0.839) data 0.000 (0.129) loss 0.6089 (0.5798) acc 84.3750 (82.8125) lr 1.7804e-03 eta 0:06:31
epoch [45/200] batch [3/3] time 0.712 (0.797) data 0.000 (0.086) loss 0.4395 (0.5330) acc 90.6250 (85.4167) lr 1.7705e-03 eta 0:06:10
epoch [46/200] batch [1/3] time 0.974 (0.974) data 0.265 (0.265) loss 0.4167 (0.4167) acc 93.7500 (93.7500) lr 1.7705e-03 eta 0:07:31
epoch [46/200] batch [2/3] time 0.712 (0.843) data 0.000 (0.132) loss 0.7007 (0.5587) acc 87.5000 (90.6250) lr 1.7705e-03 eta 0:06:30
epoch [46/200] batch [3/3] time 0.712 (0.799) data 0.000 (0.088) loss 0.5757 (0.5644) acc 87.5000 (89.5833) lr 1.7604e-03 eta 0:06:09
epoch [47/200] batch [1/3] time 0.968 (0.968) data 0.259 (0.259) loss 0.7930 (0.7930) acc 78.1250 (78.1250) lr 1.7604e-03 eta 0:07:26
epoch [47/200] batch [2/3] time 0.712 (0.840) data 0.000 (0.129) loss 0.8413 (0.8171) acc 78.1250 (78.1250) lr 1.7604e-03 eta 0:06:26
epoch [47/200] batch [3/3] time 0.713 (0.798) data 0.000 (0.086) loss 0.3835 (0.6726) acc 93.7500 (83.3333) lr 1.7501e-03 eta 0:06:06
epoch [48/200] batch [1/3] time 0.970 (0.970) data 0.257 (0.257) loss 0.8936 (0.8936) acc 75.0000 (75.0000) lr 1.7501e-03 eta 0:07:24
epoch [48/200] batch [2/3] time 0.712 (0.841) data 0.000 (0.128) loss 0.4519 (0.6727) acc 90.6250 (82.8125) lr 1.7501e-03 eta 0:06:24
epoch [48/200] batch [3/3] time 0.716 (0.799) data 0.000 (0.086) loss 0.3921 (0.5792) acc 90.6250 (85.4167) lr 1.7396e-03 eta 0:06:04
epoch [49/200] batch [1/3] time 0.971 (0.971) data 0.259 (0.259) loss 0.6675 (0.6675) acc 87.5000 (87.5000) lr 1.7396e-03 eta 0:07:21
epoch [49/200] batch [2/3] time 0.712 (0.841) data 0.000 (0.130) loss 0.2000 (0.4337) acc 93.7500 (90.6250) lr 1.7396e-03 eta 0:06:22
epoch [49/200] batch [3/3] time 0.712 (0.798) data 0.000 (0.087) loss 0.2903 (0.3859) acc 93.7500 (91.6667) lr 1.7290e-03 eta 0:06:01
epoch [50/200] batch [1/3] time 0.986 (0.986) data 0.274 (0.274) loss 0.4871 (0.4871) acc 84.3750 (84.3750) lr 1.7290e-03 eta 0:07:25
epoch [50/200] batch [2/3] time 0.712 (0.849) data 0.000 (0.137) loss 0.6099 (0.5485) acc 87.5000 (85.9375) lr 1.7290e-03 eta 0:06:22
epoch [50/200] batch [3/3] time 0.717 (0.805) data 0.000 (0.091) loss 0.5562 (0.5510) acc 90.6250 (87.5000) lr 1.7181e-03 eta 0:06:02
epoch [51/200] batch [1/3] time 0.989 (0.989) data 0.278 (0.278) loss 0.3738 (0.3738) acc 93.7500 (93.7500) lr 1.7181e-03 eta 0:07:24
epoch [51/200] batch [2/3] time 0.712 (0.851) data 0.000 (0.139) loss 0.6812 (0.5275) acc 81.2500 (87.5000) lr 1.7181e-03 eta 0:06:21
epoch [51/200] batch [3/3] time 0.712 (0.805) data 0.000 (0.093) loss 0.6729 (0.5759) acc 81.2500 (85.4167) lr 1.7071e-03 eta 0:05:59
epoch [52/200] batch [1/3] time 0.988 (0.988) data 0.275 (0.275) loss 0.7925 (0.7925) acc 87.5000 (87.5000) lr 1.7071e-03 eta 0:07:20
epoch [52/200] batch [2/3] time 0.716 (0.852) data 0.000 (0.138) loss 0.6934 (0.7429) acc 84.3750 (85.9375) lr 1.7071e-03 eta 0:06:19
epoch [52/200] batch [3/3] time 0.712 (0.805) data 0.000 (0.092) loss 0.6460 (0.7106) acc 90.6250 (87.5000) lr 1.6959e-03 eta 0:05:57
epoch [53/200] batch [1/3] time 0.989 (0.989) data 0.276 (0.276) loss 0.4629 (0.4629) acc 87.5000 (87.5000) lr 1.6959e-03 eta 0:07:18
epoch [53/200] batch [2/3] time 0.712 (0.851) data 0.000 (0.138) loss 0.5171 (0.4900) acc 87.5000 (87.5000) lr 1.6959e-03 eta 0:06:16
epoch [53/200] batch [3/3] time 0.712 (0.805) data 0.000 (0.092) loss 0.5352 (0.5050) acc 90.6250 (88.5417) lr 1.6845e-03 eta 0:05:54
epoch [54/200] batch [1/3] time 0.977 (0.977) data 0.265 (0.265) loss 0.5083 (0.5083) acc 87.5000 (87.5000) lr 1.6845e-03 eta 0:07:09
epoch [54/200] batch [2/3] time 0.712 (0.845) data 0.000 (0.132) loss 0.2793 (0.3938) acc 93.7500 (90.6250) lr 1.6845e-03 eta 0:06:10
epoch [54/200] batch [3/3] time 0.713 (0.801) data 0.000 (0.088) loss 0.2468 (0.3448) acc 90.6250 (90.6250) lr 1.6730e-03 eta 0:05:50
epoch [55/200] batch [1/3] time 0.984 (0.984) data 0.273 (0.273) loss 0.6602 (0.6602) acc 84.3750 (84.3750) lr 1.6730e-03 eta 0:07:09
epoch [55/200] batch [2/3] time 0.715 (0.849) data 0.000 (0.136) loss 0.7300 (0.6951) acc 84.3750 (84.3750) lr 1.6730e-03 eta 0:06:10
epoch [55/200] batch [3/3] time 0.712 (0.804) data 0.000 (0.091) loss 0.4980 (0.6294) acc 87.5000 (85.4167) lr 1.6613e-03 eta 0:05:49
epoch [56/200] batch [1/3] time 0.979 (0.979) data 0.266 (0.266) loss 0.3308 (0.3308) acc 93.7500 (93.7500) lr 1.6613e-03 eta 0:07:04
epoch [56/200] batch [2/3] time 0.712 (0.845) data 0.000 (0.133) loss 0.3979 (0.3644) acc 93.7500 (93.7500) lr 1.6613e-03 eta 0:06:06
epoch [56/200] batch [3/3] time 0.712 (0.801) data 0.000 (0.089) loss 0.4307 (0.3865) acc 90.6250 (92.7083) lr 1.6494e-03 eta 0:05:45
epoch [57/200] batch [1/3] time 0.986 (0.986) data 0.275 (0.275) loss 0.8125 (0.8125) acc 81.2500 (81.2500) lr 1.6494e-03 eta 0:07:05
epoch [57/200] batch [2/3] time 0.712 (0.849) data 0.000 (0.138) loss 0.3560 (0.5842) acc 93.7500 (87.5000) lr 1.6494e-03 eta 0:06:05
epoch [57/200] batch [3/3] time 0.712 (0.804) data 0.000 (0.092) loss 0.6025 (0.5903) acc 84.3750 (86.4583) lr 1.6374e-03 eta 0:05:44
epoch [58/200] batch [1/3] time 0.978 (0.978) data 0.269 (0.269) loss 0.4021 (0.4021) acc 93.7500 (93.7500) lr 1.6374e-03 eta 0:06:58
epoch [58/200] batch [2/3] time 0.712 (0.845) data 0.000 (0.135) loss 0.6147 (0.5084) acc 84.3750 (89.0625) lr 1.6374e-03 eta 0:06:00
epoch [58/200] batch [3/3] time 0.712 (0.801) data 0.000 (0.090) loss 0.5664 (0.5278) acc 84.3750 (87.5000) lr 1.6252e-03 eta 0:05:41
epoch [59/200] batch [1/3] time 0.978 (0.978) data 0.269 (0.269) loss 0.5444 (0.5444) acc 78.1250 (78.1250) lr 1.6252e-03 eta 0:06:55
epoch [59/200] batch [2/3] time 0.712 (0.845) data 0.000 (0.134) loss 0.3416 (0.4430) acc 93.7500 (85.9375) lr 1.6252e-03 eta 0:05:58
epoch [59/200] batch [3/3] time 0.713 (0.801) data 0.000 (0.090) loss 0.8481 (0.5780) acc 78.1250 (83.3333) lr 1.6129e-03 eta 0:05:38
epoch [60/200] batch [1/3] time 0.985 (0.985) data 0.276 (0.276) loss 0.6270 (0.6270) acc 90.6250 (90.6250) lr 1.6129e-03 eta 0:06:55
epoch [60/200] batch [2/3] time 0.712 (0.848) data 0.000 (0.138) loss 0.3174 (0.4722) acc 96.8750 (93.7500) lr 1.6129e-03 eta 0:05:57
epoch [60/200] batch [3/3] time 0.714 (0.803) data 0.000 (0.092) loss 0.6128 (0.5190) acc 90.6250 (92.7083) lr 1.6004e-03 eta 0:05:37
epoch [61/200] batch [1/3] time 0.985 (0.985) data 0.275 (0.275) loss 0.6216 (0.6216) acc 87.5000 (87.5000) lr 1.6004e-03 eta 0:06:52
epoch [61/200] batch [2/3] time 0.712 (0.848) data 0.000 (0.138) loss 0.5811 (0.6013) acc 87.5000 (87.5000) lr 1.6004e-03 eta 0:05:54
epoch [61/200] batch [3/3] time 0.712 (0.803) data 0.000 (0.092) loss 0.2869 (0.4965) acc 90.6250 (88.5417) lr 1.5878e-03 eta 0:05:34
epoch [62/200] batch [1/3] time 0.978 (0.978) data 0.267 (0.267) loss 0.3596 (0.3596) acc 93.7500 (93.7500) lr 1.5878e-03 eta 0:06:46
epoch [62/200] batch [2/3] time 0.712 (0.845) data 0.000 (0.134) loss 0.5854 (0.4725) acc 81.2500 (87.5000) lr 1.5878e-03 eta 0:05:50
epoch [62/200] batch [3/3] time 0.712 (0.801) data 0.000 (0.089) loss 0.3662 (0.4371) acc 90.6250 (88.5417) lr 1.5750e-03 eta 0:05:31
epoch [63/200] batch [1/3] time 0.984 (0.984) data 0.273 (0.273) loss 0.4924 (0.4924) acc 87.5000 (87.5000) lr 1.5750e-03 eta 0:06:46
epoch [63/200] batch [2/3] time 0.713 (0.848) data 0.000 (0.137) loss 0.5723 (0.5323) acc 87.5000 (87.5000) lr 1.5750e-03 eta 0:05:49
epoch [63/200] batch [3/3] time 0.712 (0.803) data 0.000 (0.091) loss 0.5830 (0.5492) acc 84.3750 (86.4583) lr 1.5621e-03 eta 0:05:30
epoch [64/200] batch [1/3] time 0.987 (0.987) data 0.276 (0.276) loss 0.2159 (0.2159) acc 96.8750 (96.8750) lr 1.5621e-03 eta 0:06:44
epoch [64/200] batch [2/3] time 0.712 (0.849) data 0.000 (0.138) loss 0.6323 (0.4241) acc 84.3750 (90.6250) lr 1.5621e-03 eta 0:05:47
epoch [64/200] batch [3/3] time 0.712 (0.804) data 0.000 (0.092) loss 0.3296 (0.3926) acc 90.6250 (90.6250) lr 1.5490e-03 eta 0:05:27
epoch [65/200] batch [1/3] time 0.978 (0.978) data 0.266 (0.266) loss 0.3618 (0.3618) acc 90.6250 (90.6250) lr 1.5490e-03 eta 0:06:37
epoch [65/200] batch [2/3] time 0.712 (0.845) data 0.000 (0.133) loss 0.4807 (0.4213) acc 87.5000 (89.0625) lr 1.5490e-03 eta 0:05:43
epoch [65/200] batch [3/3] time 0.712 (0.801) data 0.000 (0.089) loss 0.6821 (0.5082) acc 87.5000 (88.5417) lr 1.5358e-03 eta 0:05:24
epoch [66/200] batch [1/3] time 0.980 (0.980) data 0.269 (0.269) loss 0.4790 (0.4790) acc 87.5000 (87.5000) lr 1.5358e-03 eta 0:06:36
epoch [66/200] batch [2/3] time 0.712 (0.846) data 0.000 (0.134) loss 0.3796 (0.4293) acc 87.5000 (87.5000) lr 1.5358e-03 eta 0:05:41
epoch [66/200] batch [3/3] time 0.712 (0.802) data 0.000 (0.090) loss 0.4658 (0.4415) acc 87.5000 (87.5000) lr 1.5225e-03 eta 0:05:22
epoch [67/200] batch [1/3] time 0.986 (0.986) data 0.273 (0.273) loss 0.4023 (0.4023) acc 96.8750 (96.8750) lr 1.5225e-03 eta 0:06:35
epoch [67/200] batch [2/3] time 0.713 (0.849) data 0.000 (0.137) loss 0.8472 (0.6248) acc 84.3750 (90.6250) lr 1.5225e-03 eta 0:05:39
epoch [67/200] batch [3/3] time 0.713 (0.804) data 0.000 (0.091) loss 0.5234 (0.5910) acc 87.5000 (89.5833) lr 1.5090e-03 eta 0:05:20
epoch [68/200] batch [1/3] time 0.971 (0.971) data 0.258 (0.258) loss 0.5835 (0.5835) acc 81.2500 (81.2500) lr 1.5090e-03 eta 0:06:26
epoch [68/200] batch [2/3] time 0.712 (0.841) data 0.000 (0.129) loss 0.5527 (0.5681) acc 87.5000 (84.3750) lr 1.5090e-03 eta 0:05:34
epoch [68/200] batch [3/3] time 0.711 (0.798) data 0.000 (0.086) loss 1.2373 (0.7912) acc 71.8750 (80.2083) lr 1.4955e-03 eta 0:05:16
epoch [69/200] batch [1/3] time 0.985 (0.985) data 0.275 (0.275) loss 0.2725 (0.2725) acc 96.8750 (96.8750) lr 1.4955e-03 eta 0:06:29
epoch [69/200] batch [2/3] time 0.712 (0.849) data 0.000 (0.137) loss 0.3906 (0.3315) acc 90.6250 (93.7500) lr 1.4955e-03 eta 0:05:34
epoch [69/200] batch [3/3] time 0.715 (0.804) data 0.000 (0.092) loss 0.3621 (0.3417) acc 87.5000 (91.6667) lr 1.4818e-03 eta 0:05:15
epoch [70/200] batch [1/3] time 0.981 (0.981) data 0.272 (0.272) loss 0.4258 (0.4258) acc 93.7500 (93.7500) lr 1.4818e-03 eta 0:06:24
epoch [70/200] batch [2/3] time 0.712 (0.846) data 0.000 (0.136) loss 0.9780 (0.7019) acc 84.3750 (89.0625) lr 1.4818e-03 eta 0:05:30
epoch [70/200] batch [3/3] time 0.713 (0.802) data 0.000 (0.091) loss 0.8618 (0.7552) acc 75.0000 (84.3750) lr 1.4679e-03 eta 0:05:12
epoch [71/200] batch [1/3] time 0.990 (0.990) data 0.281 (0.281) loss 0.2959 (0.2959) acc 93.7500 (93.7500) lr 1.4679e-03 eta 0:06:25
epoch [71/200] batch [2/3] time 0.712 (0.851) data 0.000 (0.141) loss 0.2969 (0.2964) acc 93.7500 (93.7500) lr 1.4679e-03 eta 0:05:30
epoch [71/200] batch [3/3] time 0.713 (0.805) data 0.000 (0.094) loss 0.3635 (0.3188) acc 93.7500 (93.7500) lr 1.4540e-03 eta 0:05:11
epoch [72/200] batch [1/3] time 0.978 (0.978) data 0.269 (0.269) loss 0.3450 (0.3450) acc 90.6250 (90.6250) lr 1.4540e-03 eta 0:06:17
epoch [72/200] batch [2/3] time 0.712 (0.845) data 0.000 (0.135) loss 0.4468 (0.3959) acc 90.6250 (90.6250) lr 1.4540e-03 eta 0:05:25
epoch [72/200] batch [3/3] time 0.714 (0.801) data 0.000 (0.090) loss 0.5615 (0.4511) acc 90.6250 (90.6250) lr 1.4399e-03 eta 0:05:07
epoch [73/200] batch [1/3] time 0.982 (0.982) data 0.274 (0.274) loss 0.3208 (0.3208) acc 90.6250 (90.6250) lr 1.4399e-03 eta 0:06:16
epoch [73/200] batch [2/3] time 0.712 (0.847) data 0.000 (0.137) loss 0.3542 (0.3375) acc 93.7500 (92.1875) lr 1.4399e-03 eta 0:05:23
epoch [73/200] batch [3/3] time 0.712 (0.802) data 0.000 (0.091) loss 0.7329 (0.4693) acc 78.1250 (87.5000) lr 1.4258e-03 eta 0:05:05
epoch [74/200] batch [1/3] time 0.985 (0.985) data 0.273 (0.273) loss 0.2175 (0.2175) acc 96.8750 (96.8750) lr 1.4258e-03 eta 0:06:14
epoch [74/200] batch [2/3] time 0.717 (0.851) data 0.000 (0.136) loss 0.5127 (0.3651) acc 87.5000 (92.1875) lr 1.4258e-03 eta 0:05:22
epoch [74/200] batch [3/3] time 0.712 (0.805) data 0.000 (0.091) loss 0.3035 (0.3446) acc 96.8750 (93.7500) lr 1.4115e-03 eta 0:05:04
epoch [75/200] batch [1/3] time 0.987 (0.987) data 0.276 (0.276) loss 0.4270 (0.4270) acc 93.7500 (93.7500) lr 1.4115e-03 eta 0:06:11
epoch [75/200] batch [2/3] time 0.712 (0.849) data 0.000 (0.138) loss 0.4543 (0.4407) acc 93.7500 (93.7500) lr 1.4115e-03 eta 0:05:19
epoch [75/200] batch [3/3] time 0.713 (0.804) data 0.000 (0.092) loss 0.6318 (0.5044) acc 84.3750 (90.6250) lr 1.3971e-03 eta 0:05:01
epoch [76/200] batch [1/3] time 0.974 (0.974) data 0.261 (0.261) loss 0.3347 (0.3347) acc 93.7500 (93.7500) lr 1.3971e-03 eta 0:06:04
epoch [76/200] batch [2/3] time 0.712 (0.843) data 0.000 (0.131) loss 0.4387 (0.3867) acc 84.3750 (89.0625) lr 1.3971e-03 eta 0:05:14
epoch [76/200] batch [3/3] time 0.712 (0.799) data 0.000 (0.087) loss 0.4294 (0.4010) acc 90.6250 (89.5833) lr 1.3827e-03 eta 0:04:57
epoch [77/200] batch [1/3] time 0.986 (0.986) data 0.275 (0.275) loss 0.5278 (0.5278) acc 87.5000 (87.5000) lr 1.3827e-03 eta 0:06:05
epoch [77/200] batch [2/3] time 0.712 (0.849) data 0.000 (0.137) loss 0.5044 (0.5161) acc 90.6250 (89.0625) lr 1.3827e-03 eta 0:05:14
epoch [77/200] batch [3/3] time 0.713 (0.803) data 0.000 (0.092) loss 0.4548 (0.4957) acc 90.6250 (89.5833) lr 1.3681e-03 eta 0:04:56
epoch [78/200] batch [1/3] time 0.985 (0.985) data 0.274 (0.274) loss 0.8184 (0.8184) acc 75.0000 (75.0000) lr 1.3681e-03 eta 0:06:02
epoch [78/200] batch [2/3] time 0.712 (0.848) data 0.000 (0.137) loss 0.4092 (0.6138) acc 93.7500 (84.3750) lr 1.3681e-03 eta 0:05:11
epoch [78/200] batch [3/3] time 0.712 (0.803) data 0.000 (0.091) loss 0.4963 (0.5746) acc 84.3750 (84.3750) lr 1.3535e-03 eta 0:04:53
epoch [79/200] batch [1/3] time 0.978 (0.978) data 0.267 (0.267) loss 0.3167 (0.3167) acc 87.5000 (87.5000) lr 1.3535e-03 eta 0:05:56
epoch [79/200] batch [2/3] time 0.712 (0.845) data 0.000 (0.133) loss 0.6890 (0.5028) acc 78.1250 (82.8125) lr 1.3535e-03 eta 0:05:07
epoch [79/200] batch [3/3] time 0.713 (0.801) data 0.000 (0.089) loss 0.2783 (0.4280) acc 90.6250 (85.4167) lr 1.3387e-03 eta 0:04:50
epoch [80/200] batch [1/3] time 0.986 (0.986) data 0.273 (0.273) loss 0.3755 (0.3755) acc 93.7500 (93.7500) lr 1.3387e-03 eta 0:05:56
epoch [80/200] batch [2/3] time 0.711 (0.849) data 0.000 (0.137) loss 0.5718 (0.4736) acc 87.5000 (90.6250) lr 1.3387e-03 eta 0:05:06
epoch [80/200] batch [3/3] time 0.712 (0.803) data 0.000 (0.091) loss 0.1892 (0.3788) acc 96.8750 (92.7083) lr 1.3239e-03 eta 0:04:49
epoch [81/200] batch [1/3] time 0.986 (0.986) data 0.275 (0.275) loss 0.1881 (0.1881) acc 96.8750 (96.8750) lr 1.3239e-03 eta 0:05:54
epoch [81/200] batch [2/3] time 0.712 (0.849) data 0.000 (0.137) loss 0.4312 (0.3096) acc 84.3750 (90.6250) lr 1.3239e-03 eta 0:05:03
epoch [81/200] batch [3/3] time 0.712 (0.804) data 0.000 (0.092) loss 0.5039 (0.3744) acc 90.6250 (90.6250) lr 1.3090e-03 eta 0:04:46
epoch [82/200] batch [1/3] time 0.970 (0.970) data 0.258 (0.258) loss 0.6445 (0.6445) acc 84.3750 (84.3750) lr 1.3090e-03 eta 0:05:45
epoch [82/200] batch [2/3] time 0.712 (0.841) data 0.000 (0.129) loss 0.1578 (0.4012) acc 100.0000 (92.1875) lr 1.3090e-03 eta 0:04:58
epoch [82/200] batch [3/3] time 0.712 (0.798) data 0.000 (0.086) loss 0.8823 (0.5616) acc 78.1250 (87.5000) lr 1.2940e-03 eta 0:04:42
epoch [83/200] batch [1/3] time 0.986 (0.986) data 0.275 (0.275) loss 0.6245 (0.6245) acc 84.3750 (84.3750) lr 1.2940e-03 eta 0:05:48
epoch [83/200] batch [2/3] time 0.712 (0.849) data 0.000 (0.138) loss 0.4819 (0.5532) acc 87.5000 (85.9375) lr 1.2940e-03 eta 0:04:58
epoch [83/200] batch [3/3] time 0.713 (0.804) data 0.000 (0.092) loss 0.0919 (0.3995) acc 96.8750 (89.5833) lr 1.2790e-03 eta 0:04:42
epoch [84/200] batch [1/3] time 0.983 (0.983) data 0.275 (0.275) loss 0.4517 (0.4517) acc 87.5000 (87.5000) lr 1.2790e-03 eta 0:05:44
epoch [84/200] batch [2/3] time 0.712 (0.848) data 0.000 (0.137) loss 0.3345 (0.3931) acc 93.7500 (90.6250) lr 1.2790e-03 eta 0:04:55
epoch [84/200] batch [3/3] time 0.712 (0.803) data 0.000 (0.092) loss 0.2866 (0.3576) acc 90.6250 (90.6250) lr 1.2639e-03 eta 0:04:39
epoch [85/200] batch [1/3] time 0.968 (0.968) data 0.260 (0.260) loss 0.7275 (0.7275) acc 84.3750 (84.3750) lr 1.2639e-03 eta 0:05:35
epoch [85/200] batch [2/3] time 0.713 (0.840) data 0.000 (0.130) loss 0.3884 (0.5580) acc 90.6250 (87.5000) lr 1.2639e-03 eta 0:04:50
epoch [85/200] batch [3/3] time 0.713 (0.798) data 0.000 (0.087) loss 0.5708 (0.5623) acc 84.3750 (86.4583) lr 1.2487e-03 eta 0:04:35
epoch [86/200] batch [1/3] time 0.968 (0.968) data 0.259 (0.259) loss 0.3843 (0.3843) acc 87.5000 (87.5000) lr 1.2487e-03 eta 0:05:32
epoch [86/200] batch [2/3] time 0.712 (0.840) data 0.000 (0.130) loss 0.5845 (0.4844) acc 87.5000 (87.5000) lr 1.2487e-03 eta 0:04:48
epoch [86/200] batch [3/3] time 0.716 (0.799) data 0.000 (0.087) loss 0.2959 (0.4215) acc 93.7500 (89.5833) lr 1.2334e-03 eta 0:04:33
epoch [87/200] batch [1/3] time 0.980 (0.980) data 0.268 (0.268) loss 0.2262 (0.2262) acc 96.8750 (96.8750) lr 1.2334e-03 eta 0:05:34
epoch [87/200] batch [2/3] time 0.712 (0.846) data 0.000 (0.134) loss 0.7017 (0.4639) acc 90.6250 (93.7500) lr 1.2334e-03 eta 0:04:47
epoch [87/200] batch [3/3] time 0.712 (0.801) data 0.000 (0.089) loss 0.1956 (0.3745) acc 96.8750 (94.7917) lr 1.2181e-03 eta 0:04:31
epoch [88/200] batch [1/3] time 0.971 (0.971) data 0.258 (0.258) loss 0.4775 (0.4775) acc 84.3750 (84.3750) lr 1.2181e-03 eta 0:05:28
epoch [88/200] batch [2/3] time 0.712 (0.841) data 0.000 (0.129) loss 0.3645 (0.4210) acc 93.7500 (89.0625) lr 1.2181e-03 eta 0:04:43
epoch [88/200] batch [3/3] time 0.712 (0.798) data 0.000 (0.086) loss 0.6475 (0.4965) acc 84.3750 (87.5000) lr 1.2028e-03 eta 0:04:28
epoch [89/200] batch [1/3] time 0.979 (0.979) data 0.266 (0.266) loss 0.3286 (0.3286) acc 96.8750 (96.8750) lr 1.2028e-03 eta 0:05:28
epoch [89/200] batch [2/3] time 0.712 (0.846) data 0.000 (0.133) loss 0.5039 (0.4163) acc 81.2500 (89.0625) lr 1.2028e-03 eta 0:04:42
epoch [89/200] batch [3/3] time 0.712 (0.801) data 0.000 (0.089) loss 0.1844 (0.3390) acc 96.8750 (91.6667) lr 1.1874e-03 eta 0:04:26
epoch [90/200] batch [1/3] time 0.973 (0.973) data 0.260 (0.260) loss 0.5703 (0.5703) acc 87.5000 (87.5000) lr 1.1874e-03 eta 0:05:23
epoch [90/200] batch [2/3] time 0.712 (0.842) data 0.000 (0.130) loss 0.1536 (0.3619) acc 100.0000 (93.7500) lr 1.1874e-03 eta 0:04:38
epoch [90/200] batch [3/3] time 0.712 (0.799) data 0.000 (0.087) loss 0.3120 (0.3453) acc 90.6250 (92.7083) lr 1.1719e-03 eta 0:04:23
epoch [91/200] batch [1/3] time 0.987 (0.987) data 0.276 (0.276) loss 0.3428 (0.3428) acc 90.6250 (90.6250) lr 1.1719e-03 eta 0:05:24
epoch [91/200] batch [2/3] time 0.712 (0.849) data 0.000 (0.138) loss 0.3345 (0.3386) acc 93.7500 (92.1875) lr 1.1719e-03 eta 0:04:38
epoch [91/200] batch [3/3] time 0.712 (0.804) data 0.000 (0.092) loss 0.5181 (0.3984) acc 93.7500 (92.7083) lr 1.1564e-03 eta 0:04:22
epoch [92/200] batch [1/3] time 0.984 (0.984) data 0.276 (0.276) loss 0.3386 (0.3386) acc 93.7500 (93.7500) lr 1.1564e-03 eta 0:05:20
epoch [92/200] batch [2/3] time 0.712 (0.848) data 0.000 (0.138) loss 0.1604 (0.2495) acc 100.0000 (96.8750) lr 1.1564e-03 eta 0:04:35
epoch [92/200] batch [3/3] time 0.712 (0.803) data 0.000 (0.092) loss 0.1653 (0.2214) acc 100.0000 (97.9167) lr 1.1409e-03 eta 0:04:20
epoch [93/200] batch [1/3] time 0.970 (0.970) data 0.259 (0.259) loss 0.5000 (0.5000) acc 87.5000 (87.5000) lr 1.1409e-03 eta 0:05:13
epoch [93/200] batch [2/3] time 0.712 (0.841) data 0.000 (0.129) loss 0.6748 (0.5874) acc 84.3750 (85.9375) lr 1.1409e-03 eta 0:04:30
epoch [93/200] batch [3/3] time 0.712 (0.798) data 0.000 (0.086) loss 0.3408 (0.5052) acc 93.7500 (88.5417) lr 1.1253e-03 eta 0:04:16
epoch [94/200] batch [1/3] time 0.970 (0.970) data 0.258 (0.258) loss 0.3174 (0.3174) acc 93.7500 (93.7500) lr 1.1253e-03 eta 0:05:10
epoch [94/200] batch [2/3] time 0.712 (0.841) data 0.000 (0.129) loss 0.3838 (0.3506) acc 90.6250 (92.1875) lr 1.1253e-03 eta 0:04:28
epoch [94/200] batch [3/3] time 0.712 (0.798) data 0.000 (0.086) loss 0.4089 (0.3700) acc 87.5000 (90.6250) lr 1.1097e-03 eta 0:04:13
epoch [95/200] batch [1/3] time 0.983 (0.983) data 0.275 (0.275) loss 0.6123 (0.6123) acc 87.5000 (87.5000) lr 1.1097e-03 eta 0:05:11
epoch [95/200] batch [2/3] time 0.712 (0.848) data 0.000 (0.137) loss 0.2869 (0.4496) acc 90.6250 (89.0625) lr 1.1097e-03 eta 0:04:27
epoch [95/200] batch [3/3] time 0.712 (0.802) data 0.000 (0.092) loss 0.3357 (0.4116) acc 93.7500 (90.6250) lr 1.0941e-03 eta 0:04:12
epoch [96/200] batch [1/3] time 0.967 (0.967) data 0.257 (0.257) loss 0.1833 (0.1833) acc 93.7500 (93.7500) lr 1.0941e-03 eta 0:05:03
epoch [96/200] batch [2/3] time 0.712 (0.840) data 0.000 (0.129) loss 0.5288 (0.3561) acc 87.5000 (90.6250) lr 1.0941e-03 eta 0:04:22
epoch [96/200] batch [3/3] time 0.712 (0.797) data 0.000 (0.086) loss 0.3191 (0.3438) acc 90.6250 (90.6250) lr 1.0785e-03 eta 0:04:08
epoch [97/200] batch [1/3] time 0.976 (0.976) data 0.268 (0.268) loss 0.8687 (0.8687) acc 81.2500 (81.2500) lr 1.0785e-03 eta 0:05:03
epoch [97/200] batch [2/3] time 0.712 (0.844) data 0.000 (0.134) loss 0.1274 (0.4980) acc 100.0000 (90.6250) lr 1.0785e-03 eta 0:04:21
epoch [97/200] batch [3/3] time 0.713 (0.800) data 0.000 (0.089) loss 0.3108 (0.4356) acc 93.7500 (91.6667) lr 1.0628e-03 eta 0:04:07
epoch [98/200] batch [1/3] time 0.985 (0.985) data 0.273 (0.273) loss 0.2957 (0.2957) acc 90.6250 (90.6250) lr 1.0628e-03 eta 0:05:03
epoch [98/200] batch [2/3] time 0.712 (0.849) data 0.000 (0.137) loss 0.4900 (0.3928) acc 87.5000 (89.0625) lr 1.0628e-03 eta 0:04:20
epoch [98/200] batch [3/3] time 0.712 (0.803) data 0.000 (0.091) loss 0.2251 (0.3369) acc 93.7500 (90.6250) lr 1.0471e-03 eta 0:04:05
epoch [99/200] batch [1/3] time 0.985 (0.985) data 0.274 (0.274) loss 0.3894 (0.3894) acc 84.3750 (84.3750) lr 1.0471e-03 eta 0:05:00
epoch [99/200] batch [2/3] time 0.712 (0.849) data 0.000 (0.137) loss 0.3711 (0.3802) acc 90.6250 (87.5000) lr 1.0471e-03 eta 0:04:17
epoch [99/200] batch [3/3] time 0.712 (0.803) data 0.000 (0.091) loss 0.3301 (0.3635) acc 93.7500 (89.5833) lr 1.0314e-03 eta 0:04:03
epoch [100/200] batch [1/3] time 0.969 (0.969) data 0.258 (0.258) loss 0.4446 (0.4446) acc 84.3750 (84.3750) lr 1.0314e-03 eta 0:04:52
epoch [100/200] batch [2/3] time 0.712 (0.841) data 0.000 (0.129) loss 0.3408 (0.3927) acc 84.3750 (84.3750) lr 1.0314e-03 eta 0:04:12
epoch [100/200] batch [3/3] time 0.711 (0.797) data 0.000 (0.086) loss 0.2101 (0.3318) acc 93.7500 (87.5000) lr 1.0157e-03 eta 0:03:59
epoch [101/200] batch [1/3] time 0.986 (0.986) data 0.275 (0.275) loss 0.4094 (0.4094) acc 93.7500 (93.7500) lr 1.0157e-03 eta 0:04:54
epoch [101/200] batch [2/3] time 0.712 (0.849) data 0.000 (0.138) loss 0.6255 (0.5175) acc 87.5000 (90.6250) lr 1.0157e-03 eta 0:04:12
epoch [101/200] batch [3/3] time 0.712 (0.803) data 0.000 (0.092) loss 0.4590 (0.4980) acc 87.5000 (89.5833) lr 1.0000e-03 eta 0:03:58
epoch [102/200] batch [1/3] time 0.985 (0.985) data 0.273 (0.273) loss 0.2676 (0.2676) acc 93.7500 (93.7500) lr 1.0000e-03 eta 0:04:51
epoch [102/200] batch [2/3] time 0.712 (0.848) data 0.000 (0.137) loss 0.4819 (0.3748) acc 84.3750 (89.0625) lr 1.0000e-03 eta 0:04:10
epoch [102/200] batch [3/3] time 0.712 (0.803) data 0.000 (0.091) loss 0.3093 (0.3529) acc 90.6250 (89.5833) lr 9.8429e-04 eta 0:03:56
epoch [103/200] batch [1/3] time 0.985 (0.985) data 0.274 (0.274) loss 0.7944 (0.7944) acc 75.0000 (75.0000) lr 9.8429e-04 eta 0:04:48
epoch [103/200] batch [2/3] time 0.712 (0.849) data 0.000 (0.137) loss 0.2754 (0.5349) acc 96.8750 (85.9375) lr 9.8429e-04 eta 0:04:07
epoch [103/200] batch [3/3] time 0.712 (0.803) data 0.000 (0.091) loss 0.4924 (0.5208) acc 90.6250 (87.5000) lr 9.6859e-04 eta 0:03:53
epoch [104/200] batch [1/3] time 0.985 (0.985) data 0.274 (0.274) loss 0.3379 (0.3379) acc 93.7500 (93.7500) lr 9.6859e-04 eta 0:04:45
epoch [104/200] batch [2/3] time 0.712 (0.849) data 0.000 (0.137) loss 0.3591 (0.3485) acc 93.7500 (93.7500) lr 9.6859e-04 eta 0:04:05
epoch [104/200] batch [3/3] time 0.713 (0.803) data 0.000 (0.091) loss 0.4543 (0.3838) acc 87.5000 (91.6667) lr 9.5289e-04 eta 0:03:51
epoch [105/200] batch [1/3] time 0.970 (0.970) data 0.259 (0.259) loss 0.3215 (0.3215) acc 93.7500 (93.7500) lr 9.5289e-04 eta 0:04:38
epoch [105/200] batch [2/3] time 0.716 (0.843) data 0.000 (0.129) loss 0.3245 (0.3230) acc 93.7500 (93.7500) lr 9.5289e-04 eta 0:04:01
epoch [105/200] batch [3/3] time 0.713 (0.800) data 0.000 (0.086) loss 0.3669 (0.3376) acc 84.3750 (90.6250) lr 9.3721e-04 eta 0:03:47
epoch [106/200] batch [1/3] time 0.974 (0.974) data 0.261 (0.261) loss 0.3088 (0.3088) acc 90.6250 (90.6250) lr 9.3721e-04 eta 0:04:36
epoch [106/200] batch [2/3] time 0.712 (0.843) data 0.000 (0.130) loss 0.1178 (0.2133) acc 100.0000 (95.3125) lr 9.3721e-04 eta 0:03:58
epoch [106/200] batch [3/3] time 0.712 (0.800) data 0.000 (0.087) loss 0.2637 (0.2301) acc 87.5000 (92.7083) lr 9.2154e-04 eta 0:03:45
epoch [107/200] batch [1/3] time 0.980 (0.980) data 0.272 (0.272) loss 0.5215 (0.5215) acc 90.6250 (90.6250) lr 9.2154e-04 eta 0:04:35
epoch [107/200] batch [2/3] time 0.712 (0.846) data 0.000 (0.136) loss 0.5728 (0.5471) acc 87.5000 (89.0625) lr 9.2154e-04 eta 0:03:56
epoch [107/200] batch [3/3] time 0.712 (0.802) data 0.000 (0.091) loss 0.4275 (0.5072) acc 90.6250 (89.5833) lr 9.0589e-04 eta 0:03:43
epoch [108/200] batch [1/3] time 0.983 (0.983) data 0.274 (0.274) loss 0.4912 (0.4912) acc 84.3750 (84.3750) lr 9.0589e-04 eta 0:04:33
epoch [108/200] batch [2/3] time 0.712 (0.848) data 0.000 (0.137) loss 0.4585 (0.4749) acc 87.5000 (85.9375) lr 9.0589e-04 eta 0:03:54
epoch [108/200] batch [3/3] time 0.716 (0.804) data 0.000 (0.091) loss 0.4304 (0.4600) acc 84.3750 (85.4167) lr 8.9027e-04 eta 0:03:41
epoch [109/200] batch [1/3] time 0.982 (0.982) data 0.274 (0.274) loss 0.4321 (0.4321) acc 90.6250 (90.6250) lr 8.9027e-04 eta 0:04:30
epoch [109/200] batch [2/3] time 0.712 (0.847) data 0.000 (0.137) loss 0.3176 (0.3749) acc 96.8750 (93.7500) lr 8.9027e-04 eta 0:03:52
epoch [109/200] batch [3/3] time 0.712 (0.802) data 0.000 (0.091) loss 0.3230 (0.3576) acc 93.7500 (93.7500) lr 8.7467e-04 eta 0:03:38
epoch [110/200] batch [1/3] time 0.969 (0.969) data 0.260 (0.260) loss 0.8105 (0.8105) acc 84.3750 (84.3750) lr 8.7467e-04 eta 0:04:23
epoch [110/200] batch [2/3] time 0.713 (0.841) data 0.000 (0.130) loss 0.3491 (0.5798) acc 93.7500 (89.0625) lr 8.7467e-04 eta 0:03:47
epoch [110/200] batch [3/3] time 0.712 (0.798) data 0.000 (0.087) loss 0.3171 (0.4923) acc 93.7500 (90.6250) lr 8.5910e-04 eta 0:03:35
epoch [111/200] batch [1/3] time 0.967 (0.967) data 0.256 (0.256) loss 0.5317 (0.5317) acc 87.5000 (87.5000) lr 8.5910e-04 eta 0:04:20
epoch [111/200] batch [2/3] time 0.712 (0.840) data 0.000 (0.128) loss 0.1656 (0.3487) acc 93.7500 (90.6250) lr 8.5910e-04 eta 0:03:44
epoch [111/200] batch [3/3] time 0.713 (0.797) data 0.000 (0.085) loss 0.1139 (0.2704) acc 96.8750 (92.7083) lr 8.4357e-04 eta 0:03:32
epoch [112/200] batch [1/3] time 0.976 (0.976) data 0.265 (0.265) loss 0.5010 (0.5010) acc 90.6250 (90.6250) lr 8.4357e-04 eta 0:04:19
epoch [112/200] batch [2/3] time 0.713 (0.844) data 0.000 (0.132) loss 0.0671 (0.2841) acc 100.0000 (95.3125) lr 8.4357e-04 eta 0:03:43
epoch [112/200] batch [3/3] time 0.712 (0.800) data 0.000 (0.088) loss 0.4712 (0.3464) acc 84.3750 (91.6667) lr 8.2807e-04 eta 0:03:31
epoch [113/200] batch [1/3] time 0.970 (0.970) data 0.259 (0.259) loss 0.6138 (0.6138) acc 81.2500 (81.2500) lr 8.2807e-04 eta 0:04:15
epoch [113/200] batch [2/3] time 0.712 (0.841) data 0.000 (0.129) loss 0.4102 (0.5120) acc 87.5000 (84.3750) lr 8.2807e-04 eta 0:03:40
epoch [113/200] batch [3/3] time 0.712 (0.798) data 0.000 (0.086) loss 0.2588 (0.4276) acc 93.7500 (87.5000) lr 8.1262e-04 eta 0:03:28
epoch [114/200] batch [1/3] time 0.988 (0.988) data 0.274 (0.274) loss 0.4646 (0.4646) acc 90.6250 (90.6250) lr 8.1262e-04 eta 0:04:16
epoch [114/200] batch [2/3] time 0.713 (0.850) data 0.000 (0.137) loss 0.3289 (0.3967) acc 93.7500 (92.1875) lr 8.1262e-04 eta 0:03:40
epoch [114/200] batch [3/3] time 0.712 (0.804) data 0.000 (0.092) loss 0.2966 (0.3634) acc 96.8750 (93.7500) lr 7.9721e-04 eta 0:03:27
epoch [115/200] batch [1/3] time 0.978 (0.978) data 0.265 (0.265) loss 0.2573 (0.2573) acc 93.7500 (93.7500) lr 7.9721e-04 eta 0:04:11
epoch [115/200] batch [2/3] time 0.714 (0.846) data 0.000 (0.133) loss 0.2179 (0.2376) acc 93.7500 (93.7500) lr 7.9721e-04 eta 0:03:36
epoch [115/200] batch [3/3] time 0.712 (0.802) data 0.000 (0.089) loss 0.4365 (0.3039) acc 93.7500 (93.7500) lr 7.8186e-04 eta 0:03:24
epoch [116/200] batch [1/3] time 0.970 (0.970) data 0.259 (0.259) loss 0.5381 (0.5381) acc 87.5000 (87.5000) lr 7.8186e-04 eta 0:04:06
epoch [116/200] batch [2/3] time 0.713 (0.842) data 0.000 (0.129) loss 0.1820 (0.3600) acc 93.7500 (90.6250) lr 7.8186e-04 eta 0:03:32
epoch [116/200] batch [3/3] time 0.712 (0.798) data 0.000 (0.086) loss 0.4644 (0.3948) acc 90.6250 (90.6250) lr 7.6655e-04 eta 0:03:21
epoch [117/200] batch [1/3] time 0.974 (0.974) data 0.261 (0.261) loss 0.1998 (0.1998) acc 96.8750 (96.8750) lr 7.6655e-04 eta 0:04:04
epoch [117/200] batch [2/3] time 0.714 (0.844) data 0.000 (0.131) loss 0.3215 (0.2607) acc 84.3750 (90.6250) lr 7.6655e-04 eta 0:03:30
epoch [117/200] batch [3/3] time 0.712 (0.800) data 0.000 (0.087) loss 0.1409 (0.2207) acc 96.8750 (92.7083) lr 7.5131e-04 eta 0:03:19
epoch [118/200] batch [1/3] time 0.977 (0.977) data 0.266 (0.266) loss 0.5581 (0.5581) acc 90.6250 (90.6250) lr 7.5131e-04 eta 0:04:02
epoch [118/200] batch [2/3] time 0.712 (0.845) data 0.000 (0.133) loss 0.4604 (0.5093) acc 87.5000 (89.0625) lr 7.5131e-04 eta 0:03:28
epoch [118/200] batch [3/3] time 0.714 (0.801) data 0.000 (0.089) loss 0.3691 (0.4626) acc 90.6250 (89.5833) lr 7.3613e-04 eta 0:03:17
epoch [119/200] batch [1/3] time 0.983 (0.983) data 0.274 (0.274) loss 0.5649 (0.5649) acc 87.5000 (87.5000) lr 7.3613e-04 eta 0:04:00
epoch [119/200] batch [2/3] time 0.712 (0.848) data 0.000 (0.137) loss 0.2900 (0.4275) acc 90.6250 (89.0625) lr 7.3613e-04 eta 0:03:26
epoch [119/200] batch [3/3] time 0.712 (0.803) data 0.000 (0.092) loss 0.3889 (0.4146) acc 93.7500 (90.6250) lr 7.2101e-04 eta 0:03:15
epoch [120/200] batch [1/3] time 0.973 (0.973) data 0.264 (0.264) loss 0.4165 (0.4165) acc 90.6250 (90.6250) lr 7.2101e-04 eta 0:03:55
epoch [120/200] batch [2/3] time 0.712 (0.843) data 0.000 (0.132) loss 0.5044 (0.4604) acc 81.2500 (85.9375) lr 7.2101e-04 eta 0:03:23
epoch [120/200] batch [3/3] time 0.715 (0.800) data 0.000 (0.088) loss 0.4854 (0.4688) acc 87.5000 (86.4583) lr 7.0596e-04 eta 0:03:12
epoch [121/200] batch [1/3] time 0.969 (0.969) data 0.260 (0.260) loss 0.2015 (0.2015) acc 96.8750 (96.8750) lr 7.0596e-04 eta 0:03:51
epoch [121/200] batch [2/3] time 0.712 (0.840) data 0.000 (0.130) loss 0.6304 (0.4160) acc 87.5000 (92.1875) lr 7.0596e-04 eta 0:03:19
epoch [121/200] batch [3/3] time 0.713 (0.798) data 0.000 (0.087) loss 0.2632 (0.3650) acc 93.7500 (92.7083) lr 6.9098e-04 eta 0:03:09
epoch [122/200] batch [1/3] time 0.978 (0.978) data 0.266 (0.266) loss 0.1562 (0.1562) acc 96.8750 (96.8750) lr 6.9098e-04 eta 0:03:50
epoch [122/200] batch [2/3] time 0.713 (0.845) data 0.000 (0.133) loss 0.4458 (0.3010) acc 84.3750 (90.6250) lr 6.9098e-04 eta 0:03:18
epoch [122/200] batch [3/3] time 0.714 (0.802) data 0.000 (0.089) loss 0.4084 (0.3368) acc 90.6250 (90.6250) lr 6.7608e-04 eta 0:03:07
epoch [123/200] batch [1/3] time 0.971 (0.971) data 0.259 (0.259) loss 0.4802 (0.4802) acc 90.6250 (90.6250) lr 6.7608e-04 eta 0:03:46
epoch [123/200] batch [2/3] time 0.712 (0.841) data 0.000 (0.130) loss 0.2805 (0.3804) acc 87.5000 (89.0625) lr 6.7608e-04 eta 0:03:15
epoch [123/200] batch [3/3] time 0.712 (0.798) data 0.000 (0.087) loss 0.2119 (0.3242) acc 93.7500 (90.6250) lr 6.6126e-04 eta 0:03:04
epoch [124/200] batch [1/3] time 0.971 (0.971) data 0.259 (0.259) loss 0.5332 (0.5332) acc 90.6250 (90.6250) lr 6.6126e-04 eta 0:03:43
epoch [124/200] batch [2/3] time 0.714 (0.843) data 0.000 (0.130) loss 0.6772 (0.6052) acc 84.3750 (87.5000) lr 6.6126e-04 eta 0:03:12
epoch [124/200] batch [3/3] time 0.713 (0.799) data 0.000 (0.086) loss 0.2059 (0.4721) acc 96.8750 (90.6250) lr 6.4653e-04 eta 0:03:02
epoch [125/200] batch [1/3] time 0.977 (0.977) data 0.266 (0.266) loss 0.2563 (0.2563) acc 90.6250 (90.6250) lr 6.4653e-04 eta 0:03:41
epoch [125/200] batch [2/3] time 0.712 (0.845) data 0.000 (0.133) loss 0.1429 (0.1996) acc 96.8750 (93.7500) lr 6.4653e-04 eta 0:03:10
epoch [125/200] batch [3/3] time 0.712 (0.801) data 0.000 (0.089) loss 0.2693 (0.2229) acc 93.7500 (93.7500) lr 6.3188e-04 eta 0:03:00
epoch [126/200] batch [1/3] time 0.969 (0.969) data 0.258 (0.258) loss 0.3921 (0.3921) acc 84.3750 (84.3750) lr 6.3188e-04 eta 0:03:37
epoch [126/200] batch [2/3] time 0.712 (0.841) data 0.000 (0.129) loss 0.3020 (0.3470) acc 93.7500 (89.0625) lr 6.3188e-04 eta 0:03:07
epoch [126/200] batch [3/3] time 0.712 (0.798) data 0.000 (0.086) loss 0.1199 (0.2713) acc 100.0000 (92.7083) lr 6.1732e-04 eta 0:02:57
epoch [127/200] batch [1/3] time 0.971 (0.971) data 0.258 (0.258) loss 0.7207 (0.7207) acc 81.2500 (81.2500) lr 6.1732e-04 eta 0:03:34
epoch [127/200] batch [2/3] time 0.712 (0.842) data 0.000 (0.129) loss 0.5132 (0.6169) acc 84.3750 (82.8125) lr 6.1732e-04 eta 0:03:05
epoch [127/200] batch [3/3] time 0.712 (0.799) data 0.000 (0.086) loss 0.5103 (0.5814) acc 87.5000 (84.3750) lr 6.0285e-04 eta 0:02:54
epoch [128/200] batch [1/3] time 0.979 (0.979) data 0.268 (0.268) loss 0.2299 (0.2299) acc 90.6250 (90.6250) lr 6.0285e-04 eta 0:03:33
epoch [128/200] batch [2/3] time 0.714 (0.846) data 0.000 (0.134) loss 0.1948 (0.2123) acc 93.7500 (92.1875) lr 6.0285e-04 eta 0:03:03
epoch [128/200] batch [3/3] time 0.713 (0.802) data 0.000 (0.089) loss 0.0999 (0.1748) acc 93.7500 (92.7083) lr 5.8849e-04 eta 0:02:53
epoch [129/200] batch [1/3] time 0.980 (0.980) data 0.266 (0.266) loss 0.3572 (0.3572) acc 90.6250 (90.6250) lr 5.8849e-04 eta 0:03:30
epoch [129/200] batch [2/3] time 0.712 (0.846) data 0.000 (0.133) loss 0.1837 (0.2704) acc 96.8750 (93.7500) lr 5.8849e-04 eta 0:03:01
epoch [129/200] batch [3/3] time 0.713 (0.802) data 0.000 (0.089) loss 0.1663 (0.2357) acc 96.8750 (94.7917) lr 5.7422e-04 eta 0:02:50
epoch [130/200] batch [1/3] time 0.968 (0.968) data 0.259 (0.259) loss 0.4690 (0.4690) acc 87.5000 (87.5000) lr 5.7422e-04 eta 0:03:25
epoch [130/200] batch [2/3] time 0.712 (0.840) data 0.000 (0.130) loss 0.7891 (0.6290) acc 81.2500 (84.3750) lr 5.7422e-04 eta 0:02:57
epoch [130/200] batch [3/3] time 0.713 (0.797) data 0.000 (0.086) loss 0.0641 (0.4407) acc 100.0000 (89.5833) lr 5.6006e-04 eta 0:02:47
epoch [131/200] batch [1/3] time 0.983 (0.983) data 0.275 (0.275) loss 0.4470 (0.4470) acc 90.6250 (90.6250) lr 5.6006e-04 eta 0:03:25
epoch [131/200] batch [2/3] time 0.713 (0.848) data 0.000 (0.137) loss 0.4238 (0.4354) acc 93.7500 (92.1875) lr 5.6006e-04 eta 0:02:56
epoch [131/200] batch [3/3] time 0.716 (0.804) data 0.000 (0.092) loss 0.0724 (0.3144) acc 100.0000 (94.7917) lr 5.4601e-04 eta 0:02:46
epoch [132/200] batch [1/3] time 0.978 (0.978) data 0.266 (0.266) loss 0.5049 (0.5049) acc 87.5000 (87.5000) lr 5.4601e-04 eta 0:03:21
epoch [132/200] batch [2/3] time 0.712 (0.845) data 0.000 (0.133) loss 0.4858 (0.4954) acc 90.6250 (89.0625) lr 5.4601e-04 eta 0:02:53
epoch [132/200] batch [3/3] time 0.713 (0.801) data 0.000 (0.089) loss 0.3606 (0.4504) acc 84.3750 (87.5000) lr 5.3207e-04 eta 0:02:43
epoch [133/200] batch [1/3] time 0.983 (0.983) data 0.270 (0.270) loss 0.1247 (0.1247) acc 100.0000 (100.0000) lr 5.3207e-04 eta 0:03:19
epoch [133/200] batch [2/3] time 0.712 (0.848) data 0.000 (0.135) loss 0.1794 (0.1521) acc 100.0000 (100.0000) lr 5.3207e-04 eta 0:02:51
epoch [133/200] batch [3/3] time 0.712 (0.802) data 0.000 (0.090) loss 0.4866 (0.2636) acc 87.5000 (95.8333) lr 5.1825e-04 eta 0:02:41
epoch [134/200] batch [1/3] time 0.988 (0.988) data 0.278 (0.278) loss 0.1582 (0.1582) acc 100.0000 (100.0000) lr 5.1825e-04 eta 0:03:17
epoch [134/200] batch [2/3] time 0.712 (0.850) data 0.000 (0.139) loss 0.4570 (0.3076) acc 90.6250 (95.3125) lr 5.1825e-04 eta 0:02:49
epoch [134/200] batch [3/3] time 0.713 (0.804) data 0.000 (0.093) loss 0.0727 (0.2293) acc 100.0000 (96.8750) lr 5.0454e-04 eta 0:02:39
epoch [135/200] batch [1/3] time 0.970 (0.970) data 0.258 (0.258) loss 0.1580 (0.1580) acc 96.8750 (96.8750) lr 5.0454e-04 eta 0:03:11
epoch [135/200] batch [2/3] time 0.712 (0.841) data 0.000 (0.129) loss 0.4136 (0.2858) acc 93.7500 (95.3125) lr 5.0454e-04 eta 0:02:44
epoch [135/200] batch [3/3] time 0.712 (0.798) data 0.000 (0.086) loss 0.3530 (0.3082) acc 90.6250 (93.7500) lr 4.9096e-04 eta 0:02:35
epoch [136/200] batch [1/3] time 0.981 (0.981) data 0.270 (0.270) loss 0.0806 (0.0806) acc 100.0000 (100.0000) lr 4.9096e-04 eta 0:03:10
epoch [136/200] batch [2/3] time 0.714 (0.848) data 0.000 (0.135) loss 0.2668 (0.1737) acc 96.8750 (98.4375) lr 4.9096e-04 eta 0:02:43
epoch [136/200] batch [3/3] time 0.712 (0.803) data 0.000 (0.090) loss 0.1619 (0.1698) acc 93.7500 (96.8750) lr 4.7750e-04 eta 0:02:34
epoch [137/200] batch [1/3] time 0.986 (0.986) data 0.273 (0.273) loss 0.2230 (0.2230) acc 93.7500 (93.7500) lr 4.7750e-04 eta 0:03:08
epoch [137/200] batch [2/3] time 0.712 (0.849) data 0.000 (0.137) loss 0.1646 (0.1938) acc 100.0000 (96.8750) lr 4.7750e-04 eta 0:02:41
epoch [137/200] batch [3/3] time 0.712 (0.803) data 0.000 (0.091) loss 0.1981 (0.1952) acc 96.8750 (96.8750) lr 4.6417e-04 eta 0:02:31
epoch [138/200] batch [1/3] time 0.986 (0.986) data 0.275 (0.275) loss 0.4045 (0.4045) acc 90.6250 (90.6250) lr 4.6417e-04 eta 0:03:05
epoch [138/200] batch [2/3] time 0.712 (0.849) data 0.000 (0.138) loss 0.2849 (0.3447) acc 93.7500 (92.1875) lr 4.6417e-04 eta 0:02:38
epoch [138/200] batch [3/3] time 0.713 (0.804) data 0.000 (0.092) loss 0.2369 (0.3088) acc 96.8750 (93.7500) lr 4.5098e-04 eta 0:02:29
epoch [139/200] batch [1/3] time 0.986 (0.986) data 0.275 (0.275) loss 0.1586 (0.1586) acc 96.8750 (96.8750) lr 4.5098e-04 eta 0:03:02
epoch [139/200] batch [2/3] time 0.712 (0.849) data 0.000 (0.138) loss 0.1372 (0.1479) acc 100.0000 (98.4375) lr 4.5098e-04 eta 0:02:36
epoch [139/200] batch [3/3] time 0.712 (0.803) data 0.000 (0.092) loss 0.2157 (0.1705) acc 93.7500 (96.8750) lr 4.3792e-04 eta 0:02:27
epoch [140/200] batch [1/3] time 0.987 (0.987) data 0.275 (0.275) loss 0.1761 (0.1761) acc 96.8750 (96.8750) lr 4.3792e-04 eta 0:02:59
epoch [140/200] batch [2/3] time 0.712 (0.850) data 0.000 (0.138) loss 0.6626 (0.4194) acc 84.3750 (90.6250) lr 4.3792e-04 eta 0:02:33
epoch [140/200] batch [3/3] time 0.713 (0.804) data 0.000 (0.092) loss 0.4346 (0.4244) acc 90.6250 (90.6250) lr 4.2499e-04 eta 0:02:24
epoch [141/200] batch [1/3] time 0.971 (0.971) data 0.258 (0.258) loss 0.3425 (0.3425) acc 93.7500 (93.7500) lr 4.2499e-04 eta 0:02:53
epoch [141/200] batch [2/3] time 0.712 (0.842) data 0.000 (0.129) loss 0.1992 (0.2709) acc 96.8750 (95.3125) lr 4.2499e-04 eta 0:02:29
epoch [141/200] batch [3/3] time 0.712 (0.799) data 0.000 (0.086) loss 0.5132 (0.3516) acc 87.5000 (92.7083) lr 4.1221e-04 eta 0:02:21
epoch [142/200] batch [1/3] time 0.968 (0.968) data 0.259 (0.259) loss 0.3254 (0.3254) acc 93.7500 (93.7500) lr 4.1221e-04 eta 0:02:50
epoch [142/200] batch [2/3] time 0.712 (0.840) data 0.000 (0.130) loss 0.2842 (0.3048) acc 96.8750 (95.3125) lr 4.1221e-04 eta 0:02:27
epoch [142/200] batch [3/3] time 0.713 (0.798) data 0.000 (0.087) loss 0.4607 (0.3568) acc 84.3750 (91.6667) lr 3.9958e-04 eta 0:02:18
epoch [143/200] batch [1/3] time 0.976 (0.976) data 0.267 (0.267) loss 0.1481 (0.1481) acc 96.8750 (96.8750) lr 3.9958e-04 eta 0:02:48
epoch [143/200] batch [2/3] time 0.712 (0.844) data 0.000 (0.134) loss 0.4072 (0.2776) acc 87.5000 (92.1875) lr 3.9958e-04 eta 0:02:25
epoch [143/200] batch [3/3] time 0.714 (0.801) data 0.000 (0.089) loss 0.3271 (0.2941) acc 93.7500 (92.7083) lr 3.8709e-04 eta 0:02:16
epoch [144/200] batch [1/3] time 0.977 (0.977) data 0.268 (0.268) loss 0.1271 (0.1271) acc 100.0000 (100.0000) lr 3.8709e-04 eta 0:02:46
epoch [144/200] batch [2/3] time 0.712 (0.845) data 0.000 (0.134) loss 0.0536 (0.0904) acc 100.0000 (100.0000) lr 3.8709e-04 eta 0:02:22
epoch [144/200] batch [3/3] time 0.712 (0.801) data 0.000 (0.089) loss 0.1823 (0.1210) acc 100.0000 (100.0000) lr 3.7476e-04 eta 0:02:14
epoch [145/200] batch [1/3] time 0.981 (0.981) data 0.268 (0.268) loss 0.2010 (0.2010) acc 93.7500 (93.7500) lr 3.7476e-04 eta 0:02:43
epoch [145/200] batch [2/3] time 0.712 (0.847) data 0.000 (0.134) loss 0.3962 (0.2986) acc 96.8750 (95.3125) lr 3.7476e-04 eta 0:02:20
epoch [145/200] batch [3/3] time 0.712 (0.802) data 0.000 (0.089) loss 0.1394 (0.2456) acc 100.0000 (96.8750) lr 3.6258e-04 eta 0:02:12
epoch [146/200] batch [1/3] time 0.980 (0.980) data 0.268 (0.268) loss 0.4888 (0.4888) acc 84.3750 (84.3750) lr 3.6258e-04 eta 0:02:40
epoch [146/200] batch [2/3] time 0.712 (0.846) data 0.000 (0.134) loss 0.2090 (0.3489) acc 93.7500 (89.0625) lr 3.6258e-04 eta 0:02:17
epoch [146/200] batch [3/3] time 0.712 (0.801) data 0.000 (0.090) loss 0.1515 (0.2831) acc 93.7500 (90.6250) lr 3.5055e-04 eta 0:02:09
epoch [147/200] batch [1/3] time 0.980 (0.980) data 0.267 (0.267) loss 0.2341 (0.2341) acc 93.7500 (93.7500) lr 3.5055e-04 eta 0:02:37
epoch [147/200] batch [2/3] time 0.712 (0.846) data 0.000 (0.134) loss 0.7827 (0.5084) acc 84.3750 (89.0625) lr 3.5055e-04 eta 0:02:15
epoch [147/200] batch [3/3] time 0.712 (0.802) data 0.000 (0.089) loss 0.1780 (0.3983) acc 93.7500 (90.6250) lr 3.3869e-04 eta 0:02:07
epoch [148/200] batch [1/3] time 0.976 (0.976) data 0.265 (0.265) loss 0.4016 (0.4016) acc 90.6250 (90.6250) lr 3.3869e-04 eta 0:02:34
epoch [148/200] batch [2/3] time 0.712 (0.844) data 0.000 (0.133) loss 0.4282 (0.4149) acc 90.6250 (90.6250) lr 3.3869e-04 eta 0:02:12
epoch [148/200] batch [3/3] time 0.713 (0.800) data 0.000 (0.088) loss 0.1962 (0.3420) acc 93.7500 (91.6667) lr 3.2699e-04 eta 0:02:04
epoch [149/200] batch [1/3] time 0.986 (0.986) data 0.273 (0.273) loss 0.3110 (0.3110) acc 90.6250 (90.6250) lr 3.2699e-04 eta 0:02:32
epoch [149/200] batch [2/3] time 0.714 (0.850) data 0.000 (0.137) loss 0.3115 (0.3113) acc 90.6250 (90.6250) lr 3.2699e-04 eta 0:02:10
epoch [149/200] batch [3/3] time 0.713 (0.804) data 0.000 (0.091) loss 0.3430 (0.3219) acc 93.7500 (91.6667) lr 3.1545e-04 eta 0:02:03
epoch [150/200] batch [1/3] time 0.979 (0.979) data 0.266 (0.266) loss 0.1053 (0.1053) acc 96.8750 (96.8750) lr 3.1545e-04 eta 0:02:28
epoch [150/200] batch [2/3] time 0.712 (0.846) data 0.000 (0.133) loss 0.4192 (0.2623) acc 93.7500 (95.3125) lr 3.1545e-04 eta 0:02:07
epoch [150/200] batch [3/3] time 0.713 (0.801) data 0.000 (0.089) loss 0.4700 (0.3315) acc 90.6250 (93.7500) lr 3.0409e-04 eta 0:02:00
epoch [151/200] batch [1/3] time 0.972 (0.972) data 0.259 (0.259) loss 0.3823 (0.3823) acc 96.8750 (96.8750) lr 3.0409e-04 eta 0:02:24
epoch [151/200] batch [2/3] time 0.718 (0.845) data 0.000 (0.130) loss 0.1311 (0.2567) acc 96.8750 (96.8750) lr 3.0409e-04 eta 0:02:05
epoch [151/200] batch [3/3] time 0.712 (0.801) data 0.000 (0.086) loss 0.3940 (0.3025) acc 84.3750 (92.7083) lr 2.9289e-04 eta 0:01:57
epoch [152/200] batch [1/3] time 0.986 (0.986) data 0.275 (0.275) loss 0.3086 (0.3086) acc 90.6250 (90.6250) lr 2.9289e-04 eta 0:02:23
epoch [152/200] batch [2/3] time 0.712 (0.849) data 0.000 (0.137) loss 0.2637 (0.2861) acc 93.7500 (92.1875) lr 2.9289e-04 eta 0:02:03
epoch [152/200] batch [3/3] time 0.713 (0.804) data 0.000 (0.092) loss 0.2812 (0.2845) acc 96.8750 (93.7500) lr 2.8187e-04 eta 0:01:55
epoch [153/200] batch [1/3] time 0.968 (0.968) data 0.259 (0.259) loss 0.4229 (0.4229) acc 87.5000 (87.5000) lr 2.8187e-04 eta 0:02:18
epoch [153/200] batch [2/3] time 0.712 (0.840) data 0.000 (0.130) loss 0.3201 (0.3715) acc 93.7500 (90.6250) lr 2.8187e-04 eta 0:01:59
epoch [153/200] batch [3/3] time 0.712 (0.798) data 0.000 (0.086) loss 0.4556 (0.3995) acc 93.7500 (91.6667) lr 2.7103e-04 eta 0:01:52
epoch [154/200] batch [1/3] time 0.977 (0.977) data 0.266 (0.266) loss 0.3098 (0.3098) acc 90.6250 (90.6250) lr 2.7103e-04 eta 0:02:16
epoch [154/200] batch [2/3] time 0.712 (0.844) data 0.000 (0.133) loss 0.2976 (0.3037) acc 93.7500 (92.1875) lr 2.7103e-04 eta 0:01:57
epoch [154/200] batch [3/3] time 0.712 (0.800) data 0.000 (0.089) loss 0.6504 (0.4193) acc 84.3750 (89.5833) lr 2.6037e-04 eta 0:01:50
epoch [155/200] batch [1/3] time 0.978 (0.978) data 0.268 (0.268) loss 0.6201 (0.6201) acc 90.6250 (90.6250) lr 2.6037e-04 eta 0:02:13
epoch [155/200] batch [2/3] time 0.712 (0.845) data 0.000 (0.134) loss 0.3020 (0.4611) acc 90.6250 (90.6250) lr 2.6037e-04 eta 0:01:54
epoch [155/200] batch [3/3] time 0.713 (0.801) data 0.000 (0.089) loss 0.0587 (0.3269) acc 100.0000 (93.7500) lr 2.4989e-04 eta 0:01:48
epoch [156/200] batch [1/3] time 0.985 (0.985) data 0.276 (0.276) loss 0.1102 (0.1102) acc 100.0000 (100.0000) lr 2.4989e-04 eta 0:02:11
epoch [156/200] batch [2/3] time 0.712 (0.848) data 0.000 (0.138) loss 0.5005 (0.3054) acc 90.6250 (95.3125) lr 2.4989e-04 eta 0:01:52
epoch [156/200] batch [3/3] time 0.712 (0.803) data 0.000 (0.092) loss 0.2114 (0.2740) acc 96.8750 (95.8333) lr 2.3959e-04 eta 0:01:45
epoch [157/200] batch [1/3] time 0.975 (0.975) data 0.266 (0.266) loss 0.1622 (0.1622) acc 96.8750 (96.8750) lr 2.3959e-04 eta 0:02:07
epoch [157/200] batch [2/3] time 0.712 (0.844) data 0.000 (0.133) loss 0.2477 (0.2050) acc 96.8750 (96.8750) lr 2.3959e-04 eta 0:01:49
epoch [157/200] batch [3/3] time 0.712 (0.800) data 0.000 (0.089) loss 0.3574 (0.2558) acc 90.6250 (94.7917) lr 2.2949e-04 eta 0:01:43
epoch [158/200] batch [1/3] time 0.987 (0.987) data 0.276 (0.276) loss 0.1652 (0.1652) acc 100.0000 (100.0000) lr 2.2949e-04 eta 0:02:06
epoch [158/200] batch [2/3] time 0.712 (0.849) data 0.000 (0.138) loss 0.3633 (0.2642) acc 90.6250 (95.3125) lr 2.2949e-04 eta 0:01:47
epoch [158/200] batch [3/3] time 0.712 (0.804) data 0.000 (0.092) loss 0.0706 (0.1997) acc 96.8750 (95.8333) lr 2.1957e-04 eta 0:01:41
epoch [159/200] batch [1/3] time 0.978 (0.978) data 0.266 (0.266) loss 0.1390 (0.1390) acc 96.8750 (96.8750) lr 2.1957e-04 eta 0:02:02
epoch [159/200] batch [2/3] time 0.712 (0.845) data 0.000 (0.133) loss 0.2969 (0.2180) acc 90.6250 (93.7500) lr 2.1957e-04 eta 0:01:44
epoch [159/200] batch [3/3] time 0.715 (0.802) data 0.000 (0.089) loss 0.4565 (0.2975) acc 87.5000 (91.6667) lr 2.0984e-04 eta 0:01:38
epoch [160/200] batch [1/3] time 0.988 (0.988) data 0.277 (0.277) loss 0.6807 (0.6807) acc 84.3750 (84.3750) lr 2.0984e-04 eta 0:02:00
epoch [160/200] batch [2/3] time 0.712 (0.850) data 0.000 (0.138) loss 0.3872 (0.5339) acc 87.5000 (85.9375) lr 2.0984e-04 eta 0:01:42
epoch [160/200] batch [3/3] time 0.712 (0.804) data 0.000 (0.092) loss 0.2769 (0.4482) acc 96.8750 (89.5833) lr 2.0032e-04 eta 0:01:36
epoch [161/200] batch [1/3] time 0.981 (0.981) data 0.267 (0.267) loss 0.1578 (0.1578) acc 96.8750 (96.8750) lr 2.0032e-04 eta 0:01:56
epoch [161/200] batch [2/3] time 0.712 (0.846) data 0.000 (0.134) loss 0.6270 (0.3924) acc 90.6250 (93.7500) lr 2.0032e-04 eta 0:01:39
epoch [161/200] batch [3/3] time 0.713 (0.802) data 0.000 (0.089) loss 0.2632 (0.3493) acc 90.6250 (92.7083) lr 1.9098e-04 eta 0:01:33
epoch [162/200] batch [1/3] time 0.974 (0.974) data 0.263 (0.263) loss 0.1949 (0.1949) acc 100.0000 (100.0000) lr 1.9098e-04 eta 0:01:53
epoch [162/200] batch [2/3] time 0.712 (0.843) data 0.000 (0.131) loss 0.4888 (0.3419) acc 81.2500 (90.6250) lr 1.9098e-04 eta 0:01:36
epoch [162/200] batch [3/3] time 0.712 (0.799) data 0.000 (0.088) loss 0.3462 (0.3433) acc 90.6250 (90.6250) lr 1.8185e-04 eta 0:01:31
epoch [163/200] batch [1/3] time 0.986 (0.986) data 0.274 (0.274) loss 0.4719 (0.4719) acc 84.3750 (84.3750) lr 1.8185e-04 eta 0:01:51
epoch [163/200] batch [2/3] time 0.712 (0.849) data 0.000 (0.137) loss 0.3574 (0.4147) acc 90.6250 (87.5000) lr 1.8185e-04 eta 0:01:35
epoch [163/200] batch [3/3] time 0.712 (0.803) data 0.000 (0.092) loss 0.2771 (0.3688) acc 96.8750 (90.6250) lr 1.7292e-04 eta 0:01:29
epoch [164/200] batch [1/3] time 0.976 (0.976) data 0.265 (0.265) loss 0.4099 (0.4099) acc 93.7500 (93.7500) lr 1.7292e-04 eta 0:01:47
epoch [164/200] batch [2/3] time 0.712 (0.844) data 0.000 (0.133) loss 0.6045 (0.5072) acc 78.1250 (85.9375) lr 1.7292e-04 eta 0:01:32
epoch [164/200] batch [3/3] time 0.712 (0.800) data 0.000 (0.088) loss 0.2947 (0.4364) acc 96.8750 (89.5833) lr 1.6419e-04 eta 0:01:26
epoch [165/200] batch [1/3] time 0.986 (0.986) data 0.275 (0.275) loss 0.3562 (0.3562) acc 93.7500 (93.7500) lr 1.6419e-04 eta 0:01:45
epoch [165/200] batch [2/3] time 0.715 (0.850) data 0.000 (0.138) loss 0.3723 (0.3643) acc 90.6250 (92.1875) lr 1.6419e-04 eta 0:01:30
epoch [165/200] batch [3/3] time 0.712 (0.804) data 0.000 (0.092) loss 0.1807 (0.3031) acc 96.8750 (93.7500) lr 1.5567e-04 eta 0:01:24
epoch [166/200] batch [1/3] time 0.981 (0.981) data 0.269 (0.269) loss 0.2534 (0.2534) acc 93.7500 (93.7500) lr 1.5567e-04 eta 0:01:42
epoch [166/200] batch [2/3] time 0.712 (0.847) data 0.000 (0.134) loss 0.3240 (0.2887) acc 87.5000 (90.6250) lr 1.5567e-04 eta 0:01:27
epoch [166/200] batch [3/3] time 0.712 (0.802) data 0.000 (0.090) loss 0.1935 (0.2570) acc 96.8750 (92.7083) lr 1.4736e-04 eta 0:01:21
epoch [167/200] batch [1/3] time 0.974 (0.974) data 0.261 (0.261) loss 0.6938 (0.6938) acc 84.3750 (84.3750) lr 1.4736e-04 eta 0:01:38
epoch [167/200] batch [2/3] time 0.712 (0.843) data 0.000 (0.130) loss 0.1462 (0.4200) acc 93.7500 (89.0625) lr 1.4736e-04 eta 0:01:24
epoch [167/200] batch [3/3] time 0.712 (0.799) data 0.000 (0.087) loss 0.0533 (0.2978) acc 100.0000 (92.7083) lr 1.3926e-04 eta 0:01:19
epoch [168/200] batch [1/3] time 0.974 (0.974) data 0.266 (0.266) loss 0.2006 (0.2006) acc 96.8750 (96.8750) lr 1.3926e-04 eta 0:01:35
epoch [168/200] batch [2/3] time 0.712 (0.843) data 0.000 (0.133) loss 0.1511 (0.1758) acc 100.0000 (98.4375) lr 1.3926e-04 eta 0:01:21
epoch [168/200] batch [3/3] time 0.713 (0.800) data 0.000 (0.089) loss 0.3101 (0.2206) acc 93.7500 (96.8750) lr 1.3137e-04 eta 0:01:16
epoch [169/200] batch [1/3] time 0.984 (0.984) data 0.275 (0.275) loss 0.1295 (0.1295) acc 100.0000 (100.0000) lr 1.3137e-04 eta 0:01:33
epoch [169/200] batch [2/3] time 0.712 (0.848) data 0.000 (0.137) loss 0.3396 (0.2346) acc 90.6250 (95.3125) lr 1.3137e-04 eta 0:01:19
epoch [169/200] batch [3/3] time 0.712 (0.802) data 0.000 (0.092) loss 0.3132 (0.2608) acc 90.6250 (93.7500) lr 1.2369e-04 eta 0:01:14
epoch [170/200] batch [1/3] time 0.979 (0.979) data 0.268 (0.268) loss 0.2064 (0.2064) acc 93.7500 (93.7500) lr 1.2369e-04 eta 0:01:30
epoch [170/200] batch [2/3] time 0.712 (0.846) data 0.000 (0.134) loss 0.1724 (0.1894) acc 100.0000 (96.8750) lr 1.2369e-04 eta 0:01:16
epoch [170/200] batch [3/3] time 0.712 (0.801) data 0.000 (0.089) loss 0.2749 (0.2179) acc 93.7500 (95.8333) lr 1.1623e-04 eta 0:01:12
epoch [171/200] batch [1/3] time 0.979 (0.979) data 0.267 (0.267) loss 0.3406 (0.3406) acc 90.6250 (90.6250) lr 1.1623e-04 eta 0:01:27
epoch [171/200] batch [2/3] time 0.712 (0.845) data 0.000 (0.134) loss 0.2069 (0.2737) acc 93.7500 (92.1875) lr 1.1623e-04 eta 0:01:14
epoch [171/200] batch [3/3] time 0.712 (0.801) data 0.000 (0.089) loss 0.1996 (0.2490) acc 96.8750 (93.7500) lr 1.0899e-04 eta 0:01:09
epoch [172/200] batch [1/3] time 0.980 (0.980) data 0.267 (0.267) loss 0.2294 (0.2294) acc 96.8750 (96.8750) lr 1.0899e-04 eta 0:01:24
epoch [172/200] batch [2/3] time 0.712 (0.846) data 0.000 (0.133) loss 0.4297 (0.3295) acc 87.5000 (92.1875) lr 1.0899e-04 eta 0:01:11
epoch [172/200] batch [3/3] time 0.712 (0.801) data 0.000 (0.089) loss 0.4309 (0.3633) acc 87.5000 (90.6250) lr 1.0197e-04 eta 0:01:07
epoch [173/200] batch [1/3] time 0.971 (0.971) data 0.260 (0.260) loss 0.1890 (0.1890) acc 96.8750 (96.8750) lr 1.0197e-04 eta 0:01:20
epoch [173/200] batch [2/3] time 0.715 (0.843) data 0.000 (0.130) loss 0.1215 (0.1552) acc 96.8750 (96.8750) lr 1.0197e-04 eta 0:01:09
epoch [173/200] batch [3/3] time 0.712 (0.799) data 0.000 (0.087) loss 0.1697 (0.1600) acc 96.8750 (96.8750) lr 9.5173e-05 eta 0:01:04
epoch [174/200] batch [1/3] time 0.980 (0.980) data 0.268 (0.268) loss 0.1941 (0.1941) acc 96.8750 (96.8750) lr 9.5173e-05 eta 0:01:18
epoch [174/200] batch [2/3] time 0.712 (0.846) data 0.000 (0.134) loss 0.0997 (0.1469) acc 96.8750 (96.8750) lr 9.5173e-05 eta 0:01:06
epoch [174/200] batch [3/3] time 0.712 (0.801) data 0.000 (0.089) loss 0.6021 (0.2986) acc 87.5000 (93.7500) lr 8.8597e-05 eta 0:01:02
epoch [175/200] batch [1/3] time 0.972 (0.972) data 0.260 (0.260) loss 0.6919 (0.6919) acc 90.6250 (90.6250) lr 8.8597e-05 eta 0:01:14
epoch [175/200] batch [2/3] time 0.713 (0.843) data 0.000 (0.130) loss 0.4934 (0.5927) acc 87.5000 (89.0625) lr 8.8597e-05 eta 0:01:04
epoch [175/200] batch [3/3] time 0.712 (0.799) data 0.000 (0.087) loss 0.2417 (0.4757) acc 93.7500 (90.6250) lr 8.2245e-05 eta 0:00:59
epoch [176/200] batch [1/3] time 0.971 (0.971) data 0.258 (0.258) loss 0.2837 (0.2837) acc 93.7500 (93.7500) lr 8.2245e-05 eta 0:01:11
epoch [176/200] batch [2/3] time 0.715 (0.843) data 0.000 (0.129) loss 0.4114 (0.3475) acc 96.8750 (95.3125) lr 8.2245e-05 eta 0:01:01
epoch [176/200] batch [3/3] time 0.717 (0.801) data 0.000 (0.086) loss 0.1675 (0.2875) acc 93.7500 (94.7917) lr 7.6120e-05 eta 0:00:57
epoch [177/200] batch [1/3] time 0.984 (0.984) data 0.273 (0.273) loss 0.1870 (0.1870) acc 100.0000 (100.0000) lr 7.6120e-05 eta 0:01:09
epoch [177/200] batch [2/3] time 0.712 (0.848) data 0.000 (0.137) loss 0.5752 (0.3811) acc 90.6250 (95.3125) lr 7.6120e-05 eta 0:00:59
epoch [177/200] batch [3/3] time 0.712 (0.803) data 0.000 (0.091) loss 0.2949 (0.3524) acc 93.7500 (94.7917) lr 7.0224e-05 eta 0:00:55
epoch [178/200] batch [1/3] time 0.979 (0.979) data 0.266 (0.266) loss 0.4346 (0.4346) acc 90.6250 (90.6250) lr 7.0224e-05 eta 0:01:06
epoch [178/200] batch [2/3] time 0.712 (0.845) data 0.000 (0.133) loss 0.0979 (0.2662) acc 100.0000 (95.3125) lr 7.0224e-05 eta 0:00:56
epoch [178/200] batch [3/3] time 0.717 (0.802) data 0.000 (0.089) loss 0.2156 (0.2493) acc 96.8750 (95.8333) lr 6.4556e-05 eta 0:00:52
epoch [179/200] batch [1/3] time 0.978 (0.978) data 0.267 (0.267) loss 0.3557 (0.3557) acc 93.7500 (93.7500) lr 6.4556e-05 eta 0:01:03
epoch [179/200] batch [2/3] time 0.713 (0.845) data 0.000 (0.134) loss 0.5698 (0.4628) acc 87.5000 (90.6250) lr 6.4556e-05 eta 0:00:54
epoch [179/200] batch [3/3] time 0.712 (0.801) data 0.000 (0.089) loss 0.1949 (0.3735) acc 93.7500 (91.6667) lr 5.9119e-05 eta 0:00:50
epoch [180/200] batch [1/3] time 0.970 (0.970) data 0.261 (0.261) loss 0.0682 (0.0682) acc 100.0000 (100.0000) lr 5.9119e-05 eta 0:01:00
epoch [180/200] batch [2/3] time 0.712 (0.841) data 0.000 (0.131) loss 0.0931 (0.0807) acc 100.0000 (100.0000) lr 5.9119e-05 eta 0:00:51
epoch [180/200] batch [3/3] time 0.713 (0.798) data 0.000 (0.087) loss 0.1714 (0.1109) acc 96.8750 (98.9583) lr 5.3915e-05 eta 0:00:47
epoch [181/200] batch [1/3] time 0.974 (0.974) data 0.266 (0.266) loss 0.2512 (0.2512) acc 96.8750 (96.8750) lr 5.3915e-05 eta 0:00:57
epoch [181/200] batch [2/3] time 0.712 (0.843) data 0.000 (0.133) loss 0.2600 (0.2556) acc 93.7500 (95.3125) lr 5.3915e-05 eta 0:00:48
epoch [181/200] batch [3/3] time 0.715 (0.800) data 0.000 (0.089) loss 0.3938 (0.3017) acc 90.6250 (93.7500) lr 4.8943e-05 eta 0:00:45
epoch [182/200] batch [1/3] time 0.974 (0.974) data 0.266 (0.266) loss 0.2590 (0.2590) acc 96.8750 (96.8750) lr 4.8943e-05 eta 0:00:54
epoch [182/200] batch [2/3] time 0.712 (0.843) data 0.000 (0.133) loss 0.1772 (0.2181) acc 96.8750 (96.8750) lr 4.8943e-05 eta 0:00:46
epoch [182/200] batch [3/3] time 0.712 (0.800) data 0.000 (0.089) loss 0.3857 (0.2740) acc 87.5000 (93.7500) lr 4.4207e-05 eta 0:00:43
epoch [183/200] batch [1/3] time 0.970 (0.970) data 0.259 (0.259) loss 0.4597 (0.4597) acc 87.5000 (87.5000) lr 4.4207e-05 eta 0:00:51
epoch [183/200] batch [2/3] time 0.713 (0.841) data 0.000 (0.129) loss 0.5000 (0.4799) acc 87.5000 (87.5000) lr 4.4207e-05 eta 0:00:43
epoch [183/200] batch [3/3] time 0.714 (0.799) data 0.000 (0.086) loss 0.2734 (0.4111) acc 96.8750 (90.6250) lr 3.9706e-05 eta 0:00:40
epoch [184/200] batch [1/3] time 0.987 (0.987) data 0.275 (0.275) loss 0.1422 (0.1422) acc 96.8750 (96.8750) lr 3.9706e-05 eta 0:00:49
epoch [184/200] batch [2/3] time 0.712 (0.850) data 0.000 (0.138) loss 0.1842 (0.1632) acc 96.8750 (96.8750) lr 3.9706e-05 eta 0:00:41
epoch [184/200] batch [3/3] time 0.713 (0.804) data 0.000 (0.092) loss 0.4668 (0.2644) acc 90.6250 (94.7917) lr 3.5443e-05 eta 0:00:38
epoch [185/200] batch [1/3] time 0.986 (0.986) data 0.275 (0.275) loss 0.1478 (0.1478) acc 93.7500 (93.7500) lr 3.5443e-05 eta 0:00:46
epoch [185/200] batch [2/3] time 0.713 (0.849) data 0.000 (0.138) loss 0.5254 (0.3366) acc 87.5000 (90.6250) lr 3.5443e-05 eta 0:00:39
epoch [185/200] batch [3/3] time 0.713 (0.804) data 0.000 (0.092) loss 0.3684 (0.3472) acc 87.5000 (89.5833) lr 3.1417e-05 eta 0:00:36
epoch [186/200] batch [1/3] time 0.986 (0.986) data 0.273 (0.273) loss 0.2446 (0.2446) acc 96.8750 (96.8750) lr 3.1417e-05 eta 0:00:43
epoch [186/200] batch [2/3] time 0.711 (0.848) data 0.000 (0.137) loss 0.4373 (0.3409) acc 87.5000 (92.1875) lr 3.1417e-05 eta 0:00:36
epoch [186/200] batch [3/3] time 0.712 (0.803) data 0.000 (0.091) loss 0.6108 (0.4309) acc 84.3750 (89.5833) lr 2.7630e-05 eta 0:00:33
epoch [187/200] batch [1/3] time 0.981 (0.981) data 0.269 (0.269) loss 0.3425 (0.3425) acc 93.7500 (93.7500) lr 2.7630e-05 eta 0:00:40
epoch [187/200] batch [2/3] time 0.713 (0.847) data 0.000 (0.135) loss 0.4780 (0.4103) acc 84.3750 (89.0625) lr 2.7630e-05 eta 0:00:33
epoch [187/200] batch [3/3] time 0.712 (0.802) data 0.000 (0.090) loss 0.5088 (0.4431) acc 87.5000 (88.5417) lr 2.4083e-05 eta 0:00:31
epoch [188/200] batch [1/3] time 0.980 (0.980) data 0.267 (0.267) loss 0.1115 (0.1115) acc 96.8750 (96.8750) lr 2.4083e-05 eta 0:00:37
epoch [188/200] batch [2/3] time 0.712 (0.846) data 0.000 (0.133) loss 0.2373 (0.1744) acc 96.8750 (96.8750) lr 2.4083e-05 eta 0:00:31
epoch [188/200] batch [3/3] time 0.712 (0.801) data 0.000 (0.089) loss 0.2354 (0.1947) acc 96.8750 (96.8750) lr 2.0777e-05 eta 0:00:28
epoch [189/200] batch [1/3] time 0.986 (0.986) data 0.275 (0.275) loss 0.4602 (0.4602) acc 90.6250 (90.6250) lr 2.0777e-05 eta 0:00:34
epoch [189/200] batch [2/3] time 0.712 (0.849) data 0.000 (0.138) loss 0.4873 (0.4738) acc 87.5000 (89.0625) lr 2.0777e-05 eta 0:00:28
epoch [189/200] batch [3/3] time 0.712 (0.804) data 0.000 (0.092) loss 0.2539 (0.4005) acc 96.8750 (91.6667) lr 1.7713e-05 eta 0:00:26
epoch [190/200] batch [1/3] time 0.972 (0.972) data 0.259 (0.259) loss 0.1971 (0.1971) acc 90.6250 (90.6250) lr 1.7713e-05 eta 0:00:31
epoch [190/200] batch [2/3] time 0.712 (0.842) data 0.000 (0.130) loss 0.1865 (0.1918) acc 96.8750 (93.7500) lr 1.7713e-05 eta 0:00:26
epoch [190/200] batch [3/3] time 0.712 (0.799) data 0.000 (0.087) loss 0.5083 (0.2973) acc 90.6250 (92.7083) lr 1.4891e-05 eta 0:00:23
epoch [191/200] batch [1/3] time 0.980 (0.980) data 0.268 (0.268) loss 0.2996 (0.2996) acc 90.6250 (90.6250) lr 1.4891e-05 eta 0:00:28
epoch [191/200] batch [2/3] time 0.712 (0.846) data 0.000 (0.134) loss 0.1185 (0.2090) acc 96.8750 (93.7500) lr 1.4891e-05 eta 0:00:23
epoch [191/200] batch [3/3] time 0.713 (0.801) data 0.000 (0.089) loss 0.0525 (0.1569) acc 100.0000 (95.8333) lr 1.2312e-05 eta 0:00:21
epoch [192/200] batch [1/3] time 0.986 (0.986) data 0.274 (0.274) loss 0.2448 (0.2448) acc 96.8750 (96.8750) lr 1.2312e-05 eta 0:00:25
epoch [192/200] batch [2/3] time 0.712 (0.849) data 0.000 (0.137) loss 0.3242 (0.2845) acc 93.7500 (95.3125) lr 1.2312e-05 eta 0:00:21
epoch [192/200] batch [3/3] time 0.712 (0.804) data 0.000 (0.091) loss 0.2488 (0.2726) acc 90.6250 (93.7500) lr 9.9763e-06 eta 0:00:19
epoch [193/200] batch [1/3] time 0.965 (0.965) data 0.256 (0.256) loss 0.6895 (0.6895) acc 81.2500 (81.2500) lr 9.9763e-06 eta 0:00:22
epoch [193/200] batch [2/3] time 0.712 (0.838) data 0.000 (0.128) loss 0.3020 (0.4957) acc 90.6250 (85.9375) lr 9.9763e-06 eta 0:00:18
epoch [193/200] batch [3/3] time 0.713 (0.796) data 0.000 (0.085) loss 0.4041 (0.4652) acc 93.7500 (88.5417) lr 7.8853e-06 eta 0:00:16
epoch [194/200] batch [1/3] time 0.970 (0.970) data 0.262 (0.262) loss 0.4124 (0.4124) acc 90.6250 (90.6250) lr 7.8853e-06 eta 0:00:19
epoch [194/200] batch [2/3] time 0.712 (0.841) data 0.000 (0.131) loss 0.1394 (0.2759) acc 100.0000 (95.3125) lr 7.8853e-06 eta 0:00:15
epoch [194/200] batch [3/3] time 0.712 (0.798) data 0.000 (0.087) loss 0.4712 (0.3410) acc 87.5000 (92.7083) lr 6.0390e-06 eta 0:00:14
epoch [195/200] batch [1/3] time 0.967 (0.967) data 0.259 (0.259) loss 0.2278 (0.2278) acc 96.8750 (96.8750) lr 6.0390e-06 eta 0:00:16
epoch [195/200] batch [2/3] time 0.712 (0.840) data 0.000 (0.129) loss 0.1532 (0.1905) acc 96.8750 (96.8750) lr 6.0390e-06 eta 0:00:13
epoch [195/200] batch [3/3] time 0.713 (0.797) data 0.000 (0.086) loss 0.3938 (0.2583) acc 96.8750 (96.8750) lr 4.4380e-06 eta 0:00:11
epoch [196/200] batch [1/3] time 0.983 (0.983) data 0.273 (0.273) loss 0.3259 (0.3259) acc 93.7500 (93.7500) lr 4.4380e-06 eta 0:00:13
epoch [196/200] batch [2/3] time 0.712 (0.848) data 0.000 (0.137) loss 0.1101 (0.2180) acc 100.0000 (96.8750) lr 4.4380e-06 eta 0:00:11
epoch [196/200] batch [3/3] time 0.714 (0.803) data 0.000 (0.091) loss 0.1531 (0.1964) acc 96.8750 (96.8750) lr 3.0827e-06 eta 0:00:09
epoch [197/200] batch [1/3] time 0.970 (0.970) data 0.258 (0.258) loss 0.2920 (0.2920) acc 90.6250 (90.6250) lr 3.0827e-06 eta 0:00:10
epoch [197/200] batch [2/3] time 0.712 (0.841) data 0.000 (0.129) loss 0.0808 (0.1864) acc 100.0000 (95.3125) lr 3.0827e-06 eta 0:00:08
epoch [197/200] batch [3/3] time 0.713 (0.798) data 0.000 (0.086) loss 0.3738 (0.2489) acc 90.6250 (93.7500) lr 1.9733e-06 eta 0:00:07
epoch [198/200] batch [1/3] time 0.980 (0.980) data 0.269 (0.269) loss 0.4656 (0.4656) acc 84.3750 (84.3750) lr 1.9733e-06 eta 0:00:07
epoch [198/200] batch [2/3] time 0.714 (0.847) data 0.000 (0.134) loss 0.1298 (0.2977) acc 96.8750 (90.6250) lr 1.9733e-06 eta 0:00:05
epoch [198/200] batch [3/3] time 0.712 (0.802) data 0.000 (0.090) loss 0.2496 (0.2817) acc 93.7500 (91.6667) lr 1.1101e-06 eta 0:00:04
epoch [199/200] batch [1/3] time 0.977 (0.977) data 0.266 (0.266) loss 0.1935 (0.1935) acc 93.7500 (93.7500) lr 1.1101e-06 eta 0:00:04
epoch [199/200] batch [2/3] time 0.712 (0.845) data 0.000 (0.133) loss 0.1087 (0.1511) acc 100.0000 (96.8750) lr 1.1101e-06 eta 0:00:03
epoch [199/200] batch [3/3] time 0.713 (0.801) data 0.000 (0.089) loss 0.4895 (0.2639) acc 81.2500 (91.6667) lr 4.9344e-07 eta 0:00:02
epoch [200/200] batch [1/3] time 0.971 (0.971) data 0.258 (0.258) loss 0.3245 (0.3245) acc 93.7500 (93.7500) lr 4.9344e-07 eta 0:00:01
epoch [200/200] batch [2/3] time 0.712 (0.841) data 0.000 (0.129) loss 0.3340 (0.3292) acc 90.6250 (92.1875) lr 4.9344e-07 eta 0:00:00
epoch [200/200] batch [3/3] time 0.712 (0.798) data 0.000 (0.086) loss 0.1589 (0.2725) acc 96.8750 (93.7500) lr 1.2337e-07 eta 0:00:00
Checkpoint saved to output/Caltech/1/2/prompt_learner/model.pth.tar-200
Finish training
Deploy the last-epoch model
Evaluate on the *test* set
=> result
* total: 2,465
* correct: 1,999
* accuracy: 81.1%
* error: 18.9%
* macro_f1: 73.8%
Elapsed: 0:08:40
args2: backbone=, config_file=configs/trainers/CoOp/rn50.yaml, dataset_config_file=configs/datasets/caltech101.yaml, eval_only=False, head=, load_epoch=None, model_dir=, no_train=False,  opts: ['TRAINER.COOP.N_CTX', '16', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '1'], output_dir=output/Caltech/1/2/3, resume=, root=/home/brandnerkasper/Uni/MP/MP_CustomCoOp/data, seed=3, source_domains=None, target_domains=None, trainer=CoOp, transforms=None
Setting fixed seed: 3
***************
** Arguments **
***************
config_file: configs/trainers/CoOp/rn50.yaml
csc: False
ctp: end
dataset_config_file: configs/datasets/caltech101.yaml
n_ctx: 16
opts: ['TRAINER.COOP.N_CTX', '16', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '1']
output_dir: output/Caltech/1/2/3
root: /home/brandnerkasper/Uni/MP/MP_CustomCoOp/data
seed: 3
shots: 1
trainer: CoOp
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 4
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: Caltech101
  NUM_LABELED: -1
  NUM_SHOTS: 1
  ROOT: /home/brandnerkasper/Uni/MP/MP_CustomCoOp/data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 200
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/Caltech/1/2/3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: 
    N_CTX: 16
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: 
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: CoOp
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 2.0.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 22.04.3 LTS (x86_64)
GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.35

Python version: 3.10.12 (main, Jul  5 2023, 18:54:27) [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-6.2.0-32-generic-x86_64-with-glibc2.35
Is CUDA available: True
CUDA runtime version: 11.5.119
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA GeForce GTX 970
Nvidia driver version: 525.125.06
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Architecture:                       x86_64
CPU op-mode(s):                     32-bit, 64-bit
Address sizes:                      39 bits physical, 48 bits virtual
Byte Order:                         Little Endian
CPU(s):                             4
On-line CPU(s) list:                0-3
Vendor ID:                          GenuineIntel
Model name:                         Intel(R) Xeon(R) CPU E3-1225 v3 @ 3.20GHz
CPU family:                         6
Model:                              60
Thread(s) per core:                 1
Core(s) per socket:                 4
Socket(s):                          1
Stepping:                           3
CPU max MHz:                        3600,0000
CPU min MHz:                        800,0000
BogoMIPS:                           6397.95
Flags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm cpuid_fault invpcid_single pti ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid xsaveopt dtherm ida arat pln pts md_clear flush_l1d
Virtualization:                     VT-x
L1d cache:                          128 KiB (4 instances)
L1i cache:                          128 KiB (4 instances)
L2 cache:                           1 MiB (4 instances)
L3 cache:                           8 MiB (1 instance)
NUMA node(s):                       1
NUMA node0 CPU(s):                  0-3
Vulnerability Gather data sampling: Not affected
Vulnerability Itlb multihit:        KVM: Mitigation: VMX disabled
Vulnerability L1tf:                 Mitigation; PTE Inversion; VMX conditional cache flushes, SMT disabled
Vulnerability Mds:                  Mitigation; Clear CPU buffers; SMT disabled
Vulnerability Meltdown:             Mitigation; PTI
Vulnerability Mmio stale data:      Unknown: No mitigations
Vulnerability Retbleed:             Not affected
Vulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl
Vulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization
Vulnerability Spectre v2:           Mitigation; Retpolines, IBPB conditional, IBRS_FW, STIBP disabled, RSB filling, PBRSB-eIBRS Not affected
Vulnerability Srbds:                Mitigation; Microcode
Vulnerability Tsx async abort:      Not affected

Versions of relevant libraries:
[pip3] numpy==1.25.2
[pip3] open-clip-torch==2.20.0
[pip3] torch==2.0.1
[pip3] torchaudio==2.0.2
[pip3] torchvision==0.15.2
[conda] blas                      1.0                         mkl  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] mkl                       2023.1.0         h213fc3f_46343  
[conda] mkl-service               2.4.0           py310h5eee18b_1  
[conda] mkl_fft                   1.3.6           py310h1128e8f_1  
[conda] mkl_random                1.2.2           py310h1128e8f_1  
[conda] numpy                     1.25.2          py310h5f9d8c6_0  
[conda] numpy-base                1.25.2          py310hb5e798b_0  
[conda] open-clip-torch           2.20.0                   pypi_0    pypi
[conda] pytorch                   2.0.1           py3.10_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchaudio                2.0.2               py310_cu117    pytorch
[conda] torchtriton               2.0.0                     py310    pytorch
[conda] torchvision               0.15.2              py310_cu117    pytorch
        Pillow (9.4.0)

Loading trainer: CoOp
Loading dataset: Caltech101
Reading split from /home/brandnerkasper/Uni/MP/MP_CustomCoOp/data/caltech-101/split_zhou_Caltech101.json
Creating a 1-shot dataset
Creating a 1-shot dataset
Saving preprocessed few-shot data to /home/brandnerkasper/Uni/MP/MP_CustomCoOp/data/caltech-101/split_fewshot/shot_1-seed_3.pkl
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ----------
Dataset    Caltech101
# classes  100
# train_x  100
# val      100
# test     2,465
---------  ----------
Loading CLIP (backbone: RN50)
CLIP(
  (visual): ModifiedResNet(
    (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu1): ReLU(inplace=True)
    (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu2): ReLU(inplace=True)
    (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu3): ReLU(inplace=True)
    (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (layer1): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
        (downsample): Sequential(
          (-1): AvgPool2d(kernel_size=1, stride=1, padding=0)
          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
    )
    (layer2): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
        (downsample): Sequential(
          (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
    )
    (layer3): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
        (downsample): Sequential(
          (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
    )
    (layer4): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
        (downsample): Sequential(
          (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
    )
    (attnpool): AttentionPool2d(
      (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
      (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
      (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
      (c_proj): Linear(in_features=2048, out_features=1024, bias=True)
    )
  )
  (transformer): Transformer(
    (resblocks): Sequential(
      (0): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (6): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (7): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (8): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (9): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (10): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (11): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (token_embedding): Embedding(49408, 512)
  (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
Turning off gradients in both the image and the text encoder
Loading evaluator: Classification
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/Caltech/1/2/3/tensorboard)
epoch [1/200] batch [1/3] time 1.026 (1.026) data 0.292 (0.292) loss 2.8418 (2.8418) acc 37.5000 (37.5000) lr 1.0000e-05 eta 0:10:14
epoch [1/200] batch [2/3] time 0.709 (0.868) data 0.000 (0.146) loss 2.8086 (2.8252) acc 37.5000 (37.5000) lr 1.0000e-05 eta 0:08:38
epoch [1/200] batch [3/3] time 0.709 (0.815) data 0.000 (0.097) loss 2.3281 (2.6595) acc 46.8750 (40.6250) lr 2.0000e-03 eta 0:08:06
epoch [2/200] batch [1/3] time 0.991 (0.991) data 0.281 (0.281) loss 1.9707 (1.9707) acc 59.3750 (59.3750) lr 2.0000e-03 eta 0:09:50
epoch [2/200] batch [2/3] time 0.710 (0.850) data 0.000 (0.140) loss 2.0352 (2.0029) acc 46.8750 (53.1250) lr 2.0000e-03 eta 0:08:25
epoch [2/200] batch [3/3] time 0.712 (0.804) data 0.000 (0.094) loss 1.5977 (1.8678) acc 65.6250 (57.2917) lr 1.9999e-03 eta 0:07:57
epoch [3/200] batch [1/3] time 0.982 (0.982) data 0.273 (0.273) loss 2.1660 (2.1660) acc 53.1250 (53.1250) lr 1.9999e-03 eta 0:09:42
epoch [3/200] batch [2/3] time 0.710 (0.846) data 0.000 (0.136) loss 2.2871 (2.2266) acc 50.0000 (51.5625) lr 1.9999e-03 eta 0:08:20
epoch [3/200] batch [3/3] time 0.711 (0.801) data 0.000 (0.091) loss 2.3223 (2.2585) acc 46.8750 (50.0000) lr 1.9995e-03 eta 0:07:53
epoch [4/200] batch [1/3] time 0.976 (0.976) data 0.267 (0.267) loss 1.4062 (1.4062) acc 68.7500 (68.7500) lr 1.9995e-03 eta 0:09:35
epoch [4/200] batch [2/3] time 0.711 (0.843) data 0.000 (0.134) loss 1.7969 (1.6016) acc 68.7500 (68.7500) lr 1.9995e-03 eta 0:08:16
epoch [4/200] batch [3/3] time 0.711 (0.799) data 0.000 (0.089) loss 1.3408 (1.5146) acc 56.2500 (64.5833) lr 1.9989e-03 eta 0:07:49
epoch [5/200] batch [1/3] time 1.126 (1.126) data 0.418 (0.418) loss 1.8643 (1.8643) acc 59.3750 (59.3750) lr 1.9989e-03 eta 0:11:00
epoch [5/200] batch [2/3] time 0.711 (0.919) data 0.000 (0.209) loss 1.3887 (1.6265) acc 65.6250 (62.5000) lr 1.9989e-03 eta 0:08:58
epoch [5/200] batch [3/3] time 0.711 (0.850) data 0.000 (0.139) loss 1.6924 (1.6484) acc 59.3750 (61.4583) lr 1.9980e-03 eta 0:08:17
epoch [6/200] batch [1/3] time 1.110 (1.110) data 0.400 (0.400) loss 1.8867 (1.8867) acc 53.1250 (53.1250) lr 1.9980e-03 eta 0:10:48
epoch [6/200] batch [2/3] time 0.712 (0.911) data 0.000 (0.200) loss 1.1035 (1.4951) acc 68.7500 (60.9375) lr 1.9980e-03 eta 0:08:51
epoch [6/200] batch [3/3] time 0.712 (0.845) data 0.000 (0.134) loss 1.3086 (1.4329) acc 62.5000 (61.4583) lr 1.9969e-03 eta 0:08:11
epoch [7/200] batch [1/3] time 1.123 (1.123) data 0.415 (0.415) loss 0.8813 (0.8813) acc 75.0000 (75.0000) lr 1.9969e-03 eta 0:10:52
epoch [7/200] batch [2/3] time 0.712 (0.918) data 0.000 (0.208) loss 1.6221 (1.2517) acc 59.3750 (67.1875) lr 1.9969e-03 eta 0:08:52
epoch [7/200] batch [3/3] time 0.712 (0.849) data 0.000 (0.138) loss 1.5186 (1.3407) acc 59.3750 (64.5833) lr 1.9956e-03 eta 0:08:11
epoch [8/200] batch [1/3] time 1.124 (1.124) data 0.416 (0.416) loss 1.1348 (1.1348) acc 71.8750 (71.8750) lr 1.9956e-03 eta 0:10:49
epoch [8/200] batch [2/3] time 0.712 (0.918) data 0.000 (0.208) loss 1.3613 (1.2480) acc 65.6250 (68.7500) lr 1.9956e-03 eta 0:08:49
epoch [8/200] batch [3/3] time 0.712 (0.849) data 0.000 (0.139) loss 1.0977 (1.1979) acc 65.6250 (67.7083) lr 1.9940e-03 eta 0:08:09
epoch [9/200] batch [1/3] time 1.117 (1.117) data 0.409 (0.409) loss 0.9072 (0.9072) acc 75.0000 (75.0000) lr 1.9940e-03 eta 0:10:42
epoch [9/200] batch [2/3] time 0.712 (0.915) data 0.000 (0.205) loss 1.0215 (0.9644) acc 65.6250 (70.3125) lr 1.9940e-03 eta 0:08:44
epoch [9/200] batch [3/3] time 0.712 (0.847) data 0.000 (0.136) loss 1.5459 (1.1582) acc 59.3750 (66.6667) lr 1.9921e-03 eta 0:08:05
epoch [10/200] batch [1/3] time 1.121 (1.121) data 0.408 (0.408) loss 1.1250 (1.1250) acc 71.8750 (71.8750) lr 1.9921e-03 eta 0:10:41
epoch [10/200] batch [2/3] time 0.712 (0.916) data 0.000 (0.204) loss 1.2275 (1.1763) acc 62.5000 (67.1875) lr 1.9921e-03 eta 0:08:43
epoch [10/200] batch [3/3] time 0.713 (0.848) data 0.000 (0.136) loss 1.0967 (1.1497) acc 65.6250 (66.6667) lr 1.9900e-03 eta 0:08:03
epoch [11/200] batch [1/3] time 1.110 (1.110) data 0.399 (0.399) loss 1.2812 (1.2812) acc 71.8750 (71.8750) lr 1.9900e-03 eta 0:10:31
epoch [11/200] batch [2/3] time 0.712 (0.911) data 0.000 (0.200) loss 0.8828 (1.0820) acc 78.1250 (75.0000) lr 1.9900e-03 eta 0:08:37
epoch [11/200] batch [3/3] time 0.712 (0.845) data 0.000 (0.133) loss 1.3701 (1.1781) acc 65.6250 (71.8750) lr 1.9877e-03 eta 0:07:58
epoch [12/200] batch [1/3] time 1.132 (1.132) data 0.423 (0.423) loss 0.8442 (0.8442) acc 78.1250 (78.1250) lr 1.9877e-03 eta 0:10:40
epoch [12/200] batch [2/3] time 0.712 (0.922) data 0.000 (0.212) loss 0.8345 (0.8394) acc 81.2500 (79.6875) lr 1.9877e-03 eta 0:08:40
epoch [12/200] batch [3/3] time 0.712 (0.852) data 0.000 (0.141) loss 1.0957 (0.9248) acc 56.2500 (71.8750) lr 1.9851e-03 eta 0:08:00
epoch [13/200] batch [1/3] time 1.106 (1.106) data 0.398 (0.398) loss 0.6196 (0.6196) acc 84.3750 (84.3750) lr 1.9851e-03 eta 0:10:22
epoch [13/200] batch [2/3] time 0.716 (0.911) data 0.000 (0.199) loss 1.1016 (0.8606) acc 68.7500 (76.5625) lr 1.9851e-03 eta 0:08:31
epoch [13/200] batch [3/3] time 0.712 (0.845) data 0.000 (0.133) loss 1.1240 (0.9484) acc 71.8750 (75.0000) lr 1.9823e-03 eta 0:07:53
epoch [14/200] batch [1/3] time 1.124 (1.124) data 0.412 (0.412) loss 0.8545 (0.8545) acc 75.0000 (75.0000) lr 1.9823e-03 eta 0:10:29
epoch [14/200] batch [2/3] time 0.712 (0.918) data 0.000 (0.206) loss 1.1084 (0.9814) acc 71.8750 (73.4375) lr 1.9823e-03 eta 0:08:33
epoch [14/200] batch [3/3] time 0.712 (0.850) data 0.000 (0.137) loss 1.3984 (1.1204) acc 59.3750 (68.7500) lr 1.9792e-03 eta 0:07:54
epoch [15/200] batch [1/3] time 1.110 (1.110) data 0.399 (0.399) loss 0.8804 (0.8804) acc 78.1250 (78.1250) lr 1.9792e-03 eta 0:10:18
epoch [15/200] batch [2/3] time 0.712 (0.911) data 0.000 (0.200) loss 0.7124 (0.7964) acc 68.7500 (73.4375) lr 1.9792e-03 eta 0:08:26
epoch [15/200] batch [3/3] time 0.712 (0.845) data 0.000 (0.133) loss 0.8911 (0.8280) acc 71.8750 (72.9167) lr 1.9759e-03 eta 0:07:48
epoch [16/200] batch [1/3] time 1.128 (1.128) data 0.419 (0.419) loss 0.6753 (0.6753) acc 81.2500 (81.2500) lr 1.9759e-03 eta 0:10:25
epoch [16/200] batch [2/3] time 0.712 (0.920) data 0.000 (0.209) loss 0.6997 (0.6875) acc 75.0000 (78.1250) lr 1.9759e-03 eta 0:08:28
epoch [16/200] batch [3/3] time 0.712 (0.851) data 0.000 (0.140) loss 0.5679 (0.6476) acc 81.2500 (79.1667) lr 1.9724e-03 eta 0:07:49
epoch [17/200] batch [1/3] time 1.107 (1.107) data 0.399 (0.399) loss 1.6357 (1.6357) acc 43.7500 (43.7500) lr 1.9724e-03 eta 0:10:10
epoch [17/200] batch [2/3] time 0.712 (0.910) data 0.000 (0.199) loss 0.7129 (1.1743) acc 84.3750 (64.0625) lr 1.9724e-03 eta 0:08:20
epoch [17/200] batch [3/3] time 0.712 (0.844) data 0.000 (0.133) loss 0.4570 (0.9352) acc 81.2500 (69.7917) lr 1.9686e-03 eta 0:07:43
epoch [18/200] batch [1/3] time 1.109 (1.109) data 0.400 (0.400) loss 0.8750 (0.8750) acc 65.6250 (65.6250) lr 1.9686e-03 eta 0:10:07
epoch [18/200] batch [2/3] time 0.712 (0.910) data 0.000 (0.200) loss 0.6167 (0.7458) acc 84.3750 (75.0000) lr 1.9686e-03 eta 0:08:17
epoch [18/200] batch [3/3] time 0.712 (0.844) data 0.000 (0.133) loss 0.5591 (0.6836) acc 78.1250 (76.0417) lr 1.9646e-03 eta 0:07:40
epoch [19/200] batch [1/3] time 1.111 (1.111) data 0.400 (0.400) loss 0.7437 (0.7437) acc 75.0000 (75.0000) lr 1.9646e-03 eta 0:10:05
epoch [19/200] batch [2/3] time 0.712 (0.911) data 0.000 (0.200) loss 1.3252 (1.0344) acc 75.0000 (75.0000) lr 1.9646e-03 eta 0:08:15
epoch [19/200] batch [3/3] time 0.712 (0.845) data 0.000 (0.133) loss 1.0410 (1.0366) acc 68.7500 (72.9167) lr 1.9603e-03 eta 0:07:38
epoch [20/200] batch [1/3] time 1.130 (1.130) data 0.418 (0.418) loss 1.2178 (1.2178) acc 65.6250 (65.6250) lr 1.9603e-03 eta 0:10:12
epoch [20/200] batch [2/3] time 0.713 (0.921) data 0.000 (0.209) loss 0.6812 (0.9495) acc 81.2500 (73.4375) lr 1.9603e-03 eta 0:08:18
epoch [20/200] batch [3/3] time 0.712 (0.852) data 0.000 (0.139) loss 1.5986 (1.1659) acc 62.5000 (69.7917) lr 1.9558e-03 eta 0:07:39
epoch [21/200] batch [1/3] time 1.124 (1.124) data 0.415 (0.415) loss 0.7852 (0.7852) acc 81.2500 (81.2500) lr 1.9558e-03 eta 0:10:05
epoch [21/200] batch [2/3] time 0.712 (0.918) data 0.000 (0.207) loss 1.2861 (1.0356) acc 71.8750 (76.5625) lr 1.9558e-03 eta 0:08:13
epoch [21/200] batch [3/3] time 0.712 (0.849) data 0.000 (0.138) loss 0.8408 (0.9707) acc 75.0000 (76.0417) lr 1.9511e-03 eta 0:07:35
epoch [22/200] batch [1/3] time 1.111 (1.111) data 0.402 (0.402) loss 1.0098 (1.0098) acc 68.7500 (68.7500) lr 1.9511e-03 eta 0:09:55
epoch [22/200] batch [2/3] time 0.712 (0.912) data 0.000 (0.201) loss 0.3369 (0.6733) acc 93.7500 (81.2500) lr 1.9511e-03 eta 0:08:07
epoch [22/200] batch [3/3] time 0.712 (0.845) data 0.000 (0.134) loss 1.1826 (0.8431) acc 62.5000 (75.0000) lr 1.9461e-03 eta 0:07:31
epoch [23/200] batch [1/3] time 1.116 (1.116) data 0.407 (0.407) loss 0.4067 (0.4067) acc 90.6250 (90.6250) lr 1.9461e-03 eta 0:09:54
epoch [23/200] batch [2/3] time 0.712 (0.914) data 0.000 (0.204) loss 0.5107 (0.4587) acc 81.2500 (85.9375) lr 1.9461e-03 eta 0:08:06
epoch [23/200] batch [3/3] time 0.714 (0.847) data 0.000 (0.136) loss 0.5371 (0.4849) acc 87.5000 (86.4583) lr 1.9409e-03 eta 0:07:30
epoch [24/200] batch [1/3] time 1.119 (1.119) data 0.407 (0.407) loss 0.8213 (0.8213) acc 75.0000 (75.0000) lr 1.9409e-03 eta 0:09:53
epoch [24/200] batch [2/3] time 0.712 (0.916) data 0.000 (0.204) loss 0.9561 (0.8887) acc 78.1250 (76.5625) lr 1.9409e-03 eta 0:08:04
epoch [24/200] batch [3/3] time 0.712 (0.848) data 0.000 (0.136) loss 1.0508 (0.9427) acc 75.0000 (76.0417) lr 1.9354e-03 eta 0:07:27
epoch [25/200] batch [1/3] time 1.128 (1.128) data 0.416 (0.416) loss 0.6455 (0.6455) acc 78.1250 (78.1250) lr 1.9354e-03 eta 0:09:54
epoch [25/200] batch [2/3] time 0.712 (0.920) data 0.000 (0.208) loss 1.0186 (0.8320) acc 71.8750 (75.0000) lr 1.9354e-03 eta 0:08:03
epoch [25/200] batch [3/3] time 0.712 (0.851) data 0.000 (0.139) loss 1.0000 (0.8880) acc 81.2500 (77.0833) lr 1.9298e-03 eta 0:07:26
epoch [26/200] batch [1/3] time 1.113 (1.113) data 0.400 (0.400) loss 0.5591 (0.5591) acc 81.2500 (81.2500) lr 1.9298e-03 eta 0:09:43
epoch [26/200] batch [2/3] time 0.712 (0.913) data 0.000 (0.200) loss 0.9639 (0.7615) acc 78.1250 (79.6875) lr 1.9298e-03 eta 0:07:57
epoch [26/200] batch [3/3] time 0.712 (0.846) data 0.000 (0.133) loss 0.5415 (0.6882) acc 84.3750 (81.2500) lr 1.9239e-03 eta 0:07:21
epoch [27/200] batch [1/3] time 1.115 (1.115) data 0.407 (0.407) loss 0.5015 (0.5015) acc 81.2500 (81.2500) lr 1.9239e-03 eta 0:09:41
epoch [27/200] batch [2/3] time 0.712 (0.914) data 0.000 (0.203) loss 1.6943 (1.0979) acc 65.6250 (73.4375) lr 1.9239e-03 eta 0:07:55
epoch [27/200] batch [3/3] time 0.712 (0.847) data 0.000 (0.136) loss 0.9883 (1.0614) acc 75.0000 (73.9583) lr 1.9178e-03 eta 0:07:19
epoch [28/200] batch [1/3] time 1.123 (1.123) data 0.415 (0.415) loss 0.6729 (0.6729) acc 84.3750 (84.3750) lr 1.9178e-03 eta 0:09:41
epoch [28/200] batch [2/3] time 0.712 (0.917) data 0.000 (0.207) loss 0.6318 (0.6523) acc 84.3750 (84.3750) lr 1.9178e-03 eta 0:07:54
epoch [28/200] batch [3/3] time 0.712 (0.849) data 0.000 (0.138) loss 0.7959 (0.7002) acc 78.1250 (82.2917) lr 1.9114e-03 eta 0:07:18
epoch [29/200] batch [1/3] time 0.993 (0.993) data 0.281 (0.281) loss 1.2451 (1.2451) acc 62.5000 (62.5000) lr 1.9114e-03 eta 0:08:31
epoch [29/200] batch [2/3] time 0.712 (0.852) data 0.000 (0.141) loss 0.8350 (1.0400) acc 81.2500 (71.8750) lr 1.9114e-03 eta 0:07:18
epoch [29/200] batch [3/3] time 0.712 (0.805) data 0.000 (0.094) loss 0.6685 (0.9162) acc 78.1250 (73.9583) lr 1.9048e-03 eta 0:06:53
epoch [30/200] batch [1/3] time 0.992 (0.992) data 0.282 (0.282) loss 1.1279 (1.1279) acc 68.7500 (68.7500) lr 1.9048e-03 eta 0:08:27
epoch [30/200] batch [2/3] time 0.712 (0.852) data 0.000 (0.141) loss 0.9297 (1.0288) acc 81.2500 (75.0000) lr 1.9048e-03 eta 0:07:15
epoch [30/200] batch [3/3] time 0.712 (0.805) data 0.000 (0.094) loss 0.8569 (0.9715) acc 75.0000 (75.0000) lr 1.8980e-03 eta 0:06:50
epoch [31/200] batch [1/3] time 0.984 (0.984) data 0.275 (0.275) loss 0.7227 (0.7227) acc 81.2500 (81.2500) lr 1.8980e-03 eta 0:08:20
epoch [31/200] batch [2/3] time 0.712 (0.848) data 0.000 (0.137) loss 0.6543 (0.6885) acc 81.2500 (81.2500) lr 1.8980e-03 eta 0:07:10
epoch [31/200] batch [3/3] time 0.712 (0.803) data 0.000 (0.092) loss 0.6909 (0.6893) acc 78.1250 (80.2083) lr 1.8910e-03 eta 0:06:46
epoch [32/200] batch [1/3] time 0.975 (0.975) data 0.266 (0.266) loss 0.5635 (0.5635) acc 90.6250 (90.6250) lr 1.8910e-03 eta 0:08:13
epoch [32/200] batch [2/3] time 0.712 (0.843) data 0.000 (0.133) loss 0.5913 (0.5774) acc 84.3750 (87.5000) lr 1.8910e-03 eta 0:07:05
epoch [32/200] batch [3/3] time 0.712 (0.800) data 0.000 (0.089) loss 0.8628 (0.6725) acc 75.0000 (83.3333) lr 1.8838e-03 eta 0:06:42
epoch [33/200] batch [1/3] time 0.998 (0.998) data 0.289 (0.289) loss 0.6260 (0.6260) acc 81.2500 (81.2500) lr 1.8838e-03 eta 0:08:21
epoch [33/200] batch [2/3] time 0.712 (0.855) data 0.000 (0.145) loss 0.6387 (0.6323) acc 78.1250 (79.6875) lr 1.8838e-03 eta 0:07:09
epoch [33/200] batch [3/3] time 0.712 (0.807) data 0.000 (0.097) loss 0.5518 (0.6055) acc 78.1250 (79.1667) lr 1.8763e-03 eta 0:06:44
epoch [34/200] batch [1/3] time 0.992 (0.992) data 0.280 (0.280) loss 1.0693 (1.0693) acc 71.8750 (71.8750) lr 1.8763e-03 eta 0:08:16
epoch [34/200] batch [2/3] time 0.712 (0.852) data 0.000 (0.140) loss 0.5049 (0.7871) acc 81.2500 (76.5625) lr 1.8763e-03 eta 0:07:05
epoch [34/200] batch [3/3] time 0.712 (0.805) data 0.000 (0.094) loss 0.9023 (0.8255) acc 81.2500 (78.1250) lr 1.8686e-03 eta 0:06:41
epoch [35/200] batch [1/3] time 1.082 (1.082) data 0.374 (0.374) loss 0.7271 (0.7271) acc 81.2500 (81.2500) lr 1.8686e-03 eta 0:08:57
epoch [35/200] batch [2/3] time 0.712 (0.897) data 0.000 (0.187) loss 0.6831 (0.7051) acc 78.1250 (79.6875) lr 1.8686e-03 eta 0:07:25
epoch [35/200] batch [3/3] time 0.712 (0.835) data 0.000 (0.125) loss 0.9155 (0.7752) acc 75.0000 (78.1250) lr 1.8607e-03 eta 0:06:53
epoch [36/200] batch [1/3] time 0.976 (0.976) data 0.265 (0.265) loss 0.5908 (0.5908) acc 84.3750 (84.3750) lr 1.8607e-03 eta 0:08:02
epoch [36/200] batch [2/3] time 0.712 (0.844) data 0.000 (0.133) loss 0.9653 (0.7781) acc 78.1250 (81.2500) lr 1.8607e-03 eta 0:06:56
epoch [36/200] batch [3/3] time 0.712 (0.800) data 0.000 (0.088) loss 0.7734 (0.7765) acc 75.0000 (79.1667) lr 1.8526e-03 eta 0:06:33
epoch [37/200] batch [1/3] time 0.990 (0.990) data 0.279 (0.279) loss 0.9155 (0.9155) acc 71.8750 (71.8750) lr 1.8526e-03 eta 0:08:06
epoch [37/200] batch [2/3] time 0.712 (0.851) data 0.000 (0.139) loss 0.8115 (0.8635) acc 75.0000 (73.4375) lr 1.8526e-03 eta 0:06:56
epoch [37/200] batch [3/3] time 0.713 (0.805) data 0.000 (0.093) loss 0.7480 (0.8250) acc 81.2500 (76.0417) lr 1.8443e-03 eta 0:06:33
epoch [38/200] batch [1/3] time 0.986 (0.986) data 0.275 (0.275) loss 0.6045 (0.6045) acc 78.1250 (78.1250) lr 1.8443e-03 eta 0:08:01
epoch [38/200] batch [2/3] time 0.711 (0.849) data 0.000 (0.137) loss 0.4270 (0.5157) acc 90.6250 (84.3750) lr 1.8443e-03 eta 0:06:53
epoch [38/200] batch [3/3] time 0.712 (0.803) data 0.000 (0.092) loss 0.6899 (0.5738) acc 84.3750 (84.3750) lr 1.8358e-03 eta 0:06:30
epoch [39/200] batch [1/3] time 0.977 (0.977) data 0.265 (0.265) loss 0.5444 (0.5444) acc 87.5000 (87.5000) lr 1.8358e-03 eta 0:07:53
epoch [39/200] batch [2/3] time 0.712 (0.844) data 0.000 (0.133) loss 0.7109 (0.6277) acc 84.3750 (85.9375) lr 1.8358e-03 eta 0:06:48
epoch [39/200] batch [3/3] time 0.712 (0.800) data 0.000 (0.088) loss 0.7056 (0.6536) acc 84.3750 (85.4167) lr 1.8271e-03 eta 0:06:26
epoch [40/200] batch [1/3] time 0.994 (0.994) data 0.283 (0.283) loss 0.8296 (0.8296) acc 68.7500 (68.7500) lr 1.8271e-03 eta 0:07:59
epoch [40/200] batch [2/3] time 0.711 (0.853) data 0.000 (0.141) loss 0.6001 (0.7148) acc 87.5000 (78.1250) lr 1.8271e-03 eta 0:06:50
epoch [40/200] batch [3/3] time 0.713 (0.806) data 0.000 (0.094) loss 1.0586 (0.8294) acc 78.1250 (78.1250) lr 1.8181e-03 eta 0:06:27
epoch [41/200] batch [1/3] time 0.983 (0.983) data 0.272 (0.272) loss 0.6084 (0.6084) acc 75.0000 (75.0000) lr 1.8181e-03 eta 0:07:51
epoch [41/200] batch [2/3] time 0.712 (0.848) data 0.000 (0.136) loss 1.2217 (0.9150) acc 75.0000 (75.0000) lr 1.8181e-03 eta 0:06:45
epoch [41/200] batch [3/3] time 0.712 (0.803) data 0.000 (0.091) loss 0.8232 (0.8844) acc 84.3750 (78.1250) lr 1.8090e-03 eta 0:06:22
epoch [42/200] batch [1/3] time 0.985 (0.985) data 0.274 (0.274) loss 0.6050 (0.6050) acc 81.2500 (81.2500) lr 1.8090e-03 eta 0:07:49
epoch [42/200] batch [2/3] time 0.713 (0.849) data 0.000 (0.137) loss 0.8535 (0.7292) acc 84.3750 (82.8125) lr 1.8090e-03 eta 0:06:43
epoch [42/200] batch [3/3] time 0.713 (0.804) data 0.000 (0.091) loss 0.7153 (0.7246) acc 78.1250 (81.2500) lr 1.7997e-03 eta 0:06:20
epoch [43/200] batch [1/3] time 0.977 (0.977) data 0.264 (0.264) loss 1.2568 (1.2568) acc 65.6250 (65.6250) lr 1.7997e-03 eta 0:07:42
epoch [43/200] batch [2/3] time 0.712 (0.845) data 0.000 (0.132) loss 0.6274 (0.9421) acc 84.3750 (75.0000) lr 1.7997e-03 eta 0:06:38
epoch [43/200] batch [3/3] time 0.712 (0.800) data 0.000 (0.088) loss 0.9053 (0.9299) acc 75.0000 (75.0000) lr 1.7902e-03 eta 0:06:16
epoch [44/200] batch [1/3] time 0.996 (0.996) data 0.285 (0.285) loss 0.5850 (0.5850) acc 84.3750 (84.3750) lr 1.7902e-03 eta 0:07:48
epoch [44/200] batch [2/3] time 0.712 (0.854) data 0.000 (0.143) loss 0.7129 (0.6489) acc 87.5000 (85.9375) lr 1.7902e-03 eta 0:06:40
epoch [44/200] batch [3/3] time 0.712 (0.807) data 0.000 (0.095) loss 0.9082 (0.7354) acc 71.8750 (81.2500) lr 1.7804e-03 eta 0:06:17
epoch [45/200] batch [1/3] time 0.977 (0.977) data 0.264 (0.264) loss 0.4297 (0.4297) acc 87.5000 (87.5000) lr 1.7804e-03 eta 0:07:36
epoch [45/200] batch [2/3] time 0.712 (0.845) data 0.000 (0.132) loss 0.8032 (0.6165) acc 71.8750 (79.6875) lr 1.7804e-03 eta 0:06:33
epoch [45/200] batch [3/3] time 0.712 (0.801) data 0.000 (0.088) loss 0.8428 (0.6919) acc 81.2500 (80.2083) lr 1.7705e-03 eta 0:06:12
epoch [46/200] batch [1/3] time 0.984 (0.984) data 0.272 (0.272) loss 0.7964 (0.7964) acc 78.1250 (78.1250) lr 1.7705e-03 eta 0:07:36
epoch [46/200] batch [2/3] time 0.712 (0.848) data 0.000 (0.136) loss 0.6748 (0.7356) acc 84.3750 (81.2500) lr 1.7705e-03 eta 0:06:32
epoch [46/200] batch [3/3] time 0.712 (0.803) data 0.000 (0.091) loss 0.4846 (0.6519) acc 90.6250 (84.3750) lr 1.7604e-03 eta 0:06:10
epoch [47/200] batch [1/3] time 0.986 (0.986) data 0.273 (0.273) loss 0.2979 (0.2979) acc 90.6250 (90.6250) lr 1.7604e-03 eta 0:07:34
epoch [47/200] batch [2/3] time 0.712 (0.849) data 0.000 (0.136) loss 0.9536 (0.6257) acc 75.0000 (82.8125) lr 1.7604e-03 eta 0:06:30
epoch [47/200] batch [3/3] time 0.712 (0.804) data 0.000 (0.091) loss 0.4160 (0.5558) acc 84.3750 (83.3333) lr 1.7501e-03 eta 0:06:08
epoch [48/200] batch [1/3] time 0.988 (0.988) data 0.276 (0.276) loss 0.5396 (0.5396) acc 87.5000 (87.5000) lr 1.7501e-03 eta 0:07:32
epoch [48/200] batch [2/3] time 0.713 (0.850) data 0.000 (0.138) loss 1.0713 (0.8054) acc 68.7500 (78.1250) lr 1.7501e-03 eta 0:06:28
epoch [48/200] batch [3/3] time 0.713 (0.805) data 0.000 (0.092) loss 0.8867 (0.8325) acc 75.0000 (77.0833) lr 1.7396e-03 eta 0:06:06
epoch [49/200] batch [1/3] time 0.991 (0.991) data 0.282 (0.282) loss 1.0527 (1.0527) acc 71.8750 (71.8750) lr 1.7396e-03 eta 0:07:30
epoch [49/200] batch [2/3] time 0.714 (0.852) data 0.000 (0.141) loss 1.1064 (1.0796) acc 75.0000 (73.4375) lr 1.7396e-03 eta 0:06:26
epoch [49/200] batch [3/3] time 0.714 (0.806) data 0.000 (0.094) loss 0.6924 (0.9505) acc 78.1250 (75.0000) lr 1.7290e-03 eta 0:06:05
epoch [50/200] batch [1/3] time 0.984 (0.984) data 0.275 (0.275) loss 0.7520 (0.7520) acc 75.0000 (75.0000) lr 1.7290e-03 eta 0:07:24
epoch [50/200] batch [2/3] time 0.713 (0.849) data 0.000 (0.138) loss 0.3904 (0.5712) acc 90.6250 (82.8125) lr 1.7290e-03 eta 0:06:22
epoch [50/200] batch [3/3] time 0.712 (0.803) data 0.000 (0.092) loss 0.5405 (0.5610) acc 87.5000 (84.3750) lr 1.7181e-03 eta 0:06:01
epoch [51/200] batch [1/3] time 0.973 (0.973) data 0.265 (0.265) loss 0.5029 (0.5029) acc 87.5000 (87.5000) lr 1.7181e-03 eta 0:07:17
epoch [51/200] batch [2/3] time 0.712 (0.843) data 0.000 (0.132) loss 0.6743 (0.5886) acc 78.1250 (82.8125) lr 1.7181e-03 eta 0:06:17
epoch [51/200] batch [3/3] time 0.715 (0.800) data 0.000 (0.088) loss 0.8696 (0.6823) acc 71.8750 (79.1667) lr 1.7071e-03 eta 0:05:57
epoch [52/200] batch [1/3] time 0.977 (0.977) data 0.265 (0.265) loss 0.4658 (0.4658) acc 84.3750 (84.3750) lr 1.7071e-03 eta 0:07:15
epoch [52/200] batch [2/3] time 0.712 (0.845) data 0.000 (0.133) loss 0.4475 (0.4567) acc 90.6250 (87.5000) lr 1.7071e-03 eta 0:06:15
epoch [52/200] batch [3/3] time 0.712 (0.801) data 0.000 (0.089) loss 0.4456 (0.4530) acc 87.5000 (87.5000) lr 1.6959e-03 eta 0:05:55
epoch [53/200] batch [1/3] time 0.988 (0.988) data 0.275 (0.275) loss 0.9453 (0.9453) acc 75.0000 (75.0000) lr 1.6959e-03 eta 0:07:17
epoch [53/200] batch [2/3] time 0.712 (0.850) data 0.000 (0.137) loss 0.7729 (0.8591) acc 78.1250 (76.5625) lr 1.6959e-03 eta 0:06:15
epoch [53/200] batch [3/3] time 0.712 (0.804) data 0.000 (0.092) loss 0.4824 (0.7336) acc 90.6250 (81.2500) lr 1.6845e-03 eta 0:05:54
epoch [54/200] batch [1/3] time 0.991 (0.991) data 0.279 (0.279) loss 0.4832 (0.4832) acc 90.6250 (90.6250) lr 1.6845e-03 eta 0:07:16
epoch [54/200] batch [2/3] time 0.713 (0.852) data 0.000 (0.140) loss 0.2930 (0.3881) acc 96.8750 (93.7500) lr 1.6845e-03 eta 0:06:14
epoch [54/200] batch [3/3] time 0.712 (0.805) data 0.000 (0.093) loss 0.2155 (0.3305) acc 93.7500 (93.7500) lr 1.6730e-03 eta 0:05:52
epoch [55/200] batch [1/3] time 0.978 (0.978) data 0.265 (0.265) loss 0.6387 (0.6387) acc 78.1250 (78.1250) lr 1.6730e-03 eta 0:07:07
epoch [55/200] batch [2/3] time 0.712 (0.845) data 0.000 (0.132) loss 0.7847 (0.7117) acc 81.2500 (79.6875) lr 1.6730e-03 eta 0:06:08
epoch [55/200] batch [3/3] time 0.714 (0.801) data 0.000 (0.088) loss 0.5059 (0.6431) acc 87.5000 (82.2917) lr 1.6613e-03 eta 0:05:48
epoch [56/200] batch [1/3] time 0.993 (0.993) data 0.282 (0.282) loss 0.8706 (0.8706) acc 81.2500 (81.2500) lr 1.6613e-03 eta 0:07:10
epoch [56/200] batch [2/3] time 0.712 (0.852) data 0.000 (0.141) loss 0.9351 (0.9028) acc 71.8750 (76.5625) lr 1.6613e-03 eta 0:06:09
epoch [56/200] batch [3/3] time 0.712 (0.806) data 0.000 (0.094) loss 0.6123 (0.8060) acc 81.2500 (78.1250) lr 1.6494e-03 eta 0:05:48
epoch [57/200] batch [1/3] time 0.987 (0.987) data 0.274 (0.274) loss 0.5425 (0.5425) acc 87.5000 (87.5000) lr 1.6494e-03 eta 0:07:05
epoch [57/200] batch [2/3] time 0.713 (0.850) data 0.000 (0.137) loss 0.6562 (0.5994) acc 81.2500 (84.3750) lr 1.6494e-03 eta 0:06:05
epoch [57/200] batch [3/3] time 0.713 (0.804) data 0.000 (0.091) loss 0.7949 (0.6646) acc 78.1250 (82.2917) lr 1.6374e-03 eta 0:05:45
epoch [58/200] batch [1/3] time 0.995 (0.995) data 0.284 (0.284) loss 0.3357 (0.3357) acc 87.5000 (87.5000) lr 1.6374e-03 eta 0:07:05
epoch [58/200] batch [2/3] time 0.712 (0.854) data 0.000 (0.142) loss 0.4182 (0.3770) acc 87.5000 (87.5000) lr 1.6374e-03 eta 0:06:04
epoch [58/200] batch [3/3] time 0.713 (0.807) data 0.000 (0.095) loss 0.3494 (0.3678) acc 93.7500 (89.5833) lr 1.6252e-03 eta 0:05:43
epoch [59/200] batch [1/3] time 0.990 (0.990) data 0.277 (0.277) loss 0.5352 (0.5352) acc 87.5000 (87.5000) lr 1.6252e-03 eta 0:07:00
epoch [59/200] batch [2/3] time 0.712 (0.851) data 0.000 (0.139) loss 0.8711 (0.7031) acc 68.7500 (78.1250) lr 1.6252e-03 eta 0:06:00
epoch [59/200] batch [3/3] time 0.713 (0.805) data 0.000 (0.092) loss 1.0195 (0.8086) acc 75.0000 (77.0833) lr 1.6129e-03 eta 0:05:40
epoch [60/200] batch [1/3] time 0.988 (0.988) data 0.277 (0.277) loss 1.3691 (1.3691) acc 68.7500 (68.7500) lr 1.6129e-03 eta 0:06:56
epoch [60/200] batch [2/3] time 0.713 (0.850) data 0.000 (0.138) loss 0.4097 (0.8894) acc 87.5000 (78.1250) lr 1.6129e-03 eta 0:05:58
epoch [60/200] batch [3/3] time 0.712 (0.804) data 0.000 (0.092) loss 0.6963 (0.8250) acc 87.5000 (81.2500) lr 1.6004e-03 eta 0:05:37
epoch [61/200] batch [1/3] time 0.987 (0.987) data 0.274 (0.274) loss 0.9531 (0.9531) acc 78.1250 (78.1250) lr 1.6004e-03 eta 0:06:53
epoch [61/200] batch [2/3] time 0.712 (0.849) data 0.000 (0.137) loss 0.9131 (0.9331) acc 71.8750 (75.0000) lr 1.6004e-03 eta 0:05:55
epoch [61/200] batch [3/3] time 0.712 (0.804) data 0.000 (0.091) loss 0.7783 (0.8815) acc 81.2500 (77.0833) lr 1.5878e-03 eta 0:05:35
epoch [62/200] batch [1/3] time 0.997 (0.997) data 0.286 (0.286) loss 0.7886 (0.7886) acc 75.0000 (75.0000) lr 1.5878e-03 eta 0:06:54
epoch [62/200] batch [2/3] time 0.712 (0.855) data 0.000 (0.143) loss 0.5088 (0.6487) acc 90.6250 (82.8125) lr 1.5878e-03 eta 0:05:54
epoch [62/200] batch [3/3] time 0.712 (0.807) data 0.000 (0.095) loss 0.5107 (0.6027) acc 84.3750 (83.3333) lr 1.5750e-03 eta 0:05:34
epoch [63/200] batch [1/3] time 0.976 (0.976) data 0.265 (0.265) loss 0.4004 (0.4004) acc 90.6250 (90.6250) lr 1.5750e-03 eta 0:06:43
epoch [63/200] batch [2/3] time 0.714 (0.845) data 0.000 (0.133) loss 0.7988 (0.5996) acc 84.3750 (87.5000) lr 1.5750e-03 eta 0:05:48
epoch [63/200] batch [3/3] time 0.713 (0.801) data 0.000 (0.088) loss 0.5229 (0.5741) acc 87.5000 (87.5000) lr 1.5621e-03 eta 0:05:29
epoch [64/200] batch [1/3] time 0.995 (0.995) data 0.287 (0.287) loss 0.8462 (0.8462) acc 81.2500 (81.2500) lr 1.5621e-03 eta 0:06:48
epoch [64/200] batch [2/3] time 0.712 (0.854) data 0.000 (0.143) loss 0.9517 (0.8989) acc 78.1250 (79.6875) lr 1.5621e-03 eta 0:05:49
epoch [64/200] batch [3/3] time 0.714 (0.807) data 0.000 (0.096) loss 0.7051 (0.8343) acc 87.5000 (82.2917) lr 1.5490e-03 eta 0:05:29
epoch [65/200] batch [1/3] time 0.981 (0.981) data 0.273 (0.273) loss 0.5742 (0.5742) acc 81.2500 (81.2500) lr 1.5490e-03 eta 0:06:39
epoch [65/200] batch [2/3] time 0.712 (0.847) data 0.000 (0.137) loss 0.3633 (0.4688) acc 93.7500 (87.5000) lr 1.5490e-03 eta 0:05:43
epoch [65/200] batch [3/3] time 0.713 (0.802) data 0.000 (0.091) loss 0.5884 (0.5086) acc 78.1250 (84.3750) lr 1.5358e-03 eta 0:05:24
epoch [66/200] batch [1/3] time 0.986 (0.986) data 0.277 (0.277) loss 0.5576 (0.5576) acc 81.2500 (81.2500) lr 1.5358e-03 eta 0:06:38
epoch [66/200] batch [2/3] time 0.713 (0.849) data 0.000 (0.139) loss 0.9517 (0.7546) acc 71.8750 (76.5625) lr 1.5358e-03 eta 0:05:42
epoch [66/200] batch [3/3] time 0.712 (0.804) data 0.000 (0.092) loss 0.5771 (0.6955) acc 84.3750 (79.1667) lr 1.5225e-03 eta 0:05:23
epoch [67/200] batch [1/3] time 0.981 (0.981) data 0.269 (0.269) loss 0.3701 (0.3701) acc 93.7500 (93.7500) lr 1.5225e-03 eta 0:06:33
epoch [67/200] batch [2/3] time 0.712 (0.847) data 0.000 (0.134) loss 0.6572 (0.5137) acc 87.5000 (90.6250) lr 1.5225e-03 eta 0:05:38
epoch [67/200] batch [3/3] time 0.713 (0.802) data 0.000 (0.090) loss 0.5479 (0.5251) acc 90.6250 (90.6250) lr 1.5090e-03 eta 0:05:19
epoch [68/200] batch [1/3] time 0.982 (0.982) data 0.271 (0.271) loss 0.3784 (0.3784) acc 87.5000 (87.5000) lr 1.5090e-03 eta 0:06:30
epoch [68/200] batch [2/3] time 0.713 (0.847) data 0.000 (0.135) loss 0.8481 (0.6133) acc 84.3750 (85.9375) lr 1.5090e-03 eta 0:05:36
epoch [68/200] batch [3/3] time 0.715 (0.803) data 0.000 (0.090) loss 0.5513 (0.5926) acc 78.1250 (83.3333) lr 1.4955e-03 eta 0:05:18
epoch [69/200] batch [1/3] time 0.988 (0.988) data 0.277 (0.277) loss 0.6074 (0.6074) acc 84.3750 (84.3750) lr 1.4955e-03 eta 0:06:30
epoch [69/200] batch [2/3] time 0.712 (0.850) data 0.000 (0.139) loss 0.6572 (0.6323) acc 78.1250 (81.2500) lr 1.4955e-03 eta 0:05:34
epoch [69/200] batch [3/3] time 0.712 (0.804) data 0.000 (0.092) loss 0.7344 (0.6663) acc 68.7500 (77.0833) lr 1.4818e-03 eta 0:05:15
epoch [70/200] batch [1/3] time 0.998 (0.998) data 0.287 (0.287) loss 0.7026 (0.7026) acc 78.1250 (78.1250) lr 1.4818e-03 eta 0:06:31
epoch [70/200] batch [2/3] time 0.712 (0.855) data 0.000 (0.143) loss 0.4778 (0.5902) acc 90.6250 (84.3750) lr 1.4818e-03 eta 0:05:34
epoch [70/200] batch [3/3] time 0.712 (0.807) data 0.000 (0.096) loss 0.2612 (0.4806) acc 93.7500 (87.5000) lr 1.4679e-03 eta 0:05:14
epoch [71/200] batch [1/3] time 0.996 (0.996) data 0.285 (0.285) loss 0.5957 (0.5957) acc 90.6250 (90.6250) lr 1.4679e-03 eta 0:06:27
epoch [71/200] batch [2/3] time 0.712 (0.854) data 0.000 (0.143) loss 0.2517 (0.4237) acc 90.6250 (90.6250) lr 1.4679e-03 eta 0:05:31
epoch [71/200] batch [3/3] time 0.712 (0.807) data 0.000 (0.095) loss 0.3142 (0.3872) acc 93.7500 (91.6667) lr 1.4540e-03 eta 0:05:12
epoch [72/200] batch [1/3] time 0.990 (0.990) data 0.279 (0.279) loss 0.9082 (0.9082) acc 75.0000 (75.0000) lr 1.4540e-03 eta 0:06:22
epoch [72/200] batch [2/3] time 0.714 (0.852) data 0.000 (0.140) loss 0.5703 (0.7393) acc 84.3750 (79.6875) lr 1.4540e-03 eta 0:05:28
epoch [72/200] batch [3/3] time 0.712 (0.806) data 0.000 (0.093) loss 0.4585 (0.6457) acc 84.3750 (81.2500) lr 1.4399e-03 eta 0:05:09
epoch [73/200] batch [1/3] time 0.979 (0.979) data 0.266 (0.266) loss 0.6611 (0.6611) acc 81.2500 (81.2500) lr 1.4399e-03 eta 0:06:15
epoch [73/200] batch [2/3] time 0.711 (0.845) data 0.000 (0.133) loss 0.4124 (0.5367) acc 87.5000 (84.3750) lr 1.4399e-03 eta 0:05:22
epoch [73/200] batch [3/3] time 0.712 (0.801) data 0.000 (0.089) loss 0.8027 (0.6254) acc 71.8750 (80.2083) lr 1.4258e-03 eta 0:05:05
epoch [74/200] batch [1/3] time 0.987 (0.987) data 0.274 (0.274) loss 0.4373 (0.4373) acc 93.7500 (93.7500) lr 1.4258e-03 eta 0:06:15
epoch [74/200] batch [2/3] time 0.713 (0.850) data 0.000 (0.137) loss 0.5322 (0.4847) acc 84.3750 (89.0625) lr 1.4258e-03 eta 0:05:22
epoch [74/200] batch [3/3] time 0.712 (0.804) data 0.000 (0.091) loss 0.4604 (0.4766) acc 84.3750 (87.5000) lr 1.4115e-03 eta 0:05:03
epoch [75/200] batch [1/3] time 0.980 (0.980) data 0.266 (0.266) loss 0.4641 (0.4641) acc 90.6250 (90.6250) lr 1.4115e-03 eta 0:06:09
epoch [75/200] batch [2/3] time 0.712 (0.846) data 0.000 (0.133) loss 0.6113 (0.5377) acc 84.3750 (87.5000) lr 1.4115e-03 eta 0:05:18
epoch [75/200] batch [3/3] time 0.712 (0.801) data 0.000 (0.089) loss 0.8096 (0.6283) acc 81.2500 (85.4167) lr 1.3971e-03 eta 0:05:00
epoch [76/200] batch [1/3] time 0.980 (0.980) data 0.267 (0.267) loss 0.4387 (0.4387) acc 90.6250 (90.6250) lr 1.3971e-03 eta 0:06:06
epoch [76/200] batch [2/3] time 0.712 (0.846) data 0.000 (0.134) loss 0.4724 (0.4556) acc 90.6250 (90.6250) lr 1.3971e-03 eta 0:05:15
epoch [76/200] batch [3/3] time 0.712 (0.801) data 0.000 (0.089) loss 0.5991 (0.5034) acc 78.1250 (86.4583) lr 1.3827e-03 eta 0:04:58
epoch [77/200] batch [1/3] time 0.977 (0.977) data 0.264 (0.264) loss 0.9556 (0.9556) acc 71.8750 (71.8750) lr 1.3827e-03 eta 0:06:02
epoch [77/200] batch [2/3] time 0.712 (0.845) data 0.000 (0.132) loss 0.5337 (0.7446) acc 87.5000 (79.6875) lr 1.3827e-03 eta 0:05:12
epoch [77/200] batch [3/3] time 0.714 (0.801) data 0.000 (0.088) loss 0.5137 (0.6676) acc 84.3750 (81.2500) lr 1.3681e-03 eta 0:04:55
epoch [78/200] batch [1/3] time 0.974 (0.974) data 0.266 (0.266) loss 0.3950 (0.3950) acc 81.2500 (81.2500) lr 1.3681e-03 eta 0:05:58
epoch [78/200] batch [2/3] time 0.712 (0.843) data 0.000 (0.133) loss 0.4407 (0.4178) acc 93.7500 (87.5000) lr 1.3681e-03 eta 0:05:09
epoch [78/200] batch [3/3] time 0.713 (0.800) data 0.000 (0.089) loss 0.8013 (0.5457) acc 78.1250 (84.3750) lr 1.3535e-03 eta 0:04:52
epoch [79/200] batch [1/3] time 0.974 (0.974) data 0.266 (0.266) loss 0.1940 (0.1940) acc 100.0000 (100.0000) lr 1.3535e-03 eta 0:05:55
epoch [79/200] batch [2/3] time 0.712 (0.843) data 0.000 (0.133) loss 0.6885 (0.4412) acc 81.2500 (90.6250) lr 1.3535e-03 eta 0:05:06
epoch [79/200] batch [3/3] time 0.713 (0.800) data 0.000 (0.089) loss 0.5986 (0.4937) acc 87.5000 (89.5833) lr 1.3387e-03 eta 0:04:50
epoch [80/200] batch [1/3] time 0.983 (0.983) data 0.274 (0.274) loss 0.5220 (0.5220) acc 87.5000 (87.5000) lr 1.3387e-03 eta 0:05:55
epoch [80/200] batch [2/3] time 0.712 (0.848) data 0.000 (0.137) loss 0.4404 (0.4812) acc 93.7500 (90.6250) lr 1.3387e-03 eta 0:05:05
epoch [80/200] batch [3/3] time 0.712 (0.802) data 0.000 (0.091) loss 0.8682 (0.6102) acc 71.8750 (84.3750) lr 1.3239e-03 eta 0:04:48
epoch [81/200] batch [1/3] time 0.978 (0.978) data 0.266 (0.266) loss 0.7134 (0.7134) acc 81.2500 (81.2500) lr 1.3239e-03 eta 0:05:51
epoch [81/200] batch [2/3] time 0.712 (0.845) data 0.000 (0.133) loss 0.6519 (0.6826) acc 87.5000 (84.3750) lr 1.3239e-03 eta 0:05:02
epoch [81/200] batch [3/3] time 0.713 (0.801) data 0.000 (0.089) loss 0.9834 (0.7829) acc 65.6250 (78.1250) lr 1.3090e-03 eta 0:04:45
epoch [82/200] batch [1/3] time 0.986 (0.986) data 0.274 (0.274) loss 0.4880 (0.4880) acc 87.5000 (87.5000) lr 1.3090e-03 eta 0:05:50
epoch [82/200] batch [2/3] time 0.716 (0.851) data 0.000 (0.137) loss 0.4221 (0.4551) acc 87.5000 (87.5000) lr 1.3090e-03 eta 0:05:01
epoch [82/200] batch [3/3] time 0.713 (0.805) data 0.000 (0.092) loss 0.7051 (0.5384) acc 90.6250 (88.5417) lr 1.2940e-03 eta 0:04:44
epoch [83/200] batch [1/3] time 0.987 (0.987) data 0.276 (0.276) loss 0.2686 (0.2686) acc 87.5000 (87.5000) lr 1.2940e-03 eta 0:05:48
epoch [83/200] batch [2/3] time 0.712 (0.849) data 0.000 (0.138) loss 0.5283 (0.3984) acc 87.5000 (87.5000) lr 1.2940e-03 eta 0:04:59
epoch [83/200] batch [3/3] time 0.713 (0.804) data 0.000 (0.092) loss 0.5146 (0.4372) acc 90.6250 (88.5417) lr 1.2790e-03 eta 0:04:42
epoch [84/200] batch [1/3] time 0.976 (0.976) data 0.266 (0.266) loss 0.6475 (0.6475) acc 81.2500 (81.2500) lr 1.2790e-03 eta 0:05:41
epoch [84/200] batch [2/3] time 0.712 (0.844) data 0.000 (0.133) loss 0.3955 (0.5215) acc 87.5000 (84.3750) lr 1.2790e-03 eta 0:04:54
epoch [84/200] batch [3/3] time 0.713 (0.800) data 0.000 (0.089) loss 0.8418 (0.6283) acc 75.0000 (81.2500) lr 1.2639e-03 eta 0:04:38
epoch [85/200] batch [1/3] time 0.976 (0.976) data 0.264 (0.264) loss 0.6426 (0.6426) acc 81.2500 (81.2500) lr 1.2639e-03 eta 0:05:38
epoch [85/200] batch [2/3] time 0.713 (0.845) data 0.000 (0.132) loss 0.8535 (0.7480) acc 75.0000 (78.1250) lr 1.2639e-03 eta 0:04:52
epoch [85/200] batch [3/3] time 0.712 (0.801) data 0.000 (0.088) loss 0.4460 (0.6474) acc 87.5000 (81.2500) lr 1.2487e-03 eta 0:04:36
epoch [86/200] batch [1/3] time 0.984 (0.984) data 0.272 (0.272) loss 0.4312 (0.4312) acc 90.6250 (90.6250) lr 1.2487e-03 eta 0:05:38
epoch [86/200] batch [2/3] time 0.712 (0.848) data 0.000 (0.136) loss 0.3250 (0.3781) acc 90.6250 (90.6250) lr 1.2487e-03 eta 0:04:50
epoch [86/200] batch [3/3] time 0.713 (0.803) data 0.000 (0.091) loss 0.3364 (0.3642) acc 90.6250 (90.6250) lr 1.2334e-03 eta 0:04:34
epoch [87/200] batch [1/3] time 0.980 (0.980) data 0.268 (0.268) loss 0.2496 (0.2496) acc 96.8750 (96.8750) lr 1.2334e-03 eta 0:05:34
epoch [87/200] batch [2/3] time 0.712 (0.846) data 0.000 (0.134) loss 0.8315 (0.5406) acc 84.3750 (90.6250) lr 1.2334e-03 eta 0:04:47
epoch [87/200] batch [3/3] time 0.712 (0.801) data 0.000 (0.089) loss 0.5986 (0.5599) acc 81.2500 (87.5000) lr 1.2181e-03 eta 0:04:31
epoch [88/200] batch [1/3] time 0.978 (0.978) data 0.267 (0.267) loss 0.5488 (0.5488) acc 87.5000 (87.5000) lr 1.2181e-03 eta 0:05:30
epoch [88/200] batch [2/3] time 0.712 (0.845) data 0.000 (0.134) loss 0.5903 (0.5696) acc 81.2500 (84.3750) lr 1.2181e-03 eta 0:04:44
epoch [88/200] batch [3/3] time 0.713 (0.801) data 0.000 (0.089) loss 0.4006 (0.5133) acc 93.7500 (87.5000) lr 1.2028e-03 eta 0:04:29
epoch [89/200] batch [1/3] time 0.977 (0.977) data 0.266 (0.266) loss 0.4248 (0.4248) acc 87.5000 (87.5000) lr 1.2028e-03 eta 0:05:27
epoch [89/200] batch [2/3] time 0.712 (0.844) data 0.000 (0.133) loss 0.2208 (0.3228) acc 93.7500 (90.6250) lr 1.2028e-03 eta 0:04:42
epoch [89/200] batch [3/3] time 0.712 (0.800) data 0.000 (0.089) loss 0.6270 (0.4242) acc 81.2500 (87.5000) lr 1.1874e-03 eta 0:04:26
epoch [90/200] batch [1/3] time 0.983 (0.983) data 0.275 (0.275) loss 0.6172 (0.6172) acc 81.2500 (81.2500) lr 1.1874e-03 eta 0:05:26
epoch [90/200] batch [2/3] time 0.714 (0.849) data 0.000 (0.137) loss 0.3752 (0.4962) acc 87.5000 (84.3750) lr 1.1874e-03 eta 0:04:40
epoch [90/200] batch [3/3] time 0.714 (0.804) data 0.000 (0.092) loss 0.2922 (0.4282) acc 96.8750 (88.5417) lr 1.1719e-03 eta 0:04:25
epoch [91/200] batch [1/3] time 0.982 (0.982) data 0.274 (0.274) loss 0.2559 (0.2559) acc 93.7500 (93.7500) lr 1.1719e-03 eta 0:05:23
epoch [91/200] batch [2/3] time 0.712 (0.847) data 0.000 (0.137) loss 0.3647 (0.3103) acc 90.6250 (92.1875) lr 1.1719e-03 eta 0:04:37
epoch [91/200] batch [3/3] time 0.712 (0.802) data 0.000 (0.091) loss 0.6152 (0.4119) acc 90.6250 (91.6667) lr 1.1564e-03 eta 0:04:22
epoch [92/200] batch [1/3] time 0.983 (0.983) data 0.275 (0.275) loss 0.6514 (0.6514) acc 75.0000 (75.0000) lr 1.1564e-03 eta 0:05:20
epoch [92/200] batch [2/3] time 0.712 (0.847) data 0.000 (0.137) loss 0.7090 (0.6802) acc 81.2500 (78.1250) lr 1.1564e-03 eta 0:04:35
epoch [92/200] batch [3/3] time 0.713 (0.802) data 0.000 (0.092) loss 0.6836 (0.6813) acc 81.2500 (79.1667) lr 1.1409e-03 eta 0:04:20
epoch [93/200] batch [1/3] time 0.983 (0.983) data 0.270 (0.270) loss 0.4460 (0.4460) acc 87.5000 (87.5000) lr 1.1409e-03 eta 0:05:17
epoch [93/200] batch [2/3] time 0.713 (0.848) data 0.000 (0.135) loss 0.3147 (0.3804) acc 93.7500 (90.6250) lr 1.1409e-03 eta 0:04:33
epoch [93/200] batch [3/3] time 0.712 (0.803) data 0.000 (0.090) loss 0.3459 (0.3689) acc 93.7500 (91.6667) lr 1.1253e-03 eta 0:04:17
epoch [94/200] batch [1/3] time 0.977 (0.977) data 0.265 (0.265) loss 0.4211 (0.4211) acc 90.6250 (90.6250) lr 1.1253e-03 eta 0:05:12
epoch [94/200] batch [2/3] time 0.712 (0.845) data 0.000 (0.133) loss 0.6001 (0.5106) acc 90.6250 (90.6250) lr 1.1253e-03 eta 0:04:29
epoch [94/200] batch [3/3] time 0.715 (0.801) data 0.000 (0.089) loss 0.2607 (0.4273) acc 93.7500 (91.6667) lr 1.1097e-03 eta 0:04:14
epoch [95/200] batch [1/3] time 0.982 (0.982) data 0.269 (0.269) loss 0.3108 (0.3108) acc 87.5000 (87.5000) lr 1.1097e-03 eta 0:05:11
epoch [95/200] batch [2/3] time 0.712 (0.847) data 0.000 (0.135) loss 0.5981 (0.4545) acc 84.3750 (85.9375) lr 1.1097e-03 eta 0:04:27
epoch [95/200] batch [3/3] time 0.712 (0.802) data 0.000 (0.090) loss 0.7041 (0.5377) acc 84.3750 (85.4167) lr 1.0941e-03 eta 0:04:12
epoch [96/200] batch [1/3] time 0.987 (0.987) data 0.276 (0.276) loss 0.7778 (0.7778) acc 78.1250 (78.1250) lr 1.0941e-03 eta 0:05:09
epoch [96/200] batch [2/3] time 0.712 (0.849) data 0.000 (0.138) loss 0.6431 (0.7104) acc 84.3750 (81.2500) lr 1.0941e-03 eta 0:04:25
epoch [96/200] batch [3/3] time 0.712 (0.804) data 0.000 (0.092) loss 0.2964 (0.5724) acc 93.7500 (85.4167) lr 1.0785e-03 eta 0:04:10
epoch [97/200] batch [1/3] time 0.981 (0.981) data 0.268 (0.268) loss 0.3765 (0.3765) acc 87.5000 (87.5000) lr 1.0785e-03 eta 0:05:05
epoch [97/200] batch [2/3] time 0.714 (0.847) data 0.000 (0.134) loss 0.1818 (0.2791) acc 96.8750 (92.1875) lr 1.0785e-03 eta 0:04:22
epoch [97/200] batch [3/3] time 0.717 (0.804) data 0.000 (0.090) loss 0.4258 (0.3280) acc 90.6250 (91.6667) lr 1.0628e-03 eta 0:04:08
epoch [98/200] batch [1/3] time 0.996 (0.996) data 0.284 (0.284) loss 0.3931 (0.3931) acc 93.7500 (93.7500) lr 1.0628e-03 eta 0:05:06
epoch [98/200] batch [2/3] time 0.714 (0.855) data 0.000 (0.142) loss 0.3965 (0.3948) acc 93.7500 (93.7500) lr 1.0628e-03 eta 0:04:22
epoch [98/200] batch [3/3] time 0.712 (0.808) data 0.000 (0.095) loss 0.7749 (0.5215) acc 75.0000 (87.5000) lr 1.0471e-03 eta 0:04:07
epoch [99/200] batch [1/3] time 0.996 (0.996) data 0.284 (0.284) loss 0.3660 (0.3660) acc 93.7500 (93.7500) lr 1.0471e-03 eta 0:05:03
epoch [99/200] batch [2/3] time 0.712 (0.854) data 0.000 (0.142) loss 0.3232 (0.3446) acc 87.5000 (90.6250) lr 1.0471e-03 eta 0:04:19
epoch [99/200] batch [3/3] time 0.713 (0.807) data 0.000 (0.095) loss 0.5732 (0.4208) acc 84.3750 (88.5417) lr 1.0314e-03 eta 0:04:04
epoch [100/200] batch [1/3] time 0.986 (0.986) data 0.274 (0.274) loss 0.8882 (0.8882) acc 78.1250 (78.1250) lr 1.0314e-03 eta 0:04:57
epoch [100/200] batch [2/3] time 0.712 (0.849) data 0.000 (0.137) loss 0.0979 (0.4930) acc 100.0000 (89.0625) lr 1.0314e-03 eta 0:04:15
epoch [100/200] batch [3/3] time 0.713 (0.804) data 0.000 (0.092) loss 0.3472 (0.4444) acc 90.6250 (89.5833) lr 1.0157e-03 eta 0:04:01
epoch [101/200] batch [1/3] time 0.994 (0.994) data 0.284 (0.284) loss 0.4297 (0.4297) acc 93.7500 (93.7500) lr 1.0157e-03 eta 0:04:57
epoch [101/200] batch [2/3] time 0.712 (0.853) data 0.000 (0.142) loss 0.1984 (0.3140) acc 93.7500 (93.7500) lr 1.0157e-03 eta 0:04:14
epoch [101/200] batch [3/3] time 0.712 (0.806) data 0.000 (0.095) loss 0.4421 (0.3567) acc 90.6250 (92.7083) lr 1.0000e-03 eta 0:03:59
epoch [102/200] batch [1/3] time 0.987 (0.987) data 0.274 (0.274) loss 0.3154 (0.3154) acc 90.6250 (90.6250) lr 1.0000e-03 eta 0:04:52
epoch [102/200] batch [2/3] time 0.712 (0.850) data 0.000 (0.137) loss 0.1833 (0.2494) acc 96.8750 (93.7500) lr 1.0000e-03 eta 0:04:10
epoch [102/200] batch [3/3] time 0.712 (0.804) data 0.000 (0.092) loss 0.2107 (0.2365) acc 96.8750 (94.7917) lr 9.8429e-04 eta 0:03:56
epoch [103/200] batch [1/3] time 0.975 (0.975) data 0.266 (0.266) loss 0.7100 (0.7100) acc 81.2500 (81.2500) lr 9.8429e-04 eta 0:04:45
epoch [103/200] batch [2/3] time 0.712 (0.844) data 0.000 (0.133) loss 0.7183 (0.7141) acc 81.2500 (81.2500) lr 9.8429e-04 eta 0:04:06
epoch [103/200] batch [3/3] time 0.712 (0.800) data 0.000 (0.089) loss 1.1533 (0.8605) acc 65.6250 (76.0417) lr 9.6859e-04 eta 0:03:52
epoch [104/200] batch [1/3] time 0.993 (0.993) data 0.282 (0.282) loss 0.7017 (0.7017) acc 78.1250 (78.1250) lr 9.6859e-04 eta 0:04:47
epoch [104/200] batch [2/3] time 0.712 (0.852) data 0.000 (0.141) loss 0.6240 (0.6628) acc 90.6250 (84.3750) lr 9.6859e-04 eta 0:04:06
epoch [104/200] batch [3/3] time 0.712 (0.806) data 0.000 (0.094) loss 0.2859 (0.5372) acc 93.7500 (87.5000) lr 9.5289e-04 eta 0:03:52
epoch [105/200] batch [1/3] time 0.994 (0.994) data 0.281 (0.281) loss 0.8042 (0.8042) acc 87.5000 (87.5000) lr 9.5289e-04 eta 0:04:45
epoch [105/200] batch [2/3] time 0.712 (0.853) data 0.000 (0.141) loss 0.4775 (0.6409) acc 90.6250 (89.0625) lr 9.5289e-04 eta 0:04:04
epoch [105/200] batch [3/3] time 0.713 (0.806) data 0.000 (0.094) loss 0.2240 (0.5019) acc 93.7500 (90.6250) lr 9.3721e-04 eta 0:03:49
epoch [106/200] batch [1/3] time 0.990 (0.990) data 0.281 (0.281) loss 0.1808 (0.1808) acc 90.6250 (90.6250) lr 9.3721e-04 eta 0:04:41
epoch [106/200] batch [2/3] time 0.712 (0.851) data 0.000 (0.141) loss 0.6074 (0.3941) acc 87.5000 (89.0625) lr 9.3721e-04 eta 0:04:00
epoch [106/200] batch [3/3] time 0.713 (0.805) data 0.000 (0.094) loss 0.5630 (0.4504) acc 90.6250 (89.5833) lr 9.2154e-04 eta 0:03:46
epoch [107/200] batch [1/3] time 0.987 (0.987) data 0.278 (0.278) loss 0.3301 (0.3301) acc 93.7500 (93.7500) lr 9.2154e-04 eta 0:04:37
epoch [107/200] batch [2/3] time 0.712 (0.849) data 0.000 (0.139) loss 0.2783 (0.3042) acc 93.7500 (93.7500) lr 9.2154e-04 eta 0:03:57
epoch [107/200] batch [3/3] time 0.712 (0.804) data 0.000 (0.093) loss 0.5581 (0.3888) acc 81.2500 (89.5833) lr 9.0589e-04 eta 0:03:44
epoch [108/200] batch [1/3] time 0.993 (0.993) data 0.284 (0.284) loss 0.3882 (0.3882) acc 90.6250 (90.6250) lr 9.0589e-04 eta 0:04:36
epoch [108/200] batch [2/3] time 0.712 (0.852) data 0.000 (0.142) loss 0.3643 (0.3762) acc 90.6250 (90.6250) lr 9.0589e-04 eta 0:03:56
epoch [108/200] batch [3/3] time 0.712 (0.806) data 0.000 (0.095) loss 0.3298 (0.3608) acc 90.6250 (90.6250) lr 8.9027e-04 eta 0:03:42
epoch [109/200] batch [1/3] time 0.987 (0.987) data 0.273 (0.273) loss 0.4229 (0.4229) acc 87.5000 (87.5000) lr 8.9027e-04 eta 0:04:31
epoch [109/200] batch [2/3] time 0.712 (0.850) data 0.000 (0.137) loss 0.3345 (0.3787) acc 93.7500 (90.6250) lr 8.9027e-04 eta 0:03:52
epoch [109/200] batch [3/3] time 0.715 (0.805) data 0.000 (0.091) loss 0.5503 (0.4359) acc 84.3750 (88.5417) lr 8.7467e-04 eta 0:03:39
epoch [110/200] batch [1/3] time 0.995 (0.995) data 0.282 (0.282) loss 0.1364 (0.1364) acc 96.8750 (96.8750) lr 8.7467e-04 eta 0:04:30
epoch [110/200] batch [2/3] time 0.712 (0.853) data 0.000 (0.141) loss 0.3997 (0.2680) acc 90.6250 (93.7500) lr 8.7467e-04 eta 0:03:51
epoch [110/200] batch [3/3] time 0.716 (0.808) data 0.000 (0.094) loss 0.5024 (0.3462) acc 90.6250 (92.7083) lr 8.5910e-04 eta 0:03:38
epoch [111/200] batch [1/3] time 0.995 (0.995) data 0.282 (0.282) loss 0.5371 (0.5371) acc 84.3750 (84.3750) lr 8.5910e-04 eta 0:04:27
epoch [111/200] batch [2/3] time 0.712 (0.853) data 0.000 (0.141) loss 0.1048 (0.3210) acc 100.0000 (92.1875) lr 8.5910e-04 eta 0:03:48
epoch [111/200] batch [3/3] time 0.713 (0.806) data 0.000 (0.094) loss 1.0283 (0.5567) acc 81.2500 (88.5417) lr 8.4357e-04 eta 0:03:35
epoch [112/200] batch [1/3] time 0.980 (0.980) data 0.267 (0.267) loss 0.2935 (0.2935) acc 90.6250 (90.6250) lr 8.4357e-04 eta 0:04:20
epoch [112/200] batch [2/3] time 0.712 (0.846) data 0.000 (0.134) loss 0.3999 (0.3467) acc 90.6250 (90.6250) lr 8.4357e-04 eta 0:03:44
epoch [112/200] batch [3/3] time 0.713 (0.802) data 0.000 (0.089) loss 0.6372 (0.4435) acc 84.3750 (88.5417) lr 8.2807e-04 eta 0:03:31
epoch [113/200] batch [1/3] time 0.997 (0.997) data 0.284 (0.284) loss 0.4458 (0.4458) acc 87.5000 (87.5000) lr 8.2807e-04 eta 0:04:22
epoch [113/200] batch [2/3] time 0.712 (0.855) data 0.000 (0.142) loss 0.3438 (0.3948) acc 90.6250 (89.0625) lr 8.2807e-04 eta 0:03:43
epoch [113/200] batch [3/3] time 0.714 (0.808) data 0.000 (0.095) loss 0.3276 (0.3724) acc 96.8750 (91.6667) lr 8.1262e-04 eta 0:03:30
epoch [114/200] batch [1/3] time 0.989 (0.989) data 0.276 (0.276) loss 0.5317 (0.5317) acc 90.6250 (90.6250) lr 8.1262e-04 eta 0:04:17
epoch [114/200] batch [2/3] time 0.712 (0.851) data 0.000 (0.138) loss 0.4641 (0.4979) acc 87.5000 (89.0625) lr 8.1262e-04 eta 0:03:40
epoch [114/200] batch [3/3] time 0.712 (0.804) data 0.000 (0.092) loss 0.1316 (0.3758) acc 96.8750 (91.6667) lr 7.9721e-04 eta 0:03:27
epoch [115/200] batch [1/3] time 0.986 (0.986) data 0.274 (0.274) loss 0.4412 (0.4412) acc 87.5000 (87.5000) lr 7.9721e-04 eta 0:04:13
epoch [115/200] batch [2/3] time 0.712 (0.849) data 0.000 (0.137) loss 0.3386 (0.3899) acc 93.7500 (90.6250) lr 7.9721e-04 eta 0:03:37
epoch [115/200] batch [3/3] time 0.712 (0.803) data 0.000 (0.091) loss 0.2469 (0.3422) acc 93.7500 (91.6667) lr 7.8186e-04 eta 0:03:24
epoch [116/200] batch [1/3] time 0.978 (0.978) data 0.266 (0.266) loss 0.3940 (0.3940) acc 84.3750 (84.3750) lr 7.8186e-04 eta 0:04:08
epoch [116/200] batch [2/3] time 0.713 (0.846) data 0.000 (0.133) loss 0.3333 (0.3636) acc 90.6250 (87.5000) lr 7.8186e-04 eta 0:03:33
epoch [116/200] batch [3/3] time 0.712 (0.801) data 0.000 (0.089) loss 0.5273 (0.4182) acc 87.5000 (87.5000) lr 7.6655e-04 eta 0:03:21
epoch [117/200] batch [1/3] time 0.984 (0.984) data 0.272 (0.272) loss 0.3247 (0.3247) acc 84.3750 (84.3750) lr 7.6655e-04 eta 0:04:06
epoch [117/200] batch [2/3] time 0.713 (0.848) data 0.000 (0.136) loss 0.5386 (0.4316) acc 90.6250 (87.5000) lr 7.6655e-04 eta 0:03:32
epoch [117/200] batch [3/3] time 0.712 (0.803) data 0.000 (0.091) loss 0.9429 (0.6021) acc 75.0000 (83.3333) lr 7.5131e-04 eta 0:03:19
epoch [118/200] batch [1/3] time 0.985 (0.985) data 0.272 (0.272) loss 0.3579 (0.3579) acc 87.5000 (87.5000) lr 7.5131e-04 eta 0:04:04
epoch [118/200] batch [2/3] time 0.712 (0.848) data 0.000 (0.136) loss 0.6489 (0.5034) acc 84.3750 (85.9375) lr 7.5131e-04 eta 0:03:29
epoch [118/200] batch [3/3] time 0.713 (0.803) data 0.000 (0.091) loss 0.1884 (0.3984) acc 87.5000 (86.4583) lr 7.3613e-04 eta 0:03:17
epoch [119/200] batch [1/3] time 0.978 (0.978) data 0.265 (0.265) loss 0.4023 (0.4023) acc 93.7500 (93.7500) lr 7.3613e-04 eta 0:03:59
epoch [119/200] batch [2/3] time 0.712 (0.845) data 0.000 (0.133) loss 0.3684 (0.3854) acc 87.5000 (90.6250) lr 7.3613e-04 eta 0:03:26
epoch [119/200] batch [3/3] time 0.712 (0.801) data 0.000 (0.088) loss 0.4309 (0.4006) acc 87.5000 (89.5833) lr 7.2101e-04 eta 0:03:14
epoch [120/200] batch [1/3] time 0.977 (0.977) data 0.267 (0.267) loss 0.2842 (0.2842) acc 90.6250 (90.6250) lr 7.2101e-04 eta 0:03:56
epoch [120/200] batch [2/3] time 0.713 (0.845) data 0.000 (0.134) loss 0.4302 (0.3572) acc 87.5000 (89.0625) lr 7.2101e-04 eta 0:03:23
epoch [120/200] batch [3/3] time 0.714 (0.801) data 0.000 (0.089) loss 0.2878 (0.3341) acc 93.7500 (90.6250) lr 7.0596e-04 eta 0:03:12
epoch [121/200] batch [1/3] time 0.985 (0.985) data 0.275 (0.275) loss 0.5898 (0.5898) acc 84.3750 (84.3750) lr 7.0596e-04 eta 0:03:55
epoch [121/200] batch [2/3] time 0.712 (0.848) data 0.000 (0.138) loss 0.5591 (0.5745) acc 81.2500 (82.8125) lr 7.0596e-04 eta 0:03:21
epoch [121/200] batch [3/3] time 0.712 (0.803) data 0.000 (0.092) loss 0.3882 (0.5124) acc 90.6250 (85.4167) lr 6.9098e-04 eta 0:03:10
epoch [122/200] batch [1/3] time 0.985 (0.985) data 0.276 (0.276) loss 0.4592 (0.4592) acc 90.6250 (90.6250) lr 6.9098e-04 eta 0:03:52
epoch [122/200] batch [2/3] time 0.712 (0.848) data 0.000 (0.138) loss 0.3938 (0.4265) acc 87.5000 (89.0625) lr 6.9098e-04 eta 0:03:19
epoch [122/200] batch [3/3] time 0.713 (0.803) data 0.000 (0.092) loss 0.3987 (0.4172) acc 90.6250 (89.5833) lr 6.7608e-04 eta 0:03:07
epoch [123/200] batch [1/3] time 0.983 (0.983) data 0.274 (0.274) loss 0.7896 (0.7896) acc 87.5000 (87.5000) lr 6.7608e-04 eta 0:03:49
epoch [123/200] batch [2/3] time 0.713 (0.848) data 0.000 (0.137) loss 0.4731 (0.6313) acc 90.6250 (89.0625) lr 6.7608e-04 eta 0:03:16
epoch [123/200] batch [3/3] time 0.712 (0.802) data 0.000 (0.091) loss 0.3225 (0.5284) acc 84.3750 (87.5000) lr 6.6126e-04 eta 0:03:05
epoch [124/200] batch [1/3] time 0.977 (0.977) data 0.265 (0.265) loss 0.4741 (0.4741) acc 84.3750 (84.3750) lr 6.6126e-04 eta 0:03:44
epoch [124/200] batch [2/3] time 0.712 (0.845) data 0.000 (0.132) loss 0.3020 (0.3881) acc 90.6250 (87.5000) lr 6.6126e-04 eta 0:03:13
epoch [124/200] batch [3/3] time 0.712 (0.800) data 0.000 (0.088) loss 0.3123 (0.3628) acc 93.7500 (89.5833) lr 6.4653e-04 eta 0:03:02
epoch [125/200] batch [1/3] time 0.985 (0.985) data 0.274 (0.274) loss 0.2314 (0.2314) acc 96.8750 (96.8750) lr 6.4653e-04 eta 0:03:43
epoch [125/200] batch [2/3] time 0.712 (0.849) data 0.000 (0.137) loss 0.1958 (0.2136) acc 96.8750 (96.8750) lr 6.4653e-04 eta 0:03:11
epoch [125/200] batch [3/3] time 0.714 (0.804) data 0.000 (0.091) loss 0.9258 (0.4510) acc 87.5000 (93.7500) lr 6.3188e-04 eta 0:03:00
epoch [126/200] batch [1/3] time 0.985 (0.985) data 0.274 (0.274) loss 0.4055 (0.4055) acc 90.6250 (90.6250) lr 6.3188e-04 eta 0:03:40
epoch [126/200] batch [2/3] time 0.712 (0.849) data 0.000 (0.137) loss 0.3672 (0.3864) acc 87.5000 (89.0625) lr 6.3188e-04 eta 0:03:09
epoch [126/200] batch [3/3] time 0.715 (0.804) data 0.000 (0.091) loss 0.3440 (0.3722) acc 93.7500 (90.6250) lr 6.1732e-04 eta 0:02:58
epoch [127/200] batch [1/3] time 0.993 (0.993) data 0.281 (0.281) loss 0.3198 (0.3198) acc 96.8750 (96.8750) lr 6.1732e-04 eta 0:03:39
epoch [127/200] batch [2/3] time 0.713 (0.853) data 0.000 (0.141) loss 0.4856 (0.4027) acc 87.5000 (92.1875) lr 6.1732e-04 eta 0:03:07
epoch [127/200] batch [3/3] time 0.712 (0.806) data 0.000 (0.094) loss 0.3401 (0.3818) acc 87.5000 (90.6250) lr 6.0285e-04 eta 0:02:56
epoch [128/200] batch [1/3] time 0.977 (0.977) data 0.266 (0.266) loss 0.4246 (0.4246) acc 84.3750 (84.3750) lr 6.0285e-04 eta 0:03:33
epoch [128/200] batch [2/3] time 0.712 (0.845) data 0.000 (0.133) loss 1.0674 (0.7460) acc 78.1250 (81.2500) lr 6.0285e-04 eta 0:03:03
epoch [128/200] batch [3/3] time 0.712 (0.801) data 0.000 (0.089) loss 0.4128 (0.6349) acc 87.5000 (83.3333) lr 5.8849e-04 eta 0:02:52
epoch [129/200] batch [1/3] time 0.989 (0.989) data 0.277 (0.277) loss 0.4639 (0.4639) acc 87.5000 (87.5000) lr 5.8849e-04 eta 0:03:32
epoch [129/200] batch [2/3] time 0.713 (0.851) data 0.000 (0.139) loss 0.2522 (0.3580) acc 93.7500 (90.6250) lr 5.8849e-04 eta 0:03:02
epoch [129/200] batch [3/3] time 0.712 (0.805) data 0.000 (0.093) loss 0.5161 (0.4107) acc 87.5000 (89.5833) lr 5.7422e-04 eta 0:02:51
epoch [130/200] batch [1/3] time 0.993 (0.993) data 0.282 (0.282) loss 0.4160 (0.4160) acc 84.3750 (84.3750) lr 5.7422e-04 eta 0:03:30
epoch [130/200] batch [2/3] time 0.712 (0.853) data 0.000 (0.141) loss 0.3086 (0.3623) acc 93.7500 (89.0625) lr 5.7422e-04 eta 0:02:59
epoch [130/200] batch [3/3] time 0.712 (0.806) data 0.000 (0.094) loss 0.2854 (0.3367) acc 93.7500 (90.6250) lr 5.6006e-04 eta 0:02:49
epoch [131/200] batch [1/3] time 0.985 (0.985) data 0.272 (0.272) loss 0.3889 (0.3889) acc 87.5000 (87.5000) lr 5.6006e-04 eta 0:03:25
epoch [131/200] batch [2/3] time 0.713 (0.849) data 0.000 (0.136) loss 0.7881 (0.5885) acc 75.0000 (81.2500) lr 5.6006e-04 eta 0:02:56
epoch [131/200] batch [3/3] time 0.712 (0.803) data 0.000 (0.091) loss 0.6353 (0.6041) acc 87.5000 (83.3333) lr 5.4601e-04 eta 0:02:46
epoch [132/200] batch [1/3] time 0.986 (0.986) data 0.274 (0.274) loss 0.2764 (0.2764) acc 90.6250 (90.6250) lr 5.4601e-04 eta 0:03:23
epoch [132/200] batch [2/3] time 0.714 (0.850) data 0.000 (0.137) loss 0.1887 (0.2325) acc 93.7500 (92.1875) lr 5.4601e-04 eta 0:02:54
epoch [132/200] batch [3/3] time 0.713 (0.804) data 0.000 (0.092) loss 0.3276 (0.2642) acc 93.7500 (92.7083) lr 5.3207e-04 eta 0:02:44
epoch [133/200] batch [1/3] time 0.977 (0.977) data 0.266 (0.266) loss 0.6826 (0.6826) acc 78.1250 (78.1250) lr 5.3207e-04 eta 0:03:18
epoch [133/200] batch [2/3] time 0.713 (0.845) data 0.000 (0.133) loss 0.6724 (0.6775) acc 90.6250 (84.3750) lr 5.3207e-04 eta 0:02:50
epoch [133/200] batch [3/3] time 0.712 (0.801) data 0.000 (0.089) loss 0.3440 (0.5663) acc 84.3750 (84.3750) lr 5.1825e-04 eta 0:02:40
epoch [134/200] batch [1/3] time 0.985 (0.985) data 0.274 (0.274) loss 0.8320 (0.8320) acc 75.0000 (75.0000) lr 5.1825e-04 eta 0:03:16
epoch [134/200] batch [2/3] time 0.712 (0.848) data 0.000 (0.137) loss 0.3242 (0.5781) acc 90.6250 (82.8125) lr 5.1825e-04 eta 0:02:48
epoch [134/200] batch [3/3] time 0.712 (0.803) data 0.000 (0.091) loss 0.3984 (0.5182) acc 87.5000 (84.3750) lr 5.0454e-04 eta 0:02:38
epoch [135/200] batch [1/3] time 0.994 (0.994) data 0.285 (0.285) loss 0.2883 (0.2883) acc 90.6250 (90.6250) lr 5.0454e-04 eta 0:03:15
epoch [135/200] batch [2/3] time 0.712 (0.853) data 0.000 (0.143) loss 0.3237 (0.3060) acc 90.6250 (90.6250) lr 5.0454e-04 eta 0:02:47
epoch [135/200] batch [3/3] time 0.713 (0.806) data 0.000 (0.095) loss 0.3420 (0.3180) acc 90.6250 (90.6250) lr 4.9096e-04 eta 0:02:37
epoch [136/200] batch [1/3] time 0.991 (0.991) data 0.282 (0.282) loss 0.1031 (0.1031) acc 96.8750 (96.8750) lr 4.9096e-04 eta 0:03:12
epoch [136/200] batch [2/3] time 0.712 (0.852) data 0.000 (0.141) loss 0.6973 (0.4002) acc 84.3750 (90.6250) lr 4.9096e-04 eta 0:02:44
epoch [136/200] batch [3/3] time 0.713 (0.805) data 0.000 (0.094) loss 0.2676 (0.3560) acc 90.6250 (90.6250) lr 4.7750e-04 eta 0:02:34
epoch [137/200] batch [1/3] time 0.974 (0.974) data 0.266 (0.266) loss 0.7056 (0.7056) acc 90.6250 (90.6250) lr 4.7750e-04 eta 0:03:06
epoch [137/200] batch [2/3] time 0.713 (0.843) data 0.000 (0.133) loss 0.3213 (0.5134) acc 90.6250 (90.6250) lr 4.7750e-04 eta 0:02:40
epoch [137/200] batch [3/3] time 0.712 (0.800) data 0.000 (0.089) loss 0.2135 (0.4135) acc 96.8750 (92.7083) lr 4.6417e-04 eta 0:02:31
epoch [138/200] batch [1/3] time 0.987 (0.987) data 0.274 (0.274) loss 0.4539 (0.4539) acc 87.5000 (87.5000) lr 4.6417e-04 eta 0:03:05
epoch [138/200] batch [2/3] time 0.712 (0.849) data 0.000 (0.137) loss 0.3252 (0.3895) acc 90.6250 (89.0625) lr 4.6417e-04 eta 0:02:38
epoch [138/200] batch [3/3] time 0.713 (0.804) data 0.000 (0.092) loss 0.2024 (0.3271) acc 100.0000 (92.7083) lr 4.5098e-04 eta 0:02:29
epoch [139/200] batch [1/3] time 0.996 (0.996) data 0.284 (0.284) loss 0.3970 (0.3970) acc 90.6250 (90.6250) lr 4.5098e-04 eta 0:03:04
epoch [139/200] batch [2/3] time 0.713 (0.854) data 0.000 (0.142) loss 0.8062 (0.6016) acc 81.2500 (85.9375) lr 4.5098e-04 eta 0:02:37
epoch [139/200] batch [3/3] time 0.712 (0.807) data 0.000 (0.095) loss 0.1240 (0.4424) acc 96.8750 (89.5833) lr 4.3792e-04 eta 0:02:27
epoch [140/200] batch [1/3] time 0.984 (0.984) data 0.273 (0.273) loss 0.6782 (0.6782) acc 81.2500 (81.2500) lr 4.3792e-04 eta 0:02:59
epoch [140/200] batch [2/3] time 0.712 (0.848) data 0.000 (0.136) loss 0.3167 (0.4974) acc 87.5000 (84.3750) lr 4.3792e-04 eta 0:02:33
epoch [140/200] batch [3/3] time 0.712 (0.803) data 0.000 (0.091) loss 0.5469 (0.5139) acc 87.5000 (85.4167) lr 4.2499e-04 eta 0:02:24
epoch [141/200] batch [1/3] time 0.992 (0.992) data 0.281 (0.281) loss 0.3198 (0.3198) acc 90.6250 (90.6250) lr 4.2499e-04 eta 0:02:57
epoch [141/200] batch [2/3] time 0.713 (0.853) data 0.000 (0.141) loss 0.4556 (0.3877) acc 84.3750 (87.5000) lr 4.2499e-04 eta 0:02:31
epoch [141/200] batch [3/3] time 0.712 (0.806) data 0.000 (0.094) loss 0.5942 (0.4565) acc 87.5000 (87.5000) lr 4.1221e-04 eta 0:02:22
epoch [142/200] batch [1/3] time 0.994 (0.994) data 0.283 (0.283) loss 0.3296 (0.3296) acc 93.7500 (93.7500) lr 4.1221e-04 eta 0:02:54
epoch [142/200] batch [2/3] time 0.711 (0.853) data 0.000 (0.142) loss 0.3440 (0.3368) acc 87.5000 (90.6250) lr 4.1221e-04 eta 0:02:29
epoch [142/200] batch [3/3] time 0.712 (0.806) data 0.000 (0.094) loss 0.4434 (0.3723) acc 87.5000 (89.5833) lr 3.9958e-04 eta 0:02:20
epoch [143/200] batch [1/3] time 0.986 (0.986) data 0.272 (0.272) loss 0.4026 (0.4026) acc 84.3750 (84.3750) lr 3.9958e-04 eta 0:02:50
epoch [143/200] batch [2/3] time 0.713 (0.849) data 0.000 (0.136) loss 0.3604 (0.3815) acc 93.7500 (89.0625) lr 3.9958e-04 eta 0:02:26
epoch [143/200] batch [3/3] time 0.713 (0.804) data 0.000 (0.091) loss 0.8154 (0.5261) acc 81.2500 (86.4583) lr 3.8709e-04 eta 0:02:17
epoch [144/200] batch [1/3] time 0.986 (0.986) data 0.275 (0.275) loss 0.4299 (0.4299) acc 87.5000 (87.5000) lr 3.8709e-04 eta 0:02:47
epoch [144/200] batch [2/3] time 0.712 (0.849) data 0.000 (0.137) loss 0.2649 (0.3474) acc 90.6250 (89.0625) lr 3.8709e-04 eta 0:02:23
epoch [144/200] batch [3/3] time 0.712 (0.803) data 0.000 (0.092) loss 0.5571 (0.4173) acc 78.1250 (85.4167) lr 3.7476e-04 eta 0:02:14
epoch [145/200] batch [1/3] time 0.986 (0.986) data 0.273 (0.273) loss 0.2161 (0.2161) acc 93.7500 (93.7500) lr 3.7476e-04 eta 0:02:44
epoch [145/200] batch [2/3] time 0.712 (0.849) data 0.000 (0.136) loss 0.5493 (0.3827) acc 87.5000 (90.6250) lr 3.7476e-04 eta 0:02:20
epoch [145/200] batch [3/3] time 0.713 (0.804) data 0.000 (0.091) loss 0.4534 (0.4062) acc 81.2500 (87.5000) lr 3.6258e-04 eta 0:02:12
epoch [146/200] batch [1/3] time 0.993 (0.993) data 0.280 (0.280) loss 0.2520 (0.2520) acc 96.8750 (96.8750) lr 3.6258e-04 eta 0:02:42
epoch [146/200] batch [2/3] time 0.713 (0.853) data 0.000 (0.140) loss 0.3911 (0.3215) acc 90.6250 (93.7500) lr 3.6258e-04 eta 0:02:19
epoch [146/200] batch [3/3] time 0.712 (0.806) data 0.000 (0.094) loss 0.2903 (0.3111) acc 90.6250 (92.7083) lr 3.5055e-04 eta 0:02:10
epoch [147/200] batch [1/3] time 0.996 (0.996) data 0.283 (0.283) loss 0.4009 (0.4009) acc 87.5000 (87.5000) lr 3.5055e-04 eta 0:02:40
epoch [147/200] batch [2/3] time 0.712 (0.854) data 0.000 (0.141) loss 0.4321 (0.4165) acc 87.5000 (87.5000) lr 3.5055e-04 eta 0:02:16
epoch [147/200] batch [3/3] time 0.713 (0.807) data 0.000 (0.094) loss 0.3132 (0.3821) acc 100.0000 (91.6667) lr 3.3869e-04 eta 0:02:08
epoch [148/200] batch [1/3] time 0.999 (0.999) data 0.286 (0.286) loss 0.2462 (0.2462) acc 96.8750 (96.8750) lr 3.3869e-04 eta 0:02:37
epoch [148/200] batch [2/3] time 0.712 (0.855) data 0.000 (0.143) loss 0.6797 (0.4630) acc 81.2500 (89.0625) lr 3.3869e-04 eta 0:02:14
epoch [148/200] batch [3/3] time 0.712 (0.808) data 0.000 (0.095) loss 0.2134 (0.3798) acc 90.6250 (89.5833) lr 3.2699e-04 eta 0:02:06
epoch [149/200] batch [1/3] time 0.990 (0.990) data 0.278 (0.278) loss 0.2227 (0.2227) acc 96.8750 (96.8750) lr 3.2699e-04 eta 0:02:33
epoch [149/200] batch [2/3] time 0.715 (0.852) data 0.000 (0.139) loss 0.4663 (0.3445) acc 84.3750 (90.6250) lr 3.2699e-04 eta 0:02:11
epoch [149/200] batch [3/3] time 0.714 (0.806) data 0.000 (0.093) loss 0.3704 (0.3531) acc 93.7500 (91.6667) lr 3.1545e-04 eta 0:02:03
epoch [150/200] batch [1/3] time 0.982 (0.982) data 0.271 (0.271) loss 0.2467 (0.2467) acc 93.7500 (93.7500) lr 3.1545e-04 eta 0:02:29
epoch [150/200] batch [2/3] time 0.712 (0.847) data 0.000 (0.135) loss 0.9014 (0.5740) acc 84.3750 (89.0625) lr 3.1545e-04 eta 0:02:07
epoch [150/200] batch [3/3] time 0.711 (0.802) data 0.000 (0.090) loss 0.1471 (0.4317) acc 100.0000 (92.7083) lr 3.0409e-04 eta 0:02:00
epoch [151/200] batch [1/3] time 0.993 (0.993) data 0.282 (0.282) loss 0.3713 (0.3713) acc 96.8750 (96.8750) lr 3.0409e-04 eta 0:02:27
epoch [151/200] batch [2/3] time 0.714 (0.853) data 0.000 (0.141) loss 0.3872 (0.3793) acc 87.5000 (92.1875) lr 3.0409e-04 eta 0:02:06
epoch [151/200] batch [3/3] time 0.713 (0.806) data 0.000 (0.094) loss 0.2603 (0.3396) acc 93.7500 (92.7083) lr 2.9289e-04 eta 0:01:58
epoch [152/200] batch [1/3] time 0.987 (0.987) data 0.274 (0.274) loss 0.4810 (0.4810) acc 90.6250 (90.6250) lr 2.9289e-04 eta 0:02:24
epoch [152/200] batch [2/3] time 0.712 (0.850) data 0.000 (0.137) loss 0.4348 (0.4579) acc 90.6250 (90.6250) lr 2.9289e-04 eta 0:02:03
epoch [152/200] batch [3/3] time 0.714 (0.804) data 0.000 (0.091) loss 0.2277 (0.3811) acc 93.7500 (91.6667) lr 2.8187e-04 eta 0:01:55
epoch [153/200] batch [1/3] time 0.989 (0.989) data 0.280 (0.280) loss 0.3635 (0.3635) acc 90.6250 (90.6250) lr 2.8187e-04 eta 0:02:21
epoch [153/200] batch [2/3] time 0.712 (0.851) data 0.000 (0.140) loss 0.4927 (0.4281) acc 84.3750 (87.5000) lr 2.8187e-04 eta 0:02:00
epoch [153/200] batch [3/3] time 0.713 (0.805) data 0.000 (0.094) loss 0.2920 (0.3827) acc 93.7500 (89.5833) lr 2.7103e-04 eta 0:01:53
epoch [154/200] batch [1/3] time 0.975 (0.975) data 0.266 (0.266) loss 0.2228 (0.2228) acc 96.8750 (96.8750) lr 2.7103e-04 eta 0:02:16
epoch [154/200] batch [2/3] time 0.713 (0.844) data 0.000 (0.133) loss 0.3379 (0.2803) acc 93.7500 (95.3125) lr 2.7103e-04 eta 0:01:57
epoch [154/200] batch [3/3] time 0.713 (0.800) data 0.000 (0.089) loss 0.5952 (0.3853) acc 87.5000 (92.7083) lr 2.6037e-04 eta 0:01:50
epoch [155/200] batch [1/3] time 0.982 (0.982) data 0.273 (0.273) loss 0.6821 (0.6821) acc 84.3750 (84.3750) lr 2.6037e-04 eta 0:02:14
epoch [155/200] batch [2/3] time 0.712 (0.847) data 0.000 (0.137) loss 0.2571 (0.4696) acc 90.6250 (87.5000) lr 2.6037e-04 eta 0:01:55
epoch [155/200] batch [3/3] time 0.713 (0.802) data 0.000 (0.091) loss 0.5195 (0.4862) acc 87.5000 (87.5000) lr 2.4989e-04 eta 0:01:48
epoch [156/200] batch [1/3] time 0.989 (0.989) data 0.276 (0.276) loss 0.1772 (0.1772) acc 93.7500 (93.7500) lr 2.4989e-04 eta 0:02:12
epoch [156/200] batch [2/3] time 0.712 (0.851) data 0.000 (0.138) loss 0.5381 (0.3577) acc 90.6250 (92.1875) lr 2.4989e-04 eta 0:01:53
epoch [156/200] batch [3/3] time 0.713 (0.805) data 0.000 (0.092) loss 0.4111 (0.3755) acc 90.6250 (91.6667) lr 2.3959e-04 eta 0:01:46
epoch [157/200] batch [1/3] time 0.985 (0.985) data 0.274 (0.274) loss 0.4500 (0.4500) acc 90.6250 (90.6250) lr 2.3959e-04 eta 0:02:09
epoch [157/200] batch [2/3] time 0.712 (0.849) data 0.000 (0.137) loss 0.5371 (0.4935) acc 87.5000 (89.0625) lr 2.3959e-04 eta 0:01:50
epoch [157/200] batch [3/3] time 0.713 (0.803) data 0.000 (0.091) loss 0.6240 (0.5370) acc 87.5000 (88.5417) lr 2.2949e-04 eta 0:01:43
epoch [158/200] batch [1/3] time 0.977 (0.977) data 0.266 (0.266) loss 0.3289 (0.3289) acc 90.6250 (90.6250) lr 2.2949e-04 eta 0:02:05
epoch [158/200] batch [2/3] time 0.712 (0.845) data 0.000 (0.133) loss 0.1816 (0.2552) acc 96.8750 (93.7500) lr 2.2949e-04 eta 0:01:47
epoch [158/200] batch [3/3] time 0.714 (0.801) data 0.000 (0.089) loss 0.6060 (0.3722) acc 84.3750 (90.6250) lr 2.1957e-04 eta 0:01:40
epoch [159/200] batch [1/3] time 0.986 (0.986) data 0.273 (0.273) loss 0.6343 (0.6343) acc 87.5000 (87.5000) lr 2.1957e-04 eta 0:02:03
epoch [159/200] batch [2/3] time 0.713 (0.849) data 0.000 (0.137) loss 0.1603 (0.3973) acc 100.0000 (93.7500) lr 2.1957e-04 eta 0:01:45
epoch [159/200] batch [3/3] time 0.713 (0.804) data 0.000 (0.091) loss 0.3599 (0.3848) acc 90.6250 (92.7083) lr 2.0984e-04 eta 0:01:38
epoch [160/200] batch [1/3] time 0.984 (0.984) data 0.273 (0.273) loss 0.3438 (0.3438) acc 90.6250 (90.6250) lr 2.0984e-04 eta 0:02:00
epoch [160/200] batch [2/3] time 0.712 (0.848) data 0.000 (0.137) loss 0.2976 (0.3207) acc 93.7500 (92.1875) lr 2.0984e-04 eta 0:01:42
epoch [160/200] batch [3/3] time 0.712 (0.803) data 0.000 (0.091) loss 0.4028 (0.3481) acc 87.5000 (90.6250) lr 2.0032e-04 eta 0:01:36
epoch [161/200] batch [1/3] time 0.995 (0.995) data 0.283 (0.283) loss 0.3047 (0.3047) acc 96.8750 (96.8750) lr 2.0032e-04 eta 0:01:58
epoch [161/200] batch [2/3] time 0.714 (0.854) data 0.000 (0.142) loss 0.4512 (0.3779) acc 90.6250 (93.7500) lr 2.0032e-04 eta 0:01:40
epoch [161/200] batch [3/3] time 0.712 (0.807) data 0.000 (0.095) loss 0.6382 (0.4647) acc 84.3750 (90.6250) lr 1.9098e-04 eta 0:01:34
epoch [162/200] batch [1/3] time 0.978 (0.978) data 0.266 (0.266) loss 0.2756 (0.2756) acc 93.7500 (93.7500) lr 1.9098e-04 eta 0:01:53
epoch [162/200] batch [2/3] time 0.712 (0.845) data 0.000 (0.133) loss 0.3748 (0.3252) acc 93.7500 (93.7500) lr 1.9098e-04 eta 0:01:37
epoch [162/200] batch [3/3] time 0.712 (0.801) data 0.000 (0.089) loss 0.9160 (0.5221) acc 65.6250 (84.3750) lr 1.8185e-04 eta 0:01:31
epoch [163/200] batch [1/3] time 0.994 (0.994) data 0.282 (0.282) loss 0.4158 (0.4158) acc 93.7500 (93.7500) lr 1.8185e-04 eta 0:01:52
epoch [163/200] batch [2/3] time 0.712 (0.853) data 0.000 (0.141) loss 0.1824 (0.2991) acc 96.8750 (95.3125) lr 1.8185e-04 eta 0:01:35
epoch [163/200] batch [3/3] time 0.713 (0.806) data 0.000 (0.094) loss 0.5239 (0.3740) acc 90.6250 (93.7500) lr 1.7292e-04 eta 0:01:29
epoch [164/200] batch [1/3] time 0.987 (0.987) data 0.275 (0.275) loss 0.2310 (0.2310) acc 93.7500 (93.7500) lr 1.7292e-04 eta 0:01:48
epoch [164/200] batch [2/3] time 0.712 (0.850) data 0.000 (0.138) loss 0.3181 (0.2745) acc 93.7500 (93.7500) lr 1.7292e-04 eta 0:01:32
epoch [164/200] batch [3/3] time 0.713 (0.804) data 0.000 (0.092) loss 0.3567 (0.3019) acc 93.7500 (93.7500) lr 1.6419e-04 eta 0:01:26
epoch [165/200] batch [1/3] time 0.984 (0.984) data 0.273 (0.273) loss 0.5845 (0.5845) acc 87.5000 (87.5000) lr 1.6419e-04 eta 0:01:45
epoch [165/200] batch [2/3] time 0.712 (0.848) data 0.000 (0.137) loss 0.3516 (0.4680) acc 90.6250 (89.0625) lr 1.6419e-04 eta 0:01:29
epoch [165/200] batch [3/3] time 0.713 (0.803) data 0.000 (0.091) loss 0.1539 (0.3633) acc 96.8750 (91.6667) lr 1.5567e-04 eta 0:01:24
epoch [166/200] batch [1/3] time 0.976 (0.976) data 0.264 (0.264) loss 0.5728 (0.5728) acc 93.7500 (93.7500) lr 1.5567e-04 eta 0:01:41
epoch [166/200] batch [2/3] time 0.712 (0.844) data 0.000 (0.132) loss 0.3293 (0.4510) acc 93.7500 (93.7500) lr 1.5567e-04 eta 0:01:26
epoch [166/200] batch [3/3] time 0.712 (0.800) data 0.000 (0.088) loss 0.6143 (0.5055) acc 87.5000 (91.6667) lr 1.4736e-04 eta 0:01:21
epoch [167/200] batch [1/3] time 0.983 (0.983) data 0.274 (0.274) loss 0.7803 (0.7803) acc 75.0000 (75.0000) lr 1.4736e-04 eta 0:01:39
epoch [167/200] batch [2/3] time 0.714 (0.849) data 0.000 (0.137) loss 0.5688 (0.6746) acc 90.6250 (82.8125) lr 1.4736e-04 eta 0:01:24
epoch [167/200] batch [3/3] time 0.712 (0.803) data 0.000 (0.091) loss 0.4006 (0.5833) acc 90.6250 (85.4167) lr 1.3926e-04 eta 0:01:19
epoch [168/200] batch [1/3] time 0.990 (0.990) data 0.282 (0.282) loss 0.3477 (0.3477) acc 93.7500 (93.7500) lr 1.3926e-04 eta 0:01:37
epoch [168/200] batch [2/3] time 0.712 (0.851) data 0.000 (0.141) loss 1.1143 (0.7310) acc 75.0000 (84.3750) lr 1.3926e-04 eta 0:01:22
epoch [168/200] batch [3/3] time 0.713 (0.805) data 0.000 (0.094) loss 0.7700 (0.7440) acc 84.3750 (84.3750) lr 1.3137e-04 eta 0:01:17
epoch [169/200] batch [1/3] time 0.982 (0.982) data 0.274 (0.274) loss 0.3774 (0.3774) acc 90.6250 (90.6250) lr 1.3137e-04 eta 0:01:33
epoch [169/200] batch [2/3] time 0.712 (0.847) data 0.000 (0.137) loss 1.1279 (0.7527) acc 75.0000 (82.8125) lr 1.3137e-04 eta 0:01:19
epoch [169/200] batch [3/3] time 0.713 (0.802) data 0.000 (0.091) loss 0.0728 (0.5261) acc 100.0000 (88.5417) lr 1.2369e-04 eta 0:01:14
epoch [170/200] batch [1/3] time 0.992 (0.992) data 0.283 (0.283) loss 0.4004 (0.4004) acc 87.5000 (87.5000) lr 1.2369e-04 eta 0:01:31
epoch [170/200] batch [2/3] time 0.712 (0.852) data 0.000 (0.142) loss 0.6890 (0.5447) acc 81.2500 (84.3750) lr 1.2369e-04 eta 0:01:17
epoch [170/200] batch [3/3] time 0.714 (0.806) data 0.000 (0.094) loss 0.2399 (0.4431) acc 90.6250 (86.4583) lr 1.1623e-04 eta 0:01:12
epoch [171/200] batch [1/3] time 0.977 (0.977) data 0.265 (0.265) loss 0.6787 (0.6787) acc 84.3750 (84.3750) lr 1.1623e-04 eta 0:01:26
epoch [171/200] batch [2/3] time 0.712 (0.845) data 0.000 (0.133) loss 0.5684 (0.6235) acc 84.3750 (84.3750) lr 1.1623e-04 eta 0:01:14
epoch [171/200] batch [3/3] time 0.713 (0.801) data 0.000 (0.089) loss 0.2644 (0.5038) acc 93.7500 (87.5000) lr 1.0899e-04 eta 0:01:09
epoch [172/200] batch [1/3] time 0.978 (0.978) data 0.265 (0.265) loss 0.4343 (0.4343) acc 93.7500 (93.7500) lr 1.0899e-04 eta 0:01:24
epoch [172/200] batch [2/3] time 0.712 (0.845) data 0.000 (0.133) loss 0.1930 (0.3137) acc 96.8750 (95.3125) lr 1.0899e-04 eta 0:01:11
epoch [172/200] batch [3/3] time 0.713 (0.801) data 0.000 (0.088) loss 0.1736 (0.2670) acc 96.8750 (95.8333) lr 1.0197e-04 eta 0:01:07
epoch [173/200] batch [1/3] time 0.988 (0.988) data 0.275 (0.275) loss 0.4492 (0.4492) acc 84.3750 (84.3750) lr 1.0197e-04 eta 0:01:22
epoch [173/200] batch [2/3] time 0.712 (0.850) data 0.000 (0.138) loss 0.5854 (0.5173) acc 87.5000 (85.9375) lr 1.0197e-04 eta 0:01:09
epoch [173/200] batch [3/3] time 0.715 (0.805) data 0.000 (0.092) loss 0.3940 (0.4762) acc 93.7500 (88.5417) lr 9.5173e-05 eta 0:01:05
epoch [174/200] batch [1/3] time 0.987 (0.987) data 0.275 (0.275) loss 0.3950 (0.3950) acc 87.5000 (87.5000) lr 9.5173e-05 eta 0:01:18
epoch [174/200] batch [2/3] time 0.712 (0.850) data 0.000 (0.137) loss 0.3550 (0.3750) acc 90.6250 (89.0625) lr 9.5173e-05 eta 0:01:07
epoch [174/200] batch [3/3] time 0.712 (0.804) data 0.000 (0.092) loss 0.3074 (0.3525) acc 90.6250 (89.5833) lr 8.8597e-05 eta 0:01:02
epoch [175/200] batch [1/3] time 0.987 (0.987) data 0.274 (0.274) loss 0.2247 (0.2247) acc 93.7500 (93.7500) lr 8.8597e-05 eta 0:01:16
epoch [175/200] batch [2/3] time 0.715 (0.851) data 0.000 (0.137) loss 0.4253 (0.3250) acc 90.6250 (92.1875) lr 8.8597e-05 eta 0:01:04
epoch [175/200] batch [3/3] time 0.713 (0.805) data 0.000 (0.092) loss 0.3125 (0.3208) acc 93.7500 (92.7083) lr 8.2245e-05 eta 0:01:00
epoch [176/200] batch [1/3] time 0.988 (0.988) data 0.274 (0.274) loss 0.4065 (0.4065) acc 90.6250 (90.6250) lr 8.2245e-05 eta 0:01:13
epoch [176/200] batch [2/3] time 0.712 (0.850) data 0.000 (0.137) loss 0.2124 (0.3094) acc 90.6250 (90.6250) lr 8.2245e-05 eta 0:01:02
epoch [176/200] batch [3/3] time 0.712 (0.804) data 0.000 (0.092) loss 0.2375 (0.2855) acc 93.7500 (91.6667) lr 7.6120e-05 eta 0:00:57
epoch [177/200] batch [1/3] time 0.994 (0.994) data 0.281 (0.281) loss 0.4045 (0.4045) acc 90.6250 (90.6250) lr 7.6120e-05 eta 0:01:10
epoch [177/200] batch [2/3] time 0.712 (0.853) data 0.000 (0.141) loss 0.4910 (0.4478) acc 90.6250 (90.6250) lr 7.6120e-05 eta 0:00:59
epoch [177/200] batch [3/3] time 0.712 (0.806) data 0.000 (0.094) loss 0.2207 (0.3721) acc 93.7500 (91.6667) lr 7.0224e-05 eta 0:00:55
epoch [178/200] batch [1/3] time 0.982 (0.982) data 0.273 (0.273) loss 0.4814 (0.4814) acc 87.5000 (87.5000) lr 7.0224e-05 eta 0:01:06
epoch [178/200] batch [2/3] time 0.713 (0.848) data 0.000 (0.136) loss 0.3452 (0.4133) acc 90.6250 (89.0625) lr 7.0224e-05 eta 0:00:56
epoch [178/200] batch [3/3] time 0.712 (0.802) data 0.000 (0.091) loss 0.3042 (0.3770) acc 90.6250 (89.5833) lr 6.4556e-05 eta 0:00:52
epoch [179/200] batch [1/3] time 0.978 (0.978) data 0.266 (0.266) loss 0.2827 (0.2827) acc 93.7500 (93.7500) lr 6.4556e-05 eta 0:01:03
epoch [179/200] batch [2/3] time 0.712 (0.845) data 0.000 (0.133) loss 0.5635 (0.4231) acc 87.5000 (90.6250) lr 6.4556e-05 eta 0:00:54
epoch [179/200] batch [3/3] time 0.712 (0.801) data 0.000 (0.089) loss 0.2198 (0.3553) acc 100.0000 (93.7500) lr 5.9119e-05 eta 0:00:50
epoch [180/200] batch [1/3] time 0.977 (0.977) data 0.265 (0.265) loss 0.3718 (0.3718) acc 87.5000 (87.5000) lr 5.9119e-05 eta 0:01:00
epoch [180/200] batch [2/3] time 0.713 (0.845) data 0.000 (0.133) loss 0.6372 (0.5045) acc 81.2500 (84.3750) lr 5.9119e-05 eta 0:00:51
epoch [180/200] batch [3/3] time 0.715 (0.802) data 0.000 (0.088) loss 0.5659 (0.5250) acc 84.3750 (84.3750) lr 5.3915e-05 eta 0:00:48
epoch [181/200] batch [1/3] time 0.992 (0.992) data 0.283 (0.283) loss 0.4990 (0.4990) acc 90.6250 (90.6250) lr 5.3915e-05 eta 0:00:58
epoch [181/200] batch [2/3] time 0.712 (0.852) data 0.000 (0.142) loss 0.1780 (0.3385) acc 96.8750 (93.7500) lr 5.3915e-05 eta 0:00:49
epoch [181/200] batch [3/3] time 0.712 (0.805) data 0.000 (0.094) loss 0.4468 (0.3746) acc 93.7500 (93.7500) lr 4.8943e-05 eta 0:00:45
epoch [182/200] batch [1/3] time 0.982 (0.982) data 0.272 (0.272) loss 0.3745 (0.3745) acc 90.6250 (90.6250) lr 4.8943e-05 eta 0:00:54
epoch [182/200] batch [2/3] time 0.712 (0.847) data 0.000 (0.136) loss 0.5098 (0.4421) acc 87.5000 (89.0625) lr 4.8943e-05 eta 0:00:46
epoch [182/200] batch [3/3] time 0.712 (0.802) data 0.000 (0.091) loss 0.4951 (0.4598) acc 87.5000 (88.5417) lr 4.4207e-05 eta 0:00:43
epoch [183/200] batch [1/3] time 0.982 (0.982) data 0.273 (0.273) loss 0.3306 (0.3306) acc 93.7500 (93.7500) lr 4.4207e-05 eta 0:00:52
epoch [183/200] batch [2/3] time 0.712 (0.847) data 0.000 (0.137) loss 0.3052 (0.3179) acc 93.7500 (93.7500) lr 4.4207e-05 eta 0:00:44
epoch [183/200] batch [3/3] time 0.714 (0.803) data 0.000 (0.091) loss 0.6562 (0.4307) acc 81.2500 (89.5833) lr 3.9706e-05 eta 0:00:40
epoch [184/200] batch [1/3] time 0.990 (0.990) data 0.281 (0.281) loss 0.7095 (0.7095) acc 84.3750 (84.3750) lr 3.9706e-05 eta 0:00:49
epoch [184/200] batch [2/3] time 0.712 (0.851) data 0.000 (0.141) loss 0.1416 (0.4255) acc 100.0000 (92.1875) lr 3.9706e-05 eta 0:00:41
epoch [184/200] batch [3/3] time 0.713 (0.805) data 0.000 (0.094) loss 0.4036 (0.4182) acc 90.6250 (91.6667) lr 3.5443e-05 eta 0:00:38
epoch [185/200] batch [1/3] time 0.979 (0.979) data 0.265 (0.265) loss 0.6978 (0.6978) acc 78.1250 (78.1250) lr 3.5443e-05 eta 0:00:45
epoch [185/200] batch [2/3] time 0.712 (0.845) data 0.000 (0.133) loss 0.2554 (0.4766) acc 93.7500 (85.9375) lr 3.5443e-05 eta 0:00:38
epoch [185/200] batch [3/3] time 0.713 (0.801) data 0.000 (0.089) loss 0.3040 (0.4190) acc 90.6250 (87.5000) lr 3.1417e-05 eta 0:00:36
epoch [186/200] batch [1/3] time 0.985 (0.985) data 0.273 (0.273) loss 0.3733 (0.3733) acc 87.5000 (87.5000) lr 3.1417e-05 eta 0:00:43
epoch [186/200] batch [2/3] time 0.712 (0.848) data 0.000 (0.137) loss 0.3047 (0.3390) acc 90.6250 (89.0625) lr 3.1417e-05 eta 0:00:36
epoch [186/200] batch [3/3] time 0.713 (0.803) data 0.000 (0.091) loss 0.4695 (0.3825) acc 84.3750 (87.5000) lr 2.7630e-05 eta 0:00:33
epoch [187/200] batch [1/3] time 0.986 (0.986) data 0.274 (0.274) loss 0.7505 (0.7505) acc 78.1250 (78.1250) lr 2.7630e-05 eta 0:00:40
epoch [187/200] batch [2/3] time 0.711 (0.848) data 0.000 (0.137) loss 0.1641 (0.4573) acc 100.0000 (89.0625) lr 2.7630e-05 eta 0:00:33
epoch [187/200] batch [3/3] time 0.712 (0.803) data 0.000 (0.092) loss 0.3433 (0.4193) acc 90.6250 (89.5833) lr 2.4083e-05 eta 0:00:31
epoch [188/200] batch [1/3] time 0.995 (0.995) data 0.283 (0.283) loss 0.5669 (0.5669) acc 90.6250 (90.6250) lr 2.4083e-05 eta 0:00:37
epoch [188/200] batch [2/3] time 0.712 (0.853) data 0.000 (0.142) loss 0.4270 (0.4969) acc 84.3750 (87.5000) lr 2.4083e-05 eta 0:00:31
epoch [188/200] batch [3/3] time 0.713 (0.807) data 0.000 (0.094) loss 0.2437 (0.4125) acc 90.6250 (88.5417) lr 2.0777e-05 eta 0:00:29
epoch [189/200] batch [1/3] time 0.994 (0.994) data 0.281 (0.281) loss 0.6968 (0.6968) acc 84.3750 (84.3750) lr 2.0777e-05 eta 0:00:34
epoch [189/200] batch [2/3] time 0.714 (0.854) data 0.000 (0.141) loss 0.3789 (0.5378) acc 87.5000 (85.9375) lr 2.0777e-05 eta 0:00:29
epoch [189/200] batch [3/3] time 0.713 (0.807) data 0.000 (0.094) loss 0.5815 (0.5524) acc 87.5000 (86.4583) lr 1.7713e-05 eta 0:00:26
epoch [190/200] batch [1/3] time 0.992 (0.992) data 0.281 (0.281) loss 0.2223 (0.2223) acc 96.8750 (96.8750) lr 1.7713e-05 eta 0:00:31
epoch [190/200] batch [2/3] time 0.712 (0.852) data 0.000 (0.140) loss 0.2622 (0.2422) acc 90.6250 (93.7500) lr 1.7713e-05 eta 0:00:26
epoch [190/200] batch [3/3] time 0.712 (0.805) data 0.000 (0.094) loss 0.6094 (0.3646) acc 81.2500 (89.5833) lr 1.4891e-05 eta 0:00:24
epoch [191/200] batch [1/3] time 0.995 (0.995) data 0.282 (0.282) loss 0.2445 (0.2445) acc 90.6250 (90.6250) lr 1.4891e-05 eta 0:00:28
epoch [191/200] batch [2/3] time 0.712 (0.853) data 0.000 (0.141) loss 0.4985 (0.3715) acc 81.2500 (85.9375) lr 1.4891e-05 eta 0:00:23
epoch [191/200] batch [3/3] time 0.712 (0.806) data 0.000 (0.094) loss 0.4102 (0.3844) acc 84.3750 (85.4167) lr 1.2312e-05 eta 0:00:21
epoch [192/200] batch [1/3] time 0.986 (0.986) data 0.275 (0.275) loss 0.3337 (0.3337) acc 90.6250 (90.6250) lr 1.2312e-05 eta 0:00:25
epoch [192/200] batch [2/3] time 0.712 (0.849) data 0.000 (0.137) loss 0.2910 (0.3124) acc 93.7500 (92.1875) lr 1.2312e-05 eta 0:00:21
epoch [192/200] batch [3/3] time 0.712 (0.803) data 0.000 (0.092) loss 0.1996 (0.2748) acc 93.7500 (92.7083) lr 9.9763e-06 eta 0:00:19
epoch [193/200] batch [1/3] time 0.976 (0.976) data 0.265 (0.265) loss 0.5034 (0.5034) acc 87.5000 (87.5000) lr 9.9763e-06 eta 0:00:22
epoch [193/200] batch [2/3] time 0.712 (0.844) data 0.000 (0.133) loss 0.4338 (0.4686) acc 87.5000 (87.5000) lr 9.9763e-06 eta 0:00:18
epoch [193/200] batch [3/3] time 0.712 (0.800) data 0.000 (0.088) loss 0.3118 (0.4163) acc 90.6250 (88.5417) lr 7.8853e-06 eta 0:00:16
epoch [194/200] batch [1/3] time 0.994 (0.994) data 0.285 (0.285) loss 0.3716 (0.3716) acc 93.7500 (93.7500) lr 7.8853e-06 eta 0:00:19
epoch [194/200] batch [2/3] time 0.712 (0.853) data 0.000 (0.143) loss 0.3079 (0.3397) acc 93.7500 (93.7500) lr 7.8853e-06 eta 0:00:16
epoch [194/200] batch [3/3] time 0.712 (0.806) data 0.000 (0.095) loss 0.2732 (0.3175) acc 93.7500 (93.7500) lr 6.0390e-06 eta 0:00:14
epoch [195/200] batch [1/3] time 0.977 (0.977) data 0.268 (0.268) loss 0.4985 (0.4985) acc 93.7500 (93.7500) lr 6.0390e-06 eta 0:00:16
epoch [195/200] batch [2/3] time 0.712 (0.845) data 0.000 (0.134) loss 0.3894 (0.4440) acc 84.3750 (89.0625) lr 6.0390e-06 eta 0:00:13
epoch [195/200] batch [3/3] time 0.714 (0.801) data 0.000 (0.089) loss 0.4180 (0.4353) acc 93.7500 (90.6250) lr 4.4380e-06 eta 0:00:12
epoch [196/200] batch [1/3] time 0.991 (0.991) data 0.282 (0.282) loss 0.4497 (0.4497) acc 90.6250 (90.6250) lr 4.4380e-06 eta 0:00:13
epoch [196/200] batch [2/3] time 0.712 (0.852) data 0.000 (0.141) loss 0.4971 (0.4734) acc 87.5000 (89.0625) lr 4.4380e-06 eta 0:00:11
epoch [196/200] batch [3/3] time 0.712 (0.805) data 0.000 (0.094) loss 0.6230 (0.5233) acc 87.5000 (88.5417) lr 3.0827e-06 eta 0:00:09
epoch [197/200] batch [1/3] time 0.983 (0.983) data 0.274 (0.274) loss 0.2267 (0.2267) acc 93.7500 (93.7500) lr 3.0827e-06 eta 0:00:10
epoch [197/200] batch [2/3] time 0.712 (0.848) data 0.000 (0.137) loss 0.3557 (0.2912) acc 90.6250 (92.1875) lr 3.0827e-06 eta 0:00:08
epoch [197/200] batch [3/3] time 0.712 (0.803) data 0.000 (0.091) loss 0.2759 (0.2861) acc 93.7500 (92.7083) lr 1.9733e-06 eta 0:00:07
epoch [198/200] batch [1/3] time 0.991 (0.991) data 0.280 (0.280) loss 0.3892 (0.3892) acc 90.6250 (90.6250) lr 1.9733e-06 eta 0:00:07
epoch [198/200] batch [2/3] time 0.712 (0.852) data 0.000 (0.140) loss 0.1750 (0.2821) acc 96.8750 (93.7500) lr 1.9733e-06 eta 0:00:05
epoch [198/200] batch [3/3] time 0.713 (0.805) data 0.000 (0.093) loss 0.2739 (0.2794) acc 96.8750 (94.7917) lr 1.1101e-06 eta 0:00:04
epoch [199/200] batch [1/3] time 0.976 (0.976) data 0.265 (0.265) loss 0.4153 (0.4153) acc 93.7500 (93.7500) lr 1.1101e-06 eta 0:00:04
epoch [199/200] batch [2/3] time 0.712 (0.844) data 0.000 (0.133) loss 0.4124 (0.4138) acc 93.7500 (93.7500) lr 1.1101e-06 eta 0:00:03
epoch [199/200] batch [3/3] time 0.712 (0.800) data 0.000 (0.089) loss 0.2278 (0.3518) acc 93.7500 (93.7500) lr 4.9344e-07 eta 0:00:02
epoch [200/200] batch [1/3] time 0.995 (0.995) data 0.283 (0.283) loss 0.1844 (0.1844) acc 96.8750 (96.8750) lr 4.9344e-07 eta 0:00:01
epoch [200/200] batch [2/3] time 0.712 (0.854) data 0.000 (0.142) loss 0.3772 (0.2808) acc 87.5000 (92.1875) lr 4.9344e-07 eta 0:00:00
epoch [200/200] batch [3/3] time 0.715 (0.807) data 0.000 (0.094) loss 0.4966 (0.3527) acc 87.5000 (90.6250) lr 1.2337e-07 eta 0:00:00
Checkpoint saved to output/Caltech/1/2/3/prompt_learner/model.pth.tar-200
Finish training
Deploy the last-epoch model
Evaluate on the *test* set
=> result
* total: 2,465
* correct: 2,163
* accuracy: 87.7%
* error: 12.3%
* macro_f1: 82.4%
Elapsed: 0:08:46
