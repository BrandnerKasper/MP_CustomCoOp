args2: backbone=, config_file=configs/trainers/CoOp/rn101.yaml, dataset_config_file=configs/datasets/caltech101.yaml, eval_only=False, head=, load_epoch=None, model_dir=, no_train=False,  opts: ['TRAINER.COOP.N_CTX', '16', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '1'], output_dir=output/Caltech/1, resume=, root=/home/brandnerkasper/Uni/MP/MP_CustomCoOp/data, seed=1, source_domains=None, target_domains=None, trainer=CoOp, transforms=None
Setting fixed seed: 1
***************
** Arguments **
***************
config_file: configs/trainers/CoOp/rn101.yaml
csc: False
ctp: end
dataset_config_file: configs/datasets/caltech101.yaml
n_ctx: 16
opts: ['TRAINER.COOP.N_CTX', '16', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '1']
output_dir: output/Caltech/1
root: /home/brandnerkasper/Uni/MP/MP_CustomCoOp/data
seed: 1
shots: 1
trainer: CoOp
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 4
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: Caltech101
  NUM_LABELED: -1
  NUM_SHOTS: 1
  ROOT: /home/brandnerkasper/Uni/MP/MP_CustomCoOp/data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN101
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 200
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/Caltech/1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: 
    N_CTX: 16
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: 
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: CoOp
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 2.0.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 22.04.3 LTS (x86_64)
GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.35

Python version: 3.10.12 (main, Jul  5 2023, 18:54:27) [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-6.2.0-32-generic-x86_64-with-glibc2.35
Is CUDA available: True
CUDA runtime version: 11.5.119
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA GeForce GTX 970
Nvidia driver version: 525.125.06
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Architecture:                       x86_64
CPU op-mode(s):                     32-bit, 64-bit
Address sizes:                      39 bits physical, 48 bits virtual
Byte Order:                         Little Endian
CPU(s):                             4
On-line CPU(s) list:                0-3
Vendor ID:                          GenuineIntel
Model name:                         Intel(R) Xeon(R) CPU E3-1225 v3 @ 3.20GHz
CPU family:                         6
Model:                              60
Thread(s) per core:                 1
Core(s) per socket:                 4
Socket(s):                          1
Stepping:                           3
CPU max MHz:                        3600,0000
CPU min MHz:                        800,0000
BogoMIPS:                           6397.95
Flags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm cpuid_fault invpcid_single pti ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid xsaveopt dtherm ida arat pln pts md_clear flush_l1d
Virtualization:                     VT-x
L1d cache:                          128 KiB (4 instances)
L1i cache:                          128 KiB (4 instances)
L2 cache:                           1 MiB (4 instances)
L3 cache:                           8 MiB (1 instance)
NUMA node(s):                       1
NUMA node0 CPU(s):                  0-3
Vulnerability Gather data sampling: Not affected
Vulnerability Itlb multihit:        KVM: Mitigation: VMX disabled
Vulnerability L1tf:                 Mitigation; PTE Inversion; VMX conditional cache flushes, SMT disabled
Vulnerability Mds:                  Mitigation; Clear CPU buffers; SMT disabled
Vulnerability Meltdown:             Mitigation; PTI
Vulnerability Mmio stale data:      Unknown: No mitigations
Vulnerability Retbleed:             Not affected
Vulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl
Vulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization
Vulnerability Spectre v2:           Mitigation; Retpolines, IBPB conditional, IBRS_FW, STIBP disabled, RSB filling, PBRSB-eIBRS Not affected
Vulnerability Srbds:                Mitigation; Microcode
Vulnerability Tsx async abort:      Not affected

Versions of relevant libraries:
[pip3] numpy==1.25.2
[pip3] open-clip-torch==2.20.0
[pip3] torch==2.0.1
[pip3] torchaudio==2.0.2
[pip3] torchvision==0.15.2
[conda] blas                      1.0                         mkl  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] mkl                       2023.1.0         h213fc3f_46343  
[conda] mkl-service               2.4.0           py310h5eee18b_1  
[conda] mkl_fft                   1.3.6           py310h1128e8f_1  
[conda] mkl_random                1.2.2           py310h1128e8f_1  
[conda] numpy                     1.25.2          py310h5f9d8c6_0  
[conda] numpy-base                1.25.2          py310hb5e798b_0  
[conda] open-clip-torch           2.20.0                   pypi_0    pypi
[conda] pytorch                   2.0.1           py3.10_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchaudio                2.0.2               py310_cu117    pytorch
[conda] torchtriton               2.0.0                     py310    pytorch
[conda] torchvision               0.15.2              py310_cu117    pytorch
        Pillow (9.4.0)

Loading trainer: CoOp
Loading dataset: Caltech101
Reading split from /home/brandnerkasper/Uni/MP/MP_CustomCoOp/data/caltech-101/split_zhou_Caltech101.json
Creating a 1-shot dataset
Creating a 1-shot dataset
Saving preprocessed few-shot data to /home/brandnerkasper/Uni/MP/MP_CustomCoOp/data/caltech-101/split_fewshot/shot_1-seed_1.pkl
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ----------
Dataset    Caltech101
# classes  100
# train_x  100
# val      100
# test     2,465
---------  ----------
Loading CLIP (backbone: RN101)
CLIP(
  (visual): ModifiedResNet(
    (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu1): ReLU(inplace=True)
    (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu2): ReLU(inplace=True)
    (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu3): ReLU(inplace=True)
    (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (layer1): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
        (downsample): Sequential(
          (-1): AvgPool2d(kernel_size=1, stride=1, padding=0)
          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
    )
    (layer2): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
        (downsample): Sequential(
          (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
    )
    (layer3): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
        (downsample): Sequential(
          (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
      (6): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
      (7): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
      (8): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
      (9): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
      (10): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
      (11): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
      (12): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
      (13): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
      (14): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
      (15): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
      (16): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
      (17): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
      (18): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
      (19): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
      (20): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
      (21): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
      (22): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
    )
    (layer4): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
        (downsample): Sequential(
          (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
    )
    (attnpool): AttentionPool2d(
      (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
      (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
      (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
      (c_proj): Linear(in_features=2048, out_features=512, bias=True)
    )
  )
  (transformer): Transformer(
    (resblocks): Sequential(
      (0): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (6): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (7): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (8): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (9): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (10): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (11): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (token_embedding): Embedding(49408, 512)
  (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
Turning off gradients in both the image and the text encoder
Loading evaluator: Classification
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/Caltech/1/tensorboard)
epoch [1/200] batch [1/3] time 4.573 (4.573) data 0.274 (0.274) loss 3.6211 (3.6211) acc 28.1250 (28.1250) lr 1.0000e-05 eta 0:45:39
epoch [1/200] batch [2/3] time 0.904 (2.739) data 0.000 (0.137) loss 3.2695 (3.4453) acc 50.0000 (39.0625) lr 1.0000e-05 eta 0:27:17
epoch [1/200] batch [3/3] time 0.925 (2.134) data 0.000 (0.092) loss 2.8516 (3.2474) acc 46.8750 (41.6667) lr 2.0000e-03 eta 0:21:14
epoch [2/200] batch [1/3] time 1.176 (1.176) data 0.274 (0.274) loss 3.2148 (3.2148) acc 31.2500 (31.2500) lr 2.0000e-03 eta 0:11:41
epoch [2/200] batch [2/3] time 0.898 (1.037) data 0.000 (0.137) loss 2.0801 (2.6475) acc 50.0000 (40.6250) lr 2.0000e-03 eta 0:10:17
epoch [2/200] batch [3/3] time 0.898 (0.991) data 0.000 (0.091) loss 1.5039 (2.2663) acc 53.1250 (44.7917) lr 1.9999e-03 eta 0:09:48
epoch [3/200] batch [1/3] time 1.152 (1.152) data 0.251 (0.251) loss 1.5674 (1.5674) acc 62.5000 (62.5000) lr 1.9999e-03 eta 0:11:23
epoch [3/200] batch [2/3] time 0.896 (1.024) data 0.000 (0.126) loss 1.1357 (1.3516) acc 65.6250 (64.0625) lr 1.9999e-03 eta 0:10:06
epoch [3/200] batch [3/3] time 0.897 (0.982) data 0.000 (0.084) loss 1.2070 (1.3034) acc 78.1250 (68.7500) lr 1.9995e-03 eta 0:09:40
epoch [4/200] batch [1/3] time 1.136 (1.136) data 0.236 (0.236) loss 1.5684 (1.5684) acc 65.6250 (65.6250) lr 1.9995e-03 eta 0:11:10
epoch [4/200] batch [2/3] time 0.898 (1.017) data 0.000 (0.118) loss 1.1611 (1.3647) acc 62.5000 (64.0625) lr 1.9995e-03 eta 0:09:59
epoch [4/200] batch [3/3] time 0.900 (0.978) data 0.000 (0.079) loss 1.7412 (1.4902) acc 56.2500 (61.4583) lr 1.9989e-03 eta 0:09:35
epoch [5/200] batch [1/3] time 1.144 (1.144) data 0.245 (0.245) loss 1.4434 (1.4434) acc 56.2500 (56.2500) lr 1.9989e-03 eta 0:11:11
epoch [5/200] batch [2/3] time 0.902 (1.023) data 0.000 (0.122) loss 1.0605 (1.2520) acc 68.7500 (62.5000) lr 1.9989e-03 eta 0:09:59
epoch [5/200] batch [3/3] time 0.889 (0.978) data 0.000 (0.082) loss 0.8926 (1.1322) acc 75.0000 (66.6667) lr 1.9980e-03 eta 0:09:32
epoch [6/200] batch [1/3] time 1.154 (1.154) data 0.253 (0.253) loss 1.2158 (1.2158) acc 62.5000 (62.5000) lr 1.9980e-03 eta 0:11:13
epoch [6/200] batch [2/3] time 0.903 (1.028) data 0.000 (0.126) loss 0.7744 (0.9951) acc 81.2500 (71.8750) lr 1.9980e-03 eta 0:09:59
epoch [6/200] batch [3/3] time 0.889 (0.982) data 0.000 (0.084) loss 2.1289 (1.3730) acc 53.1250 (65.6250) lr 1.9969e-03 eta 0:09:31
epoch [7/200] batch [1/3] time 1.156 (1.156) data 0.253 (0.253) loss 1.2471 (1.2471) acc 71.8750 (71.8750) lr 1.9969e-03 eta 0:11:11
epoch [7/200] batch [2/3] time 0.902 (1.029) data 0.000 (0.127) loss 1.3525 (1.2998) acc 75.0000 (73.4375) lr 1.9969e-03 eta 0:09:56
epoch [7/200] batch [3/3] time 0.887 (0.982) data 0.000 (0.085) loss 1.4727 (1.3574) acc 62.5000 (69.7917) lr 1.9956e-03 eta 0:09:28
epoch [8/200] batch [1/3] time 1.147 (1.147) data 0.248 (0.248) loss 1.2490 (1.2490) acc 68.7500 (68.7500) lr 1.9956e-03 eta 0:11:02
epoch [8/200] batch [2/3] time 0.905 (1.026) data 0.000 (0.124) loss 0.8398 (1.0444) acc 81.2500 (75.0000) lr 1.9956e-03 eta 0:09:52
epoch [8/200] batch [3/3] time 0.893 (0.982) data 0.000 (0.083) loss 1.1377 (1.0755) acc 75.0000 (75.0000) lr 1.9940e-03 eta 0:09:25
epoch [9/200] batch [1/3] time 1.131 (1.131) data 0.234 (0.234) loss 1.0117 (1.0117) acc 71.8750 (71.8750) lr 1.9940e-03 eta 0:10:50
epoch [9/200] batch [2/3] time 0.897 (1.014) data 0.000 (0.117) loss 0.8994 (0.9556) acc 81.2500 (76.5625) lr 1.9940e-03 eta 0:09:41
epoch [9/200] batch [3/3] time 0.906 (0.978) data 0.000 (0.078) loss 0.9829 (0.9647) acc 68.7500 (73.9583) lr 1.9921e-03 eta 0:09:20
epoch [10/200] batch [1/3] time 1.140 (1.140) data 0.239 (0.239) loss 1.2197 (1.2197) acc 65.6250 (65.6250) lr 1.9921e-03 eta 0:10:51
epoch [10/200] batch [2/3] time 0.898 (1.019) data 0.000 (0.120) loss 0.6753 (0.9475) acc 81.2500 (73.4375) lr 1.9921e-03 eta 0:09:41
epoch [10/200] batch [3/3] time 0.903 (0.980) data 0.000 (0.080) loss 0.8555 (0.9168) acc 81.2500 (76.0417) lr 1.9900e-03 eta 0:09:18
epoch [11/200] batch [1/3] time 1.154 (1.154) data 0.251 (0.251) loss 0.9985 (0.9985) acc 75.0000 (75.0000) lr 1.9900e-03 eta 0:10:56
epoch [11/200] batch [2/3] time 0.897 (1.026) data 0.000 (0.126) loss 0.5498 (0.7742) acc 87.5000 (81.2500) lr 1.9900e-03 eta 0:09:42
epoch [11/200] batch [3/3] time 0.907 (0.986) data 0.000 (0.084) loss 1.4092 (0.9858) acc 65.6250 (76.0417) lr 1.9877e-03 eta 0:09:19
epoch [12/200] batch [1/3] time 1.154 (1.154) data 0.251 (0.251) loss 0.9165 (0.9165) acc 75.0000 (75.0000) lr 1.9877e-03 eta 0:10:53
epoch [12/200] batch [2/3] time 0.898 (1.026) data 0.000 (0.126) loss 1.4756 (1.1960) acc 65.6250 (70.3125) lr 1.9877e-03 eta 0:09:39
epoch [12/200] batch [3/3] time 0.907 (0.986) data 0.000 (0.084) loss 1.7051 (1.3657) acc 53.1250 (64.5833) lr 1.9851e-03 eta 0:09:16
epoch [13/200] batch [1/3] time 1.146 (1.146) data 0.244 (0.244) loss 0.8203 (0.8203) acc 81.2500 (81.2500) lr 1.9851e-03 eta 0:10:45
epoch [13/200] batch [2/3] time 0.890 (1.018) data 0.000 (0.122) loss 1.0488 (0.9346) acc 78.1250 (79.6875) lr 1.9851e-03 eta 0:09:32
epoch [13/200] batch [3/3] time 0.906 (0.980) data 0.000 (0.081) loss 0.9209 (0.9300) acc 71.8750 (77.0833) lr 1.9823e-03 eta 0:09:10
epoch [14/200] batch [1/3] time 1.154 (1.154) data 0.252 (0.252) loss 0.9321 (0.9321) acc 84.3750 (84.3750) lr 1.9823e-03 eta 0:10:46
epoch [14/200] batch [2/3] time 0.894 (1.024) data 0.000 (0.126) loss 1.2666 (1.0994) acc 68.7500 (76.5625) lr 1.9823e-03 eta 0:09:32
epoch [14/200] batch [3/3] time 0.904 (0.984) data 0.000 (0.084) loss 0.9995 (1.0661) acc 75.0000 (76.0417) lr 1.9792e-03 eta 0:09:08
epoch [15/200] batch [1/3] time 1.144 (1.144) data 0.241 (0.241) loss 1.1279 (1.1279) acc 75.0000 (75.0000) lr 1.9792e-03 eta 0:10:37
epoch [15/200] batch [2/3] time 0.901 (1.023) data 0.000 (0.121) loss 1.3877 (1.2578) acc 65.6250 (70.3125) lr 1.9792e-03 eta 0:09:28
epoch [15/200] batch [3/3] time 0.905 (0.983) data 0.000 (0.080) loss 0.5576 (1.0244) acc 87.5000 (76.0417) lr 1.9759e-03 eta 0:09:05
epoch [16/200] batch [1/3] time 1.146 (1.146) data 0.243 (0.243) loss 0.8130 (0.8130) acc 78.1250 (78.1250) lr 1.9759e-03 eta 0:10:35
epoch [16/200] batch [2/3] time 0.903 (1.025) data 0.000 (0.122) loss 0.7847 (0.7988) acc 84.3750 (81.2500) lr 1.9759e-03 eta 0:09:26
epoch [16/200] batch [3/3] time 0.905 (0.985) data 0.000 (0.081) loss 1.0029 (0.8669) acc 78.1250 (80.2083) lr 1.9724e-03 eta 0:09:03
epoch [17/200] batch [1/3] time 1.147 (1.147) data 0.244 (0.244) loss 1.3164 (1.3164) acc 71.8750 (71.8750) lr 1.9724e-03 eta 0:10:31
epoch [17/200] batch [2/3] time 0.906 (1.027) data 0.000 (0.122) loss 0.6509 (0.9836) acc 81.2500 (76.5625) lr 1.9724e-03 eta 0:09:24
epoch [17/200] batch [3/3] time 0.905 (0.986) data 0.000 (0.081) loss 0.9106 (0.9593) acc 75.0000 (76.0417) lr 1.9686e-03 eta 0:09:01
epoch [18/200] batch [1/3] time 1.139 (1.139) data 0.238 (0.238) loss 0.6235 (0.6235) acc 84.3750 (84.3750) lr 1.9686e-03 eta 0:10:24
epoch [18/200] batch [2/3] time 0.903 (1.021) data 0.000 (0.119) loss 1.1455 (0.8845) acc 68.7500 (76.5625) lr 1.9686e-03 eta 0:09:18
epoch [18/200] batch [3/3] time 0.902 (0.981) data 0.000 (0.079) loss 0.3909 (0.7200) acc 90.6250 (81.2500) lr 1.9646e-03 eta 0:08:55
epoch [19/200] batch [1/3] time 1.145 (1.145) data 0.250 (0.250) loss 0.5469 (0.5469) acc 90.6250 (90.6250) lr 1.9646e-03 eta 0:10:24
epoch [19/200] batch [2/3] time 0.904 (1.025) data 0.000 (0.125) loss 0.5449 (0.5459) acc 84.3750 (87.5000) lr 1.9646e-03 eta 0:09:17
epoch [19/200] batch [3/3] time 0.904 (0.985) data 0.000 (0.084) loss 1.1113 (0.7344) acc 71.8750 (82.2917) lr 1.9603e-03 eta 0:08:54
epoch [20/200] batch [1/3] time 1.126 (1.126) data 0.236 (0.236) loss 1.0029 (1.0029) acc 75.0000 (75.0000) lr 1.9603e-03 eta 0:10:10
epoch [20/200] batch [2/3] time 0.902 (1.014) data 0.000 (0.118) loss 1.1367 (1.0698) acc 71.8750 (73.4375) lr 1.9603e-03 eta 0:09:08
epoch [20/200] batch [3/3] time 0.903 (0.977) data 0.000 (0.079) loss 0.6411 (0.9269) acc 87.5000 (78.1250) lr 1.9558e-03 eta 0:08:47
epoch [21/200] batch [1/3] time 1.146 (1.146) data 0.245 (0.245) loss 0.5610 (0.5610) acc 84.3750 (84.3750) lr 1.9558e-03 eta 0:10:17
epoch [21/200] batch [2/3] time 0.902 (1.024) data 0.000 (0.123) loss 0.8647 (0.7129) acc 81.2500 (82.8125) lr 1.9558e-03 eta 0:09:10
epoch [21/200] batch [3/3] time 0.904 (0.984) data 0.000 (0.082) loss 0.2686 (0.5648) acc 93.7500 (86.4583) lr 1.9511e-03 eta 0:08:48
epoch [22/200] batch [1/3] time 1.146 (1.146) data 0.245 (0.245) loss 0.5630 (0.5630) acc 84.3750 (84.3750) lr 1.9511e-03 eta 0:10:14
epoch [22/200] batch [2/3] time 0.903 (1.025) data 0.000 (0.123) loss 0.8779 (0.7205) acc 71.8750 (78.1250) lr 1.9511e-03 eta 0:09:08
epoch [22/200] batch [3/3] time 0.903 (0.984) data 0.000 (0.082) loss 0.8286 (0.7565) acc 78.1250 (78.1250) lr 1.9461e-03 eta 0:08:45
epoch [23/200] batch [1/3] time 1.153 (1.153) data 0.254 (0.254) loss 0.5977 (0.5977) acc 81.2500 (81.2500) lr 1.9461e-03 eta 0:10:14
epoch [23/200] batch [2/3] time 0.905 (1.029) data 0.000 (0.127) loss 0.9517 (0.7747) acc 78.1250 (79.6875) lr 1.9461e-03 eta 0:09:07
epoch [23/200] batch [3/3] time 0.901 (0.986) data 0.000 (0.085) loss 0.8413 (0.7969) acc 81.2500 (80.2083) lr 1.9409e-03 eta 0:08:43
epoch [24/200] batch [1/3] time 1.141 (1.141) data 0.243 (0.243) loss 0.7866 (0.7866) acc 84.3750 (84.3750) lr 1.9409e-03 eta 0:10:04
epoch [24/200] batch [2/3] time 0.904 (1.022) data 0.000 (0.121) loss 0.5801 (0.6833) acc 87.5000 (85.9375) lr 1.9409e-03 eta 0:09:00
epoch [24/200] batch [3/3] time 0.904 (0.983) data 0.000 (0.081) loss 0.6587 (0.6751) acc 84.3750 (85.4167) lr 1.9354e-03 eta 0:08:38
epoch [25/200] batch [1/3] time 1.133 (1.133) data 0.236 (0.236) loss 0.9751 (0.9751) acc 81.2500 (81.2500) lr 1.9354e-03 eta 0:09:57
epoch [25/200] batch [2/3] time 0.903 (1.018) data 0.000 (0.118) loss 0.8701 (0.9226) acc 75.0000 (78.1250) lr 1.9354e-03 eta 0:08:55
epoch [25/200] batch [3/3] time 0.903 (0.979) data 0.000 (0.079) loss 1.0713 (0.9722) acc 81.2500 (79.1667) lr 1.9298e-03 eta 0:08:34
epoch [26/200] batch [1/3] time 1.150 (1.150) data 0.254 (0.254) loss 0.3369 (0.3369) acc 90.6250 (90.6250) lr 1.9298e-03 eta 0:10:02
epoch [26/200] batch [2/3] time 0.904 (1.027) data 0.000 (0.127) loss 0.6221 (0.4795) acc 84.3750 (87.5000) lr 1.9298e-03 eta 0:08:56
epoch [26/200] batch [3/3] time 0.904 (0.986) data 0.000 (0.085) loss 0.8037 (0.5876) acc 81.2500 (85.4167) lr 1.9239e-03 eta 0:08:34
epoch [27/200] batch [1/3] time 1.140 (1.140) data 0.236 (0.236) loss 0.4983 (0.4983) acc 87.5000 (87.5000) lr 1.9239e-03 eta 0:09:53
epoch [27/200] batch [2/3] time 0.905 (1.022) data 0.000 (0.118) loss 0.7290 (0.6136) acc 78.1250 (82.8125) lr 1.9239e-03 eta 0:08:51
epoch [27/200] batch [3/3] time 0.903 (0.982) data 0.000 (0.079) loss 0.8066 (0.6780) acc 87.5000 (84.3750) lr 1.9178e-03 eta 0:08:29
epoch [28/200] batch [1/3] time 1.145 (1.145) data 0.246 (0.246) loss 1.2832 (1.2832) acc 65.6250 (65.6250) lr 1.9178e-03 eta 0:09:53
epoch [28/200] batch [2/3] time 0.904 (1.025) data 0.000 (0.123) loss 0.7129 (0.9980) acc 81.2500 (73.4375) lr 1.9178e-03 eta 0:08:49
epoch [28/200] batch [3/3] time 0.903 (0.984) data 0.000 (0.082) loss 0.3887 (0.7949) acc 90.6250 (79.1667) lr 1.9114e-03 eta 0:08:27
epoch [29/200] batch [1/3] time 1.144 (1.144) data 0.252 (0.252) loss 0.8101 (0.8101) acc 81.2500 (81.2500) lr 1.9114e-03 eta 0:09:49
epoch [29/200] batch [2/3] time 0.905 (1.024) data 0.000 (0.126) loss 0.6357 (0.7229) acc 84.3750 (82.8125) lr 1.9114e-03 eta 0:08:46
epoch [29/200] batch [3/3] time 0.903 (0.984) data 0.000 (0.084) loss 0.4468 (0.6309) acc 90.6250 (85.4167) lr 1.9048e-03 eta 0:08:24
epoch [30/200] batch [1/3] time 1.137 (1.137) data 0.235 (0.235) loss 0.7388 (0.7388) acc 87.5000 (87.5000) lr 1.9048e-03 eta 0:09:42
epoch [30/200] batch [2/3] time 0.903 (1.020) data 0.000 (0.117) loss 0.4575 (0.5981) acc 87.5000 (87.5000) lr 1.9048e-03 eta 0:08:41
epoch [30/200] batch [3/3] time 0.903 (0.981) data 0.000 (0.078) loss 1.3115 (0.8359) acc 71.8750 (82.2917) lr 1.8980e-03 eta 0:08:20
epoch [31/200] batch [1/3] time 1.148 (1.148) data 0.244 (0.244) loss 1.2822 (1.2822) acc 75.0000 (75.0000) lr 1.8980e-03 eta 0:09:44
epoch [31/200] batch [2/3] time 0.904 (1.026) data 0.000 (0.122) loss 0.9116 (1.0969) acc 78.1250 (76.5625) lr 1.8980e-03 eta 0:08:41
epoch [31/200] batch [3/3] time 0.903 (0.985) data 0.000 (0.081) loss 1.2041 (1.1326) acc 71.8750 (75.0000) lr 1.8910e-03 eta 0:08:19
epoch [32/200] batch [1/3] time 1.150 (1.150) data 0.247 (0.247) loss 0.4429 (0.4429) acc 87.5000 (87.5000) lr 1.8910e-03 eta 0:09:42
epoch [32/200] batch [2/3] time 0.905 (1.027) data 0.000 (0.123) loss 1.1406 (0.7917) acc 78.1250 (82.8125) lr 1.8910e-03 eta 0:08:38
epoch [32/200] batch [3/3] time 0.905 (0.987) data 0.000 (0.082) loss 0.3345 (0.6393) acc 93.7500 (86.4583) lr 1.8838e-03 eta 0:08:17
epoch [33/200] batch [1/3] time 1.159 (1.159) data 0.259 (0.259) loss 0.7251 (0.7251) acc 81.2500 (81.2500) lr 1.8838e-03 eta 0:09:42
epoch [33/200] batch [2/3] time 0.902 (1.030) data 0.000 (0.129) loss 0.6362 (0.6807) acc 81.2500 (81.2500) lr 1.8838e-03 eta 0:08:37
epoch [33/200] batch [3/3] time 0.903 (0.988) data 0.000 (0.086) loss 0.5586 (0.6400) acc 87.5000 (83.3333) lr 1.8763e-03 eta 0:08:14
epoch [34/200] batch [1/3] time 1.134 (1.134) data 0.234 (0.234) loss 0.3970 (0.3970) acc 87.5000 (87.5000) lr 1.8763e-03 eta 0:09:27
epoch [34/200] batch [2/3] time 0.902 (1.018) data 0.001 (0.117) loss 0.5210 (0.4590) acc 87.5000 (87.5000) lr 1.8763e-03 eta 0:08:27
epoch [34/200] batch [3/3] time 0.903 (0.980) data 0.000 (0.078) loss 0.8130 (0.5770) acc 78.1250 (84.3750) lr 1.8686e-03 eta 0:08:07
epoch [35/200] batch [1/3] time 1.159 (1.159) data 0.258 (0.258) loss 0.7246 (0.7246) acc 81.2500 (81.2500) lr 1.8686e-03 eta 0:09:36
epoch [35/200] batch [2/3] time 0.902 (1.031) data 0.000 (0.129) loss 0.4749 (0.5997) acc 84.3750 (82.8125) lr 1.8686e-03 eta 0:08:31
epoch [35/200] batch [3/3] time 0.904 (0.988) data 0.000 (0.086) loss 0.7549 (0.6514) acc 87.5000 (84.3750) lr 1.8607e-03 eta 0:08:09
epoch [36/200] batch [1/3] time 1.154 (1.154) data 0.254 (0.254) loss 0.5015 (0.5015) acc 87.5000 (87.5000) lr 1.8607e-03 eta 0:09:30
epoch [36/200] batch [2/3] time 0.903 (1.029) data 0.001 (0.127) loss 0.7163 (0.6089) acc 84.3750 (85.9375) lr 1.8607e-03 eta 0:08:27
epoch [36/200] batch [3/3] time 0.903 (0.987) data 0.000 (0.085) loss 0.5405 (0.5861) acc 87.5000 (86.4583) lr 1.8526e-03 eta 0:08:05
epoch [37/200] batch [1/3] time 1.140 (1.140) data 0.237 (0.237) loss 0.4578 (0.4578) acc 87.5000 (87.5000) lr 1.8526e-03 eta 0:09:19
epoch [37/200] batch [2/3] time 0.905 (1.022) data 0.000 (0.119) loss 0.3245 (0.3911) acc 93.7500 (90.6250) lr 1.8526e-03 eta 0:08:20
epoch [37/200] batch [3/3] time 0.905 (0.983) data 0.000 (0.079) loss 0.5259 (0.4360) acc 90.6250 (90.6250) lr 1.8443e-03 eta 0:08:00
epoch [38/200] batch [1/3] time 1.154 (1.154) data 0.254 (0.254) loss 0.4607 (0.4607) acc 90.6250 (90.6250) lr 1.8443e-03 eta 0:09:23
epoch [38/200] batch [2/3] time 0.902 (1.028) data 0.000 (0.127) loss 0.8882 (0.6744) acc 75.0000 (82.8125) lr 1.8443e-03 eta 0:08:20
epoch [38/200] batch [3/3] time 0.903 (0.987) data 0.000 (0.085) loss 0.7954 (0.7148) acc 84.3750 (83.3333) lr 1.8358e-03 eta 0:07:59
epoch [39/200] batch [1/3] time 1.146 (1.146) data 0.246 (0.246) loss 0.6494 (0.6494) acc 84.3750 (84.3750) lr 1.8358e-03 eta 0:09:15
epoch [39/200] batch [2/3] time 0.905 (1.025) data 0.000 (0.123) loss 0.6694 (0.6594) acc 81.2500 (82.8125) lr 1.8358e-03 eta 0:08:16
epoch [39/200] batch [3/3] time 0.905 (0.985) data 0.000 (0.082) loss 0.9932 (0.7707) acc 84.3750 (83.3333) lr 1.8271e-03 eta 0:07:55
epoch [40/200] batch [1/3] time 1.137 (1.137) data 0.237 (0.237) loss 0.6279 (0.6279) acc 81.2500 (81.2500) lr 1.8271e-03 eta 0:09:08
epoch [40/200] batch [2/3] time 0.908 (1.022) data 0.000 (0.119) loss 0.6465 (0.6372) acc 81.2500 (81.2500) lr 1.8271e-03 eta 0:08:11
epoch [40/200] batch [3/3] time 0.903 (0.983) data 0.000 (0.079) loss 0.9497 (0.7414) acc 78.1250 (80.2083) lr 1.8181e-03 eta 0:07:51
epoch [41/200] batch [1/3] time 1.143 (1.143) data 0.242 (0.242) loss 0.5273 (0.5273) acc 84.3750 (84.3750) lr 1.8181e-03 eta 0:09:07
epoch [41/200] batch [2/3] time 0.904 (1.023) data 0.000 (0.121) loss 0.7681 (0.6477) acc 81.2500 (82.8125) lr 1.8181e-03 eta 0:08:09
epoch [41/200] batch [3/3] time 0.903 (0.983) data 0.000 (0.081) loss 0.5718 (0.6224) acc 87.5000 (84.3750) lr 1.8090e-03 eta 0:07:48
epoch [42/200] batch [1/3] time 1.141 (1.141) data 0.237 (0.237) loss 1.2100 (1.2100) acc 75.0000 (75.0000) lr 1.8090e-03 eta 0:09:02
epoch [42/200] batch [2/3] time 0.902 (1.021) data 0.000 (0.119) loss 1.0918 (1.1509) acc 78.1250 (76.5625) lr 1.8090e-03 eta 0:08:05
epoch [42/200] batch [3/3] time 0.905 (0.982) data 0.000 (0.079) loss 0.2842 (0.8620) acc 96.8750 (83.3333) lr 1.7997e-03 eta 0:07:45
epoch [43/200] batch [1/3] time 1.153 (1.153) data 0.255 (0.255) loss 0.4983 (0.4983) acc 78.1250 (78.1250) lr 1.7997e-03 eta 0:09:05
epoch [43/200] batch [2/3] time 0.904 (1.028) data 0.000 (0.127) loss 0.3481 (0.4232) acc 93.7500 (85.9375) lr 1.7997e-03 eta 0:08:05
epoch [43/200] batch [3/3] time 0.902 (0.986) data 0.000 (0.085) loss 1.0605 (0.6357) acc 81.2500 (84.3750) lr 1.7902e-03 eta 0:07:44
epoch [44/200] batch [1/3] time 1.138 (1.138) data 0.235 (0.235) loss 0.9038 (0.9038) acc 78.1250 (78.1250) lr 1.7902e-03 eta 0:08:55
epoch [44/200] batch [2/3] time 0.905 (1.022) data 0.000 (0.118) loss 0.5850 (0.7444) acc 87.5000 (82.8125) lr 1.7902e-03 eta 0:07:59
epoch [44/200] batch [3/3] time 0.905 (0.983) data 0.000 (0.078) loss 0.6499 (0.7129) acc 81.2500 (82.2917) lr 1.7804e-03 eta 0:07:39
epoch [45/200] batch [1/3] time 1.137 (1.137) data 0.235 (0.235) loss 0.5034 (0.5034) acc 87.5000 (87.5000) lr 1.7804e-03 eta 0:08:51
epoch [45/200] batch [2/3] time 0.903 (1.020) data 0.000 (0.118) loss 0.2328 (0.3681) acc 96.8750 (92.1875) lr 1.7804e-03 eta 0:07:55
epoch [45/200] batch [3/3] time 0.903 (0.981) data 0.000 (0.078) loss 0.5664 (0.4342) acc 87.5000 (90.6250) lr 1.7705e-03 eta 0:07:36
epoch [46/200] batch [1/3] time 1.134 (1.134) data 0.234 (0.234) loss 0.2988 (0.2988) acc 96.8750 (96.8750) lr 1.7705e-03 eta 0:08:46
epoch [46/200] batch [2/3] time 0.902 (1.018) data 0.000 (0.117) loss 0.5923 (0.4456) acc 81.2500 (89.0625) lr 1.7705e-03 eta 0:07:51
epoch [46/200] batch [3/3] time 0.905 (0.981) data 0.000 (0.078) loss 0.6362 (0.5091) acc 84.3750 (87.5000) lr 1.7604e-03 eta 0:07:33
epoch [47/200] batch [1/3] time 1.159 (1.159) data 0.259 (0.259) loss 0.3018 (0.3018) acc 96.8750 (96.8750) lr 1.7604e-03 eta 0:08:54
epoch [47/200] batch [2/3] time 0.904 (1.032) data 0.000 (0.130) loss 0.7134 (0.5076) acc 78.1250 (87.5000) lr 1.7604e-03 eta 0:07:54
epoch [47/200] batch [3/3] time 0.902 (0.988) data 0.000 (0.086) loss 0.7881 (0.6011) acc 81.2500 (85.4167) lr 1.7501e-03 eta 0:07:33
epoch [48/200] batch [1/3] time 1.149 (1.149) data 0.245 (0.245) loss 0.8999 (0.8999) acc 75.0000 (75.0000) lr 1.7501e-03 eta 0:08:46
epoch [48/200] batch [2/3] time 0.902 (1.025) data 0.000 (0.123) loss 0.4126 (0.6562) acc 93.7500 (84.3750) lr 1.7501e-03 eta 0:07:48
epoch [48/200] batch [3/3] time 0.905 (0.985) data 0.000 (0.082) loss 0.3513 (0.5546) acc 87.5000 (85.4167) lr 1.7396e-03 eta 0:07:29
epoch [49/200] batch [1/3] time 1.140 (1.140) data 0.237 (0.237) loss 0.2710 (0.2710) acc 96.8750 (96.8750) lr 1.7396e-03 eta 0:08:38
epoch [49/200] batch [2/3] time 0.904 (1.022) data 0.000 (0.118) loss 0.4167 (0.3439) acc 84.3750 (90.6250) lr 1.7396e-03 eta 0:07:43
epoch [49/200] batch [3/3] time 0.902 (0.982) data 0.000 (0.079) loss 0.3950 (0.3609) acc 90.6250 (90.6250) lr 1.7290e-03 eta 0:07:24
epoch [50/200] batch [1/3] time 1.156 (1.156) data 0.253 (0.253) loss 0.5156 (0.5156) acc 87.5000 (87.5000) lr 1.7290e-03 eta 0:08:42
epoch [50/200] batch [2/3] time 0.904 (1.030) data 0.000 (0.126) loss 0.5264 (0.5210) acc 84.3750 (85.9375) lr 1.7290e-03 eta 0:07:44
epoch [50/200] batch [3/3] time 0.903 (0.988) data 0.000 (0.084) loss 0.8340 (0.6253) acc 78.1250 (83.3333) lr 1.7181e-03 eta 0:07:24
epoch [51/200] batch [1/3] time 1.146 (1.146) data 0.241 (0.241) loss 0.4204 (0.4204) acc 90.6250 (90.6250) lr 1.7181e-03 eta 0:08:34
epoch [51/200] batch [2/3] time 0.900 (1.023) data 0.000 (0.120) loss 0.4478 (0.4341) acc 90.6250 (90.6250) lr 1.7181e-03 eta 0:07:38
epoch [51/200] batch [3/3] time 0.890 (0.979) data 0.000 (0.080) loss 0.5586 (0.4756) acc 87.5000 (89.5833) lr 1.7071e-03 eta 0:07:17
epoch [52/200] batch [1/3] time 1.141 (1.141) data 0.236 (0.236) loss 0.5771 (0.5771) acc 81.2500 (81.2500) lr 1.7071e-03 eta 0:08:29
epoch [52/200] batch [2/3] time 0.903 (1.022) data 0.000 (0.118) loss 0.6353 (0.6062) acc 87.5000 (84.3750) lr 1.7071e-03 eta 0:07:34
epoch [52/200] batch [3/3] time 0.891 (0.978) data 0.000 (0.079) loss 0.5181 (0.5768) acc 93.7500 (87.5000) lr 1.6959e-03 eta 0:07:14
epoch [53/200] batch [1/3] time 1.137 (1.137) data 0.236 (0.236) loss 0.3611 (0.3611) acc 96.8750 (96.8750) lr 1.6959e-03 eta 0:08:23
epoch [53/200] batch [2/3] time 0.903 (1.020) data 0.000 (0.118) loss 0.3813 (0.3712) acc 84.3750 (90.6250) lr 1.6959e-03 eta 0:07:30
epoch [53/200] batch [3/3] time 0.905 (0.982) data 0.000 (0.079) loss 0.3167 (0.3530) acc 93.7500 (91.6667) lr 1.6845e-03 eta 0:07:12
epoch [54/200] batch [1/3] time 1.147 (1.147) data 0.242 (0.242) loss 0.7510 (0.7510) acc 84.3750 (84.3750) lr 1.6845e-03 eta 0:08:24
epoch [54/200] batch [2/3] time 0.905 (1.026) data 0.000 (0.121) loss 0.8936 (0.8223) acc 78.1250 (81.2500) lr 1.6845e-03 eta 0:07:30
epoch [54/200] batch [3/3] time 0.904 (0.985) data 0.000 (0.081) loss 0.4478 (0.6974) acc 87.5000 (83.3333) lr 1.6730e-03 eta 0:07:11
epoch [55/200] batch [1/3] time 1.139 (1.139) data 0.236 (0.236) loss 1.0811 (1.0811) acc 68.7500 (68.7500) lr 1.6730e-03 eta 0:08:17
epoch [55/200] batch [2/3] time 0.903 (1.021) data 0.000 (0.118) loss 0.4875 (0.7843) acc 90.6250 (79.6875) lr 1.6730e-03 eta 0:07:25
epoch [55/200] batch [3/3] time 0.904 (0.982) data 0.000 (0.079) loss 0.5820 (0.7169) acc 90.6250 (83.3333) lr 1.6613e-03 eta 0:07:07
epoch [56/200] batch [1/3] time 1.145 (1.145) data 0.244 (0.244) loss 0.4585 (0.4585) acc 87.5000 (87.5000) lr 1.6613e-03 eta 0:08:16
epoch [56/200] batch [2/3] time 0.906 (1.026) data 0.000 (0.122) loss 0.5342 (0.4963) acc 90.6250 (89.0625) lr 1.6613e-03 eta 0:07:24
epoch [56/200] batch [3/3] time 0.903 (0.985) data 0.000 (0.081) loss 0.6592 (0.5506) acc 81.2500 (86.4583) lr 1.6494e-03 eta 0:07:05
epoch [57/200] batch [1/3] time 1.138 (1.138) data 0.237 (0.237) loss 0.6387 (0.6387) acc 87.5000 (87.5000) lr 1.6494e-03 eta 0:08:10
epoch [57/200] batch [2/3] time 0.904 (1.021) data 0.000 (0.119) loss 0.9741 (0.8064) acc 78.1250 (82.8125) lr 1.6494e-03 eta 0:07:18
epoch [57/200] batch [3/3] time 0.904 (0.982) data 0.000 (0.079) loss 0.4238 (0.6789) acc 87.5000 (84.3750) lr 1.6374e-03 eta 0:07:01
epoch [58/200] batch [1/3] time 1.159 (1.159) data 0.254 (0.254) loss 0.4192 (0.4192) acc 87.5000 (87.5000) lr 1.6374e-03 eta 0:08:16
epoch [58/200] batch [2/3] time 0.905 (1.032) data 0.000 (0.127) loss 0.8223 (0.6207) acc 84.3750 (85.9375) lr 1.6374e-03 eta 0:07:20
epoch [58/200] batch [3/3] time 0.905 (0.990) data 0.000 (0.085) loss 0.4807 (0.5741) acc 87.5000 (86.4583) lr 1.6252e-03 eta 0:07:01
epoch [59/200] batch [1/3] time 1.157 (1.157) data 0.253 (0.253) loss 0.5781 (0.5781) acc 87.5000 (87.5000) lr 1.6252e-03 eta 0:08:11
epoch [59/200] batch [2/3] time 0.904 (1.031) data 0.000 (0.127) loss 0.5225 (0.5503) acc 84.3750 (85.9375) lr 1.6252e-03 eta 0:07:16
epoch [59/200] batch [3/3] time 0.902 (0.988) data 0.000 (0.084) loss 0.4995 (0.5334) acc 90.6250 (87.5000) lr 1.6129e-03 eta 0:06:57
epoch [60/200] batch [1/3] time 1.155 (1.155) data 0.254 (0.254) loss 0.3276 (0.3276) acc 93.7500 (93.7500) lr 1.6129e-03 eta 0:08:07
epoch [60/200] batch [2/3] time 0.904 (1.030) data 0.000 (0.127) loss 0.4644 (0.3960) acc 90.6250 (92.1875) lr 1.6129e-03 eta 0:07:13
epoch [60/200] batch [3/3] time 0.904 (0.988) data 0.000 (0.085) loss 0.7134 (0.5018) acc 84.3750 (89.5833) lr 1.6004e-03 eta 0:06:54
epoch [61/200] batch [1/3] time 1.139 (1.139) data 0.236 (0.236) loss 0.6699 (0.6699) acc 90.6250 (90.6250) lr 1.6004e-03 eta 0:07:57
epoch [61/200] batch [2/3] time 0.901 (1.020) data 0.000 (0.118) loss 0.2632 (0.4666) acc 96.8750 (93.7500) lr 1.6004e-03 eta 0:07:06
epoch [61/200] batch [3/3] time 0.903 (0.981) data 0.000 (0.079) loss 0.3499 (0.4277) acc 90.6250 (92.7083) lr 1.5878e-03 eta 0:06:49
epoch [62/200] batch [1/3] time 1.143 (1.143) data 0.240 (0.240) loss 0.5059 (0.5059) acc 90.6250 (90.6250) lr 1.5878e-03 eta 0:07:55
epoch [62/200] batch [2/3] time 0.905 (1.024) data 0.000 (0.120) loss 0.4448 (0.4753) acc 90.6250 (90.6250) lr 1.5878e-03 eta 0:07:04
epoch [62/200] batch [3/3] time 0.907 (0.985) data 0.000 (0.080) loss 0.5171 (0.4893) acc 87.5000 (89.5833) lr 1.5750e-03 eta 0:06:47
epoch [63/200] batch [1/3] time 1.144 (1.144) data 0.238 (0.238) loss 0.4644 (0.4644) acc 87.5000 (87.5000) lr 1.5750e-03 eta 0:07:52
epoch [63/200] batch [2/3] time 0.905 (1.025) data 0.000 (0.119) loss 0.5386 (0.5015) acc 81.2500 (84.3750) lr 1.5750e-03 eta 0:07:02
epoch [63/200] batch [3/3] time 0.905 (0.985) data 0.000 (0.079) loss 0.3450 (0.4493) acc 90.6250 (86.4583) lr 1.5621e-03 eta 0:06:44
epoch [64/200] batch [1/3] time 1.149 (1.149) data 0.246 (0.246) loss 0.2489 (0.2489) acc 96.8750 (96.8750) lr 1.5621e-03 eta 0:07:51
epoch [64/200] batch [2/3] time 0.904 (1.027) data 0.000 (0.123) loss 0.5649 (0.4069) acc 87.5000 (92.1875) lr 1.5621e-03 eta 0:06:59
epoch [64/200] batch [3/3] time 0.905 (0.986) data 0.000 (0.082) loss 0.7788 (0.5309) acc 81.2500 (88.5417) lr 1.5490e-03 eta 0:06:42
epoch [65/200] batch [1/3] time 1.144 (1.144) data 0.238 (0.238) loss 0.5288 (0.5288) acc 90.6250 (90.6250) lr 1.5490e-03 eta 0:07:45
epoch [65/200] batch [2/3] time 0.901 (1.023) data 0.000 (0.119) loss 0.5259 (0.5273) acc 90.6250 (90.6250) lr 1.5490e-03 eta 0:06:55
epoch [65/200] batch [3/3] time 0.906 (0.984) data 0.000 (0.079) loss 0.5215 (0.5254) acc 87.5000 (89.5833) lr 1.5358e-03 eta 0:06:38
epoch [66/200] batch [1/3] time 1.146 (1.146) data 0.242 (0.242) loss 0.6895 (0.6895) acc 81.2500 (81.2500) lr 1.5358e-03 eta 0:07:43
epoch [66/200] batch [2/3] time 0.905 (1.026) data 0.000 (0.121) loss 0.3940 (0.5417) acc 93.7500 (87.5000) lr 1.5358e-03 eta 0:06:53
epoch [66/200] batch [3/3] time 0.910 (0.987) data 0.000 (0.081) loss 0.3457 (0.4764) acc 90.6250 (88.5417) lr 1.5225e-03 eta 0:06:36
epoch [67/200] batch [1/3] time 1.149 (1.149) data 0.245 (0.245) loss 0.7847 (0.7847) acc 78.1250 (78.1250) lr 1.5225e-03 eta 0:07:40
epoch [67/200] batch [2/3] time 0.904 (1.026) data 0.000 (0.123) loss 0.4963 (0.6405) acc 87.5000 (82.8125) lr 1.5225e-03 eta 0:06:50
epoch [67/200] batch [3/3] time 0.906 (0.986) data 0.000 (0.082) loss 0.2371 (0.5060) acc 93.7500 (86.4583) lr 1.5090e-03 eta 0:06:33
epoch [68/200] batch [1/3] time 1.148 (1.148) data 0.245 (0.245) loss 0.4534 (0.4534) acc 93.7500 (93.7500) lr 1.5090e-03 eta 0:07:36
epoch [68/200] batch [2/3] time 0.906 (1.027) data 0.000 (0.123) loss 0.6104 (0.5319) acc 84.3750 (89.0625) lr 1.5090e-03 eta 0:06:47
epoch [68/200] batch [3/3] time 0.903 (0.986) data 0.000 (0.082) loss 0.8110 (0.6249) acc 87.5000 (88.5417) lr 1.4955e-03 eta 0:06:30
epoch [69/200] batch [1/3] time 1.139 (1.139) data 0.238 (0.238) loss 0.4922 (0.4922) acc 87.5000 (87.5000) lr 1.4955e-03 eta 0:07:30
epoch [69/200] batch [2/3] time 0.905 (1.022) data 0.000 (0.119) loss 0.2424 (0.3673) acc 93.7500 (90.6250) lr 1.4955e-03 eta 0:06:42
epoch [69/200] batch [3/3] time 0.904 (0.983) data 0.000 (0.079) loss 0.2839 (0.3395) acc 87.5000 (89.5833) lr 1.4818e-03 eta 0:06:26
epoch [70/200] batch [1/3] time 1.137 (1.137) data 0.235 (0.235) loss 0.8286 (0.8286) acc 81.2500 (81.2500) lr 1.4818e-03 eta 0:07:25
epoch [70/200] batch [2/3] time 0.905 (1.021) data 0.000 (0.117) loss 0.6348 (0.7317) acc 87.5000 (84.3750) lr 1.4818e-03 eta 0:06:39
epoch [70/200] batch [3/3] time 0.903 (0.982) data 0.000 (0.078) loss 0.2959 (0.5864) acc 93.7500 (87.5000) lr 1.4679e-03 eta 0:06:22
epoch [71/200] batch [1/3] time 1.140 (1.140) data 0.238 (0.238) loss 0.6133 (0.6133) acc 84.3750 (84.3750) lr 1.4679e-03 eta 0:07:23
epoch [71/200] batch [2/3] time 0.901 (1.021) data 0.000 (0.119) loss 0.0721 (0.3427) acc 100.0000 (92.1875) lr 1.4679e-03 eta 0:06:36
epoch [71/200] batch [3/3] time 0.903 (0.981) data 0.000 (0.080) loss 0.9629 (0.5494) acc 78.1250 (87.5000) lr 1.4540e-03 eta 0:06:19
epoch [72/200] batch [1/3] time 1.151 (1.151) data 0.248 (0.248) loss 0.2715 (0.2715) acc 90.6250 (90.6250) lr 1.4540e-03 eta 0:07:24
epoch [72/200] batch [2/3] time 0.905 (1.028) data 0.000 (0.124) loss 0.6904 (0.4810) acc 84.3750 (87.5000) lr 1.4540e-03 eta 0:06:35
epoch [72/200] batch [3/3] time 0.906 (0.987) data 0.000 (0.083) loss 0.2465 (0.4028) acc 93.7500 (89.5833) lr 1.4399e-03 eta 0:06:19
epoch [73/200] batch [1/3] time 1.149 (1.149) data 0.245 (0.245) loss 0.4150 (0.4150) acc 93.7500 (93.7500) lr 1.4399e-03 eta 0:07:20
epoch [73/200] batch [2/3] time 0.905 (1.027) data 0.000 (0.122) loss 0.2817 (0.3484) acc 96.8750 (95.3125) lr 1.4399e-03 eta 0:06:32
epoch [73/200] batch [3/3] time 0.903 (0.986) data 0.000 (0.082) loss 0.4309 (0.3759) acc 93.7500 (94.7917) lr 1.4258e-03 eta 0:06:15
epoch [74/200] batch [1/3] time 1.150 (1.150) data 0.244 (0.244) loss 0.3723 (0.3723) acc 93.7500 (93.7500) lr 1.4258e-03 eta 0:07:16
epoch [74/200] batch [2/3] time 0.906 (1.028) data 0.000 (0.122) loss 0.4434 (0.4078) acc 87.5000 (90.6250) lr 1.4258e-03 eta 0:06:29
epoch [74/200] batch [3/3] time 0.904 (0.987) data 0.000 (0.082) loss 0.3235 (0.3797) acc 90.6250 (90.6250) lr 1.4115e-03 eta 0:06:12
epoch [75/200] batch [1/3] time 1.145 (1.145) data 0.240 (0.240) loss 0.7017 (0.7017) acc 78.1250 (78.1250) lr 1.4115e-03 eta 0:07:11
epoch [75/200] batch [2/3] time 0.904 (1.024) data 0.000 (0.120) loss 0.3630 (0.5323) acc 90.6250 (84.3750) lr 1.4115e-03 eta 0:06:25
epoch [75/200] batch [3/3] time 0.903 (0.984) data 0.000 (0.080) loss 0.1538 (0.4062) acc 96.8750 (88.5417) lr 1.3971e-03 eta 0:06:08
epoch [76/200] batch [1/3] time 1.146 (1.146) data 0.243 (0.243) loss 0.3926 (0.3926) acc 87.5000 (87.5000) lr 1.3971e-03 eta 0:07:08
epoch [76/200] batch [2/3] time 0.904 (1.025) data 0.000 (0.121) loss 0.4734 (0.4330) acc 87.5000 (87.5000) lr 1.3971e-03 eta 0:06:22
epoch [76/200] batch [3/3] time 0.904 (0.985) data 0.000 (0.081) loss 0.4333 (0.4331) acc 90.6250 (88.5417) lr 1.3827e-03 eta 0:06:06
epoch [77/200] batch [1/3] time 1.141 (1.141) data 0.237 (0.237) loss 0.6245 (0.6245) acc 87.5000 (87.5000) lr 1.3827e-03 eta 0:07:03
epoch [77/200] batch [2/3] time 0.905 (1.023) data 0.000 (0.119) loss 0.3489 (0.4867) acc 96.8750 (92.1875) lr 1.3827e-03 eta 0:06:18
epoch [77/200] batch [3/3] time 0.906 (0.984) data 0.000 (0.079) loss 0.3296 (0.4343) acc 93.7500 (92.7083) lr 1.3681e-03 eta 0:06:03
epoch [78/200] batch [1/3] time 1.146 (1.146) data 0.243 (0.243) loss 0.6304 (0.6304) acc 81.2500 (81.2500) lr 1.3681e-03 eta 0:07:01
epoch [78/200] batch [2/3] time 0.903 (1.025) data 0.000 (0.121) loss 0.3760 (0.5032) acc 90.6250 (85.9375) lr 1.3681e-03 eta 0:06:16
epoch [78/200] batch [3/3] time 0.903 (0.984) data 0.000 (0.081) loss 0.1946 (0.4003) acc 100.0000 (90.6250) lr 1.3535e-03 eta 0:06:00
epoch [79/200] batch [1/3] time 1.141 (1.141) data 0.241 (0.241) loss 0.1848 (0.1848) acc 93.7500 (93.7500) lr 1.3535e-03 eta 0:06:56
epoch [79/200] batch [2/3] time 0.905 (1.023) data 0.000 (0.121) loss 0.4851 (0.3350) acc 84.3750 (89.0625) lr 1.3535e-03 eta 0:06:12
epoch [79/200] batch [3/3] time 0.889 (0.978) data 0.000 (0.080) loss 0.4026 (0.3575) acc 93.7500 (90.6250) lr 1.3387e-03 eta 0:05:55
epoch [80/200] batch [1/3] time 1.139 (1.139) data 0.238 (0.238) loss 0.1860 (0.1860) acc 93.7500 (93.7500) lr 1.3387e-03 eta 0:06:52
epoch [80/200] batch [2/3] time 0.906 (1.022) data 0.000 (0.119) loss 0.4727 (0.3293) acc 93.7500 (93.7500) lr 1.3387e-03 eta 0:06:09
epoch [80/200] batch [3/3] time 0.896 (0.980) data 0.000 (0.079) loss 0.2043 (0.2877) acc 96.8750 (94.7917) lr 1.3239e-03 eta 0:05:52
epoch [81/200] batch [1/3] time 1.149 (1.149) data 0.243 (0.243) loss 0.7954 (0.7954) acc 87.5000 (87.5000) lr 1.3239e-03 eta 0:06:52
epoch [81/200] batch [2/3] time 0.905 (1.027) data 0.000 (0.122) loss 0.6646 (0.7300) acc 81.2500 (84.3750) lr 1.3239e-03 eta 0:06:07
epoch [81/200] batch [3/3] time 0.893 (0.982) data 0.000 (0.081) loss 0.3591 (0.6064) acc 90.6250 (86.4583) lr 1.3090e-03 eta 0:05:50
epoch [82/200] batch [1/3] time 1.149 (1.149) data 0.244 (0.244) loss 0.6328 (0.6328) acc 81.2500 (81.2500) lr 1.3090e-03 eta 0:06:48
epoch [82/200] batch [2/3] time 0.904 (1.026) data 0.000 (0.122) loss 0.3062 (0.4695) acc 90.6250 (85.9375) lr 1.3090e-03 eta 0:06:04
epoch [82/200] batch [3/3] time 0.894 (0.982) data 0.000 (0.081) loss 0.4863 (0.4751) acc 84.3750 (85.4167) lr 1.2940e-03 eta 0:05:47
epoch [83/200] batch [1/3] time 1.153 (1.153) data 0.252 (0.252) loss 0.4749 (0.4749) acc 90.6250 (90.6250) lr 1.2940e-03 eta 0:06:46
epoch [83/200] batch [2/3] time 0.906 (1.029) data 0.000 (0.126) loss 0.4543 (0.4646) acc 87.5000 (89.0625) lr 1.2940e-03 eta 0:06:02
epoch [83/200] batch [3/3] time 0.905 (0.988) data 0.000 (0.084) loss 0.3501 (0.4264) acc 90.6250 (89.5833) lr 1.2790e-03 eta 0:05:46
epoch [84/200] batch [1/3] time 1.155 (1.155) data 0.252 (0.252) loss 0.4397 (0.4397) acc 90.6250 (90.6250) lr 1.2790e-03 eta 0:06:44
epoch [84/200] batch [2/3] time 0.905 (1.030) data 0.000 (0.126) loss 0.2211 (0.3304) acc 96.8750 (93.7500) lr 1.2790e-03 eta 0:05:59
epoch [84/200] batch [3/3] time 0.902 (0.987) data 0.000 (0.084) loss 0.3945 (0.3518) acc 93.7500 (93.7500) lr 1.2639e-03 eta 0:05:43
epoch [85/200] batch [1/3] time 1.149 (1.149) data 0.244 (0.244) loss 0.1959 (0.1959) acc 100.0000 (100.0000) lr 1.2639e-03 eta 0:06:38
epoch [85/200] batch [2/3] time 0.904 (1.027) data 0.000 (0.122) loss 0.3604 (0.2781) acc 90.6250 (95.3125) lr 1.2639e-03 eta 0:05:55
epoch [85/200] batch [3/3] time 0.905 (0.986) data 0.000 (0.081) loss 0.7188 (0.4250) acc 81.2500 (90.6250) lr 1.2487e-03 eta 0:05:40
epoch [86/200] batch [1/3] time 1.159 (1.159) data 0.255 (0.255) loss 0.3657 (0.3657) acc 90.6250 (90.6250) lr 1.2487e-03 eta 0:06:38
epoch [86/200] batch [2/3] time 0.903 (1.031) data 0.000 (0.127) loss 0.3491 (0.3574) acc 87.5000 (89.0625) lr 1.2487e-03 eta 0:05:53
epoch [86/200] batch [3/3] time 0.904 (0.988) data 0.000 (0.085) loss 0.1907 (0.3018) acc 93.7500 (90.6250) lr 1.2334e-03 eta 0:05:38
epoch [87/200] batch [1/3] time 1.141 (1.141) data 0.238 (0.238) loss 0.2184 (0.2184) acc 93.7500 (93.7500) lr 1.2334e-03 eta 0:06:29
epoch [87/200] batch [2/3] time 0.904 (1.022) data 0.000 (0.119) loss 0.1855 (0.2020) acc 100.0000 (96.8750) lr 1.2334e-03 eta 0:05:47
epoch [87/200] batch [3/3] time 0.902 (0.982) data 0.000 (0.079) loss 0.4397 (0.2812) acc 81.2500 (91.6667) lr 1.2181e-03 eta 0:05:33
epoch [88/200] batch [1/3] time 1.134 (1.134) data 0.236 (0.236) loss 0.5889 (0.5889) acc 84.3750 (84.3750) lr 1.2181e-03 eta 0:06:23
epoch [88/200] batch [2/3] time 0.904 (1.019) data 0.000 (0.118) loss 0.7749 (0.6819) acc 81.2500 (82.8125) lr 1.2181e-03 eta 0:05:43
epoch [88/200] batch [3/3] time 0.903 (0.980) data 0.000 (0.079) loss 0.0638 (0.4759) acc 100.0000 (88.5417) lr 1.2028e-03 eta 0:05:29
epoch [89/200] batch [1/3] time 1.148 (1.148) data 0.244 (0.244) loss 0.8540 (0.8540) acc 75.0000 (75.0000) lr 1.2028e-03 eta 0:06:24
epoch [89/200] batch [2/3] time 0.902 (1.025) data 0.000 (0.122) loss 0.3726 (0.6133) acc 90.6250 (82.8125) lr 1.2028e-03 eta 0:05:42
epoch [89/200] batch [3/3] time 0.904 (0.985) data 0.000 (0.081) loss 0.0685 (0.4317) acc 100.0000 (88.5417) lr 1.1874e-03 eta 0:05:27
epoch [90/200] batch [1/3] time 1.141 (1.141) data 0.238 (0.238) loss 0.3000 (0.3000) acc 93.7500 (93.7500) lr 1.1874e-03 eta 0:06:18
epoch [90/200] batch [2/3] time 0.903 (1.022) data 0.000 (0.119) loss 0.5181 (0.4091) acc 90.6250 (92.1875) lr 1.1874e-03 eta 0:05:38
epoch [90/200] batch [3/3] time 0.903 (0.982) data 0.000 (0.079) loss 0.3794 (0.3992) acc 90.6250 (91.6667) lr 1.1719e-03 eta 0:05:24
epoch [91/200] batch [1/3] time 1.154 (1.154) data 0.252 (0.252) loss 0.1979 (0.1979) acc 96.8750 (96.8750) lr 1.1719e-03 eta 0:06:19
epoch [91/200] batch [2/3] time 0.906 (1.030) data 0.000 (0.126) loss 0.6890 (0.4434) acc 78.1250 (87.5000) lr 1.1719e-03 eta 0:05:37
epoch [91/200] batch [3/3] time 0.904 (0.988) data 0.000 (0.084) loss 0.1510 (0.3459) acc 100.0000 (91.6667) lr 1.1564e-03 eta 0:05:23
epoch [92/200] batch [1/3] time 1.151 (1.151) data 0.252 (0.252) loss 0.5454 (0.5454) acc 84.3750 (84.3750) lr 1.1564e-03 eta 0:06:15
epoch [92/200] batch [2/3] time 0.902 (1.026) data 0.000 (0.126) loss 0.3027 (0.4241) acc 93.7500 (89.0625) lr 1.1564e-03 eta 0:05:33
epoch [92/200] batch [3/3] time 0.904 (0.986) data 0.000 (0.084) loss 0.3020 (0.3834) acc 90.6250 (89.5833) lr 1.1409e-03 eta 0:05:19
epoch [93/200] batch [1/3] time 1.140 (1.140) data 0.236 (0.236) loss 0.6855 (0.6855) acc 84.3750 (84.3750) lr 1.1409e-03 eta 0:06:08
epoch [93/200] batch [2/3] time 0.905 (1.023) data 0.000 (0.118) loss 0.5898 (0.6377) acc 90.6250 (87.5000) lr 1.1409e-03 eta 0:05:29
epoch [93/200] batch [3/3] time 0.906 (0.984) data 0.000 (0.079) loss 0.3223 (0.5326) acc 93.7500 (89.5833) lr 1.1253e-03 eta 0:05:15
epoch [94/200] batch [1/3] time 1.153 (1.153) data 0.252 (0.252) loss 0.4712 (0.4712) acc 87.5000 (87.5000) lr 1.1253e-03 eta 0:06:08
epoch [94/200] batch [2/3] time 0.906 (1.030) data 0.000 (0.126) loss 0.6719 (0.5715) acc 87.5000 (87.5000) lr 1.1253e-03 eta 0:05:28
epoch [94/200] batch [3/3] time 0.904 (0.988) data 0.000 (0.084) loss 0.0673 (0.4035) acc 96.8750 (90.6250) lr 1.1097e-03 eta 0:05:14
epoch [95/200] batch [1/3] time 1.138 (1.138) data 0.237 (0.237) loss 0.5508 (0.5508) acc 87.5000 (87.5000) lr 1.1097e-03 eta 0:06:00
epoch [95/200] batch [2/3] time 0.904 (1.021) data 0.000 (0.118) loss 0.6333 (0.5920) acc 87.5000 (87.5000) lr 1.1097e-03 eta 0:05:22
epoch [95/200] batch [3/3] time 0.903 (0.982) data 0.000 (0.079) loss 0.4744 (0.5528) acc 90.6250 (88.5417) lr 1.0941e-03 eta 0:05:09
epoch [96/200] batch [1/3] time 1.142 (1.142) data 0.243 (0.243) loss 0.3442 (0.3442) acc 93.7500 (93.7500) lr 1.0941e-03 eta 0:05:58
epoch [96/200] batch [2/3] time 0.903 (1.023) data 0.000 (0.122) loss 0.2910 (0.3176) acc 93.7500 (93.7500) lr 1.0941e-03 eta 0:05:20
epoch [96/200] batch [3/3] time 0.904 (0.983) data 0.000 (0.081) loss 0.2910 (0.3088) acc 90.6250 (92.7083) lr 1.0785e-03 eta 0:05:06
epoch [97/200] batch [1/3] time 1.136 (1.136) data 0.236 (0.236) loss 0.4385 (0.4385) acc 90.6250 (90.6250) lr 1.0785e-03 eta 0:05:53
epoch [97/200] batch [2/3] time 0.905 (1.021) data 0.000 (0.118) loss 0.4495 (0.4440) acc 87.5000 (89.0625) lr 1.0785e-03 eta 0:05:16
epoch [97/200] batch [3/3] time 0.904 (0.982) data 0.000 (0.079) loss 0.0975 (0.3285) acc 100.0000 (92.7083) lr 1.0628e-03 eta 0:05:03
epoch [98/200] batch [1/3] time 1.139 (1.139) data 0.236 (0.236) loss 0.4924 (0.4924) acc 93.7500 (93.7500) lr 1.0628e-03 eta 0:05:50
epoch [98/200] batch [2/3] time 0.904 (1.021) data 0.000 (0.118) loss 0.3887 (0.4406) acc 87.5000 (90.6250) lr 1.0628e-03 eta 0:05:13
epoch [98/200] batch [3/3] time 0.903 (0.982) data 0.000 (0.079) loss 0.5161 (0.4657) acc 87.5000 (89.5833) lr 1.0471e-03 eta 0:05:00
epoch [99/200] batch [1/3] time 1.147 (1.147) data 0.249 (0.249) loss 0.5151 (0.5151) acc 90.6250 (90.6250) lr 1.0471e-03 eta 0:05:49
epoch [99/200] batch [2/3] time 0.905 (1.026) data 0.000 (0.125) loss 0.3777 (0.4464) acc 87.5000 (89.0625) lr 1.0471e-03 eta 0:05:11
epoch [99/200] batch [3/3] time 0.904 (0.985) data 0.000 (0.083) loss 0.6860 (0.5263) acc 84.3750 (87.5000) lr 1.0314e-03 eta 0:04:58
epoch [100/200] batch [1/3] time 1.140 (1.140) data 0.234 (0.234) loss 0.2220 (0.2220) acc 96.8750 (96.8750) lr 1.0314e-03 eta 0:05:44
epoch [100/200] batch [2/3] time 0.902 (1.021) data 0.000 (0.117) loss 0.4155 (0.3188) acc 87.5000 (92.1875) lr 1.0314e-03 eta 0:05:07
epoch [100/200] batch [3/3] time 0.902 (0.981) data 0.000 (0.078) loss 0.5547 (0.3974) acc 90.6250 (91.6667) lr 1.0157e-03 eta 0:04:54
epoch [101/200] batch [1/3] time 1.152 (1.152) data 0.250 (0.250) loss 0.1282 (0.1282) acc 96.8750 (96.8750) lr 1.0157e-03 eta 0:05:44
epoch [101/200] batch [2/3] time 0.903 (1.027) data 0.000 (0.125) loss 0.0594 (0.0938) acc 100.0000 (98.4375) lr 1.0157e-03 eta 0:05:06
epoch [101/200] batch [3/3] time 0.904 (0.986) data 0.000 (0.083) loss 0.5576 (0.2484) acc 90.6250 (95.8333) lr 1.0000e-03 eta 0:04:52
epoch [102/200] batch [1/3] time 1.231 (1.231) data 0.252 (0.252) loss 0.3816 (0.3816) acc 84.3750 (84.3750) lr 1.0000e-03 eta 0:06:04
epoch [102/200] batch [2/3] time 0.918 (1.075) data 0.000 (0.126) loss 0.2949 (0.3383) acc 90.6250 (87.5000) lr 1.0000e-03 eta 0:05:17
epoch [102/200] batch [3/3] time 0.917 (1.022) data 0.000 (0.084) loss 0.2632 (0.3132) acc 96.8750 (90.6250) lr 9.8429e-04 eta 0:05:00
epoch [103/200] batch [1/3] time 1.164 (1.164) data 0.248 (0.248) loss 0.5273 (0.5273) acc 87.5000 (87.5000) lr 9.8429e-04 eta 0:05:41
epoch [103/200] batch [2/3] time 0.909 (1.037) data 0.000 (0.124) loss 0.3730 (0.4502) acc 93.7500 (90.6250) lr 9.8429e-04 eta 0:05:02
epoch [103/200] batch [3/3] time 0.912 (0.995) data 0.000 (0.083) loss 0.3828 (0.4277) acc 90.6250 (90.6250) lr 9.6859e-04 eta 0:04:49
epoch [104/200] batch [1/3] time 1.144 (1.144) data 0.238 (0.238) loss 0.2399 (0.2399) acc 100.0000 (100.0000) lr 9.6859e-04 eta 0:05:31
epoch [104/200] batch [2/3] time 0.909 (1.027) data 0.000 (0.119) loss 0.5796 (0.4097) acc 87.5000 (93.7500) lr 9.6859e-04 eta 0:04:56
epoch [104/200] batch [3/3] time 0.911 (0.988) data 0.000 (0.079) loss 0.3086 (0.3760) acc 96.8750 (94.7917) lr 9.5289e-04 eta 0:04:44
epoch [105/200] batch [1/3] time 1.147 (1.147) data 0.243 (0.243) loss 0.1884 (0.1884) acc 100.0000 (100.0000) lr 9.5289e-04 eta 0:05:29
epoch [105/200] batch [2/3] time 0.905 (1.026) data 0.000 (0.122) loss 0.3735 (0.2809) acc 87.5000 (93.7500) lr 9.5289e-04 eta 0:04:53
epoch [105/200] batch [3/3] time 0.901 (0.985) data 0.000 (0.081) loss 0.2598 (0.2739) acc 93.7500 (93.7500) lr 9.3721e-04 eta 0:04:40
epoch [106/200] batch [1/3] time 1.129 (1.129) data 0.237 (0.237) loss 0.1498 (0.1498) acc 96.8750 (96.8750) lr 9.3721e-04 eta 0:05:20
epoch [106/200] batch [2/3] time 0.899 (1.014) data 0.000 (0.119) loss 1.0293 (0.5895) acc 81.2500 (89.0625) lr 9.3721e-04 eta 0:04:46
epoch [106/200] batch [3/3] time 0.899 (0.976) data 0.000 (0.079) loss 0.3474 (0.5088) acc 90.6250 (89.5833) lr 9.2154e-04 eta 0:04:35
epoch [107/200] batch [1/3] time 1.139 (1.139) data 0.240 (0.240) loss 0.2360 (0.2360) acc 96.8750 (96.8750) lr 9.2154e-04 eta 0:05:20
epoch [107/200] batch [2/3] time 0.899 (1.019) data 0.000 (0.120) loss 0.1826 (0.2093) acc 96.8750 (96.8750) lr 9.2154e-04 eta 0:04:45
epoch [107/200] batch [3/3] time 0.900 (0.980) data 0.000 (0.080) loss 0.4490 (0.2892) acc 90.6250 (94.7917) lr 9.0589e-04 eta 0:04:33
epoch [108/200] batch [1/3] time 1.155 (1.155) data 0.258 (0.258) loss 0.2737 (0.2737) acc 93.7500 (93.7500) lr 9.0589e-04 eta 0:05:21
epoch [108/200] batch [2/3] time 0.899 (1.027) data 0.000 (0.129) loss 0.1744 (0.2241) acc 96.8750 (95.3125) lr 9.0589e-04 eta 0:04:44
epoch [108/200] batch [3/3] time 0.898 (0.984) data 0.000 (0.086) loss 0.3982 (0.2821) acc 90.6250 (93.7500) lr 8.9027e-04 eta 0:04:31
epoch [109/200] batch [1/3] time 1.138 (1.138) data 0.240 (0.240) loss 0.5928 (0.5928) acc 84.3750 (84.3750) lr 8.9027e-04 eta 0:05:12
epoch [109/200] batch [2/3] time 0.904 (1.021) data 0.000 (0.120) loss 0.5679 (0.5803) acc 84.3750 (84.3750) lr 8.9027e-04 eta 0:04:39
epoch [109/200] batch [3/3] time 0.901 (0.981) data 0.000 (0.080) loss 0.4214 (0.5273) acc 90.6250 (86.4583) lr 8.7467e-04 eta 0:04:27
epoch [110/200] batch [1/3] time 1.140 (1.140) data 0.243 (0.243) loss 0.2898 (0.2898) acc 93.7500 (93.7500) lr 8.7467e-04 eta 0:05:10
epoch [110/200] batch [2/3] time 0.897 (1.019) data 0.000 (0.122) loss 0.6934 (0.4916) acc 87.5000 (90.6250) lr 8.7467e-04 eta 0:04:36
epoch [110/200] batch [3/3] time 0.899 (0.979) data 0.000 (0.081) loss 0.4224 (0.4685) acc 93.7500 (91.6667) lr 8.5910e-04 eta 0:04:24
epoch [111/200] batch [1/3] time 1.133 (1.133) data 0.236 (0.236) loss 0.7559 (0.7559) acc 81.2500 (81.2500) lr 8.5910e-04 eta 0:05:04
epoch [111/200] batch [2/3] time 0.900 (1.017) data 0.000 (0.118) loss 0.3120 (0.5339) acc 93.7500 (87.5000) lr 8.5910e-04 eta 0:04:32
epoch [111/200] batch [3/3] time 0.900 (0.978) data 0.000 (0.079) loss 0.2273 (0.4317) acc 90.6250 (88.5417) lr 8.4357e-04 eta 0:04:21
epoch [112/200] batch [1/3] time 1.137 (1.137) data 0.238 (0.238) loss 0.3325 (0.3325) acc 90.6250 (90.6250) lr 8.4357e-04 eta 0:05:02
epoch [112/200] batch [2/3] time 0.897 (1.017) data 0.000 (0.119) loss 0.1302 (0.2314) acc 100.0000 (95.3125) lr 8.4357e-04 eta 0:04:29
epoch [112/200] batch [3/3] time 0.899 (0.978) data 0.000 (0.079) loss 0.3628 (0.2752) acc 90.6250 (93.7500) lr 8.2807e-04 eta 0:04:18
epoch [113/200] batch [1/3] time 1.138 (1.138) data 0.242 (0.242) loss 0.5029 (0.5029) acc 87.5000 (87.5000) lr 8.2807e-04 eta 0:04:59
epoch [113/200] batch [2/3] time 0.899 (1.018) data 0.000 (0.121) loss 0.2051 (0.3540) acc 93.7500 (90.6250) lr 8.2807e-04 eta 0:04:26
epoch [113/200] batch [3/3] time 0.899 (0.978) data 0.000 (0.081) loss 0.2428 (0.3169) acc 93.7500 (91.6667) lr 8.1262e-04 eta 0:04:15
epoch [114/200] batch [1/3] time 1.149 (1.149) data 0.253 (0.253) loss 0.6152 (0.6152) acc 84.3750 (84.3750) lr 8.1262e-04 eta 0:04:58
epoch [114/200] batch [2/3] time 0.897 (1.023) data 0.000 (0.127) loss 0.0875 (0.3513) acc 100.0000 (92.1875) lr 8.1262e-04 eta 0:04:24
epoch [114/200] batch [3/3] time 0.901 (0.982) data 0.000 (0.084) loss 0.6289 (0.4439) acc 84.3750 (89.5833) lr 7.9721e-04 eta 0:04:13
epoch [115/200] batch [1/3] time 1.129 (1.129) data 0.236 (0.236) loss 0.5151 (0.5151) acc 84.3750 (84.3750) lr 7.9721e-04 eta 0:04:50
epoch [115/200] batch [2/3] time 0.897 (1.013) data 0.000 (0.118) loss 0.3486 (0.4319) acc 93.7500 (89.0625) lr 7.9721e-04 eta 0:04:19
epoch [115/200] batch [3/3] time 0.897 (0.974) data 0.000 (0.079) loss 0.4790 (0.4476) acc 87.5000 (88.5417) lr 7.8186e-04 eta 0:04:08
epoch [116/200] batch [1/3] time 1.135 (1.135) data 0.240 (0.240) loss 0.2556 (0.2556) acc 93.7500 (93.7500) lr 7.8186e-04 eta 0:04:48
epoch [116/200] batch [2/3] time 0.898 (1.016) data 0.000 (0.120) loss 0.2742 (0.2649) acc 93.7500 (93.7500) lr 7.8186e-04 eta 0:04:17
epoch [116/200] batch [3/3] time 0.899 (0.977) data 0.000 (0.080) loss 0.1532 (0.2277) acc 96.8750 (94.7917) lr 7.6655e-04 eta 0:04:06
epoch [117/200] batch [1/3] time 1.143 (1.143) data 0.248 (0.248) loss 0.2861 (0.2861) acc 96.8750 (96.8750) lr 7.6655e-04 eta 0:04:46
epoch [117/200] batch [2/3] time 0.898 (1.021) data 0.000 (0.124) loss 0.4055 (0.3458) acc 81.2500 (89.0625) lr 7.6655e-04 eta 0:04:15
epoch [117/200] batch [3/3] time 0.901 (0.981) data 0.000 (0.083) loss 0.4768 (0.3895) acc 93.7500 (90.6250) lr 7.5131e-04 eta 0:04:04
epoch [118/200] batch [1/3] time 1.134 (1.134) data 0.236 (0.236) loss 0.5137 (0.5137) acc 84.3750 (84.3750) lr 7.5131e-04 eta 0:04:41
epoch [118/200] batch [2/3] time 0.899 (1.016) data 0.000 (0.118) loss 0.3711 (0.4424) acc 90.6250 (87.5000) lr 7.5131e-04 eta 0:04:11
epoch [118/200] batch [3/3] time 0.901 (0.978) data 0.000 (0.079) loss 0.3914 (0.4254) acc 90.6250 (88.5417) lr 7.3613e-04 eta 0:04:00
epoch [119/200] batch [1/3] time 1.134 (1.134) data 0.237 (0.237) loss 0.6514 (0.6514) acc 81.2500 (81.2500) lr 7.3613e-04 eta 0:04:37
epoch [119/200] batch [2/3] time 0.898 (1.016) data 0.000 (0.119) loss 0.4338 (0.5426) acc 90.6250 (85.9375) lr 7.3613e-04 eta 0:04:07
epoch [119/200] batch [3/3] time 0.890 (0.974) data 0.000 (0.079) loss 0.1226 (0.4026) acc 96.8750 (89.5833) lr 7.2101e-04 eta 0:03:56
epoch [120/200] batch [1/3] time 1.131 (1.131) data 0.235 (0.235) loss 0.7275 (0.7275) acc 84.3750 (84.3750) lr 7.2101e-04 eta 0:04:33
epoch [120/200] batch [2/3] time 0.899 (1.015) data 0.000 (0.117) loss 0.2106 (0.4691) acc 93.7500 (89.0625) lr 7.2101e-04 eta 0:04:04
epoch [120/200] batch [3/3] time 0.899 (0.976) data 0.000 (0.078) loss 0.3005 (0.4129) acc 90.6250 (89.5833) lr 7.0596e-04 eta 0:03:54
epoch [121/200] batch [1/3] time 1.148 (1.148) data 0.249 (0.249) loss 0.5586 (0.5586) acc 90.6250 (90.6250) lr 7.0596e-04 eta 0:04:34
epoch [121/200] batch [2/3] time 0.900 (1.024) data 0.000 (0.125) loss 0.2720 (0.4153) acc 96.8750 (93.7500) lr 7.0596e-04 eta 0:04:03
epoch [121/200] batch [3/3] time 0.899 (0.982) data 0.000 (0.083) loss 0.3574 (0.3960) acc 90.6250 (92.7083) lr 6.9098e-04 eta 0:03:52
epoch [122/200] batch [1/3] time 1.139 (1.139) data 0.239 (0.239) loss 0.5610 (0.5610) acc 87.5000 (87.5000) lr 6.9098e-04 eta 0:04:28
epoch [122/200] batch [2/3] time 0.899 (1.019) data 0.000 (0.119) loss 0.4236 (0.4923) acc 90.6250 (89.0625) lr 6.9098e-04 eta 0:03:59
epoch [122/200] batch [3/3] time 0.897 (0.978) data 0.000 (0.080) loss 0.2749 (0.4198) acc 90.6250 (89.5833) lr 6.7608e-04 eta 0:03:48
epoch [123/200] batch [1/3] time 1.144 (1.144) data 0.245 (0.245) loss 0.2849 (0.2849) acc 96.8750 (96.8750) lr 6.7608e-04 eta 0:04:26
epoch [123/200] batch [2/3] time 0.890 (1.017) data 0.000 (0.122) loss 0.1240 (0.2045) acc 100.0000 (98.4375) lr 6.7608e-04 eta 0:03:55
epoch [123/200] batch [3/3] time 0.899 (0.978) data 0.000 (0.082) loss 0.3445 (0.2511) acc 90.6250 (95.8333) lr 6.6126e-04 eta 0:03:45
epoch [124/200] batch [1/3] time 1.142 (1.142) data 0.242 (0.242) loss 0.2334 (0.2334) acc 93.7500 (93.7500) lr 6.6126e-04 eta 0:04:22
epoch [124/200] batch [2/3] time 0.894 (1.018) data 0.000 (0.121) loss 0.2559 (0.2446) acc 93.7500 (93.7500) lr 6.6126e-04 eta 0:03:53
epoch [124/200] batch [3/3] time 0.899 (0.978) data 0.000 (0.081) loss 0.3435 (0.2776) acc 90.6250 (92.7083) lr 6.4653e-04 eta 0:03:43
epoch [125/200] batch [1/3] time 1.140 (1.140) data 0.239 (0.239) loss 0.2905 (0.2905) acc 87.5000 (87.5000) lr 6.4653e-04 eta 0:04:18
epoch [125/200] batch [2/3] time 0.898 (1.019) data 0.000 (0.120) loss 0.3801 (0.3353) acc 90.6250 (89.0625) lr 6.4653e-04 eta 0:03:50
epoch [125/200] batch [3/3] time 0.900 (0.979) data 0.000 (0.080) loss 0.3267 (0.3324) acc 84.3750 (87.5000) lr 6.3188e-04 eta 0:03:40
epoch [126/200] batch [1/3] time 1.127 (1.127) data 0.235 (0.235) loss 0.6880 (0.6880) acc 81.2500 (81.2500) lr 6.3188e-04 eta 0:04:12
epoch [126/200] batch [2/3] time 0.900 (1.013) data 0.000 (0.118) loss 0.2454 (0.4667) acc 93.7500 (87.5000) lr 6.3188e-04 eta 0:03:45
epoch [126/200] batch [3/3] time 0.899 (0.975) data 0.000 (0.078) loss 0.3728 (0.4354) acc 93.7500 (89.5833) lr 6.1732e-04 eta 0:03:36
epoch [127/200] batch [1/3] time 1.133 (1.133) data 0.241 (0.241) loss 0.3066 (0.3066) acc 93.7500 (93.7500) lr 6.1732e-04 eta 0:04:10
epoch [127/200] batch [2/3] time 0.899 (1.016) data 0.000 (0.121) loss 0.4380 (0.3723) acc 90.6250 (92.1875) lr 6.1732e-04 eta 0:03:43
epoch [127/200] batch [3/3] time 0.899 (0.977) data 0.000 (0.080) loss 0.4141 (0.3862) acc 90.6250 (91.6667) lr 6.0285e-04 eta 0:03:34
epoch [128/200] batch [1/3] time 1.147 (1.147) data 0.250 (0.250) loss 0.3232 (0.3232) acc 93.7500 (93.7500) lr 6.0285e-04 eta 0:04:10
epoch [128/200] batch [2/3] time 0.899 (1.023) data 0.000 (0.125) loss 0.3948 (0.3590) acc 87.5000 (90.6250) lr 6.0285e-04 eta 0:03:42
epoch [128/200] batch [3/3] time 0.898 (0.981) data 0.000 (0.083) loss 0.2900 (0.3360) acc 90.6250 (90.6250) lr 5.8849e-04 eta 0:03:32
epoch [129/200] batch [1/3] time 1.131 (1.131) data 0.233 (0.233) loss 0.3669 (0.3669) acc 84.3750 (84.3750) lr 5.8849e-04 eta 0:04:03
epoch [129/200] batch [2/3] time 0.897 (1.014) data 0.000 (0.117) loss 0.4729 (0.4199) acc 87.5000 (85.9375) lr 5.8849e-04 eta 0:03:36
epoch [129/200] batch [3/3] time 0.898 (0.975) data 0.000 (0.078) loss 0.1635 (0.3344) acc 96.8750 (89.5833) lr 5.7422e-04 eta 0:03:27
epoch [130/200] batch [1/3] time 1.141 (1.141) data 0.242 (0.242) loss 0.1086 (0.1086) acc 100.0000 (100.0000) lr 5.7422e-04 eta 0:04:01
epoch [130/200] batch [2/3] time 0.901 (1.021) data 0.000 (0.121) loss 0.2279 (0.1682) acc 93.7500 (96.8750) lr 5.7422e-04 eta 0:03:35
epoch [130/200] batch [3/3] time 0.898 (0.980) data 0.000 (0.081) loss 0.1648 (0.1671) acc 96.8750 (96.8750) lr 5.6006e-04 eta 0:03:25
epoch [131/200] batch [1/3] time 1.150 (1.150) data 0.252 (0.252) loss 0.3064 (0.3064) acc 93.7500 (93.7500) lr 5.6006e-04 eta 0:04:00
epoch [131/200] batch [2/3] time 0.895 (1.022) data 0.000 (0.126) loss 0.1364 (0.2214) acc 100.0000 (96.8750) lr 5.6006e-04 eta 0:03:32
epoch [131/200] batch [3/3] time 0.899 (0.981) data 0.000 (0.084) loss 0.2228 (0.2218) acc 96.8750 (96.8750) lr 5.4601e-04 eta 0:03:23
epoch [132/200] batch [1/3] time 1.147 (1.147) data 0.247 (0.247) loss 0.3057 (0.3057) acc 90.6250 (90.6250) lr 5.4601e-04 eta 0:03:56
epoch [132/200] batch [2/3] time 0.898 (1.023) data 0.000 (0.123) loss 0.3745 (0.3401) acc 90.6250 (90.6250) lr 5.4601e-04 eta 0:03:29
epoch [132/200] batch [3/3] time 0.899 (0.981) data 0.000 (0.082) loss 0.5332 (0.4045) acc 87.5000 (89.5833) lr 5.3207e-04 eta 0:03:20
epoch [133/200] batch [1/3] time 1.136 (1.136) data 0.238 (0.238) loss 0.1650 (0.1650) acc 96.8750 (96.8750) lr 5.3207e-04 eta 0:03:50
epoch [133/200] batch [2/3] time 0.897 (1.016) data 0.000 (0.119) loss 0.4495 (0.3073) acc 87.5000 (92.1875) lr 5.3207e-04 eta 0:03:25
epoch [133/200] batch [3/3] time 0.900 (0.977) data 0.000 (0.079) loss 0.5190 (0.3778) acc 87.5000 (90.6250) lr 5.1825e-04 eta 0:03:16
epoch [134/200] batch [1/3] time 1.147 (1.147) data 0.248 (0.248) loss 0.3428 (0.3428) acc 96.8750 (96.8750) lr 5.1825e-04 eta 0:03:49
epoch [134/200] batch [2/3] time 0.900 (1.023) data 0.000 (0.124) loss 0.2384 (0.2906) acc 96.8750 (96.8750) lr 5.1825e-04 eta 0:03:23
epoch [134/200] batch [3/3] time 0.898 (0.981) data 0.000 (0.083) loss 0.2385 (0.2732) acc 93.7500 (95.8333) lr 5.0454e-04 eta 0:03:14
epoch [135/200] batch [1/3] time 1.148 (1.148) data 0.248 (0.248) loss 0.6099 (0.6099) acc 84.3750 (84.3750) lr 5.0454e-04 eta 0:03:46
epoch [135/200] batch [2/3] time 0.898 (1.023) data 0.000 (0.124) loss 0.2869 (0.4484) acc 93.7500 (89.0625) lr 5.0454e-04 eta 0:03:20
epoch [135/200] batch [3/3] time 0.899 (0.982) data 0.000 (0.083) loss 0.2839 (0.3936) acc 93.7500 (90.6250) lr 4.9096e-04 eta 0:03:11
epoch [136/200] batch [1/3] time 1.137 (1.137) data 0.242 (0.242) loss 0.1661 (0.1661) acc 96.8750 (96.8750) lr 4.9096e-04 eta 0:03:40
epoch [136/200] batch [2/3] time 0.900 (1.019) data 0.000 (0.121) loss 0.2983 (0.2322) acc 90.6250 (93.7500) lr 4.9096e-04 eta 0:03:16
epoch [136/200] batch [3/3] time 0.901 (0.979) data 0.000 (0.081) loss 0.3481 (0.2709) acc 90.6250 (92.7083) lr 4.7750e-04 eta 0:03:08
epoch [137/200] batch [1/3] time 1.146 (1.146) data 0.248 (0.248) loss 0.1561 (0.1561) acc 96.8750 (96.8750) lr 4.7750e-04 eta 0:03:38
epoch [137/200] batch [2/3] time 0.899 (1.022) data 0.000 (0.124) loss 0.4556 (0.3058) acc 93.7500 (95.3125) lr 4.7750e-04 eta 0:03:14
epoch [137/200] batch [3/3] time 0.900 (0.982) data 0.000 (0.083) loss 0.4033 (0.3383) acc 87.5000 (92.7083) lr 4.6417e-04 eta 0:03:05
epoch [138/200] batch [1/3] time 1.146 (1.146) data 0.249 (0.249) loss 0.4480 (0.4480) acc 87.5000 (87.5000) lr 4.6417e-04 eta 0:03:35
epoch [138/200] batch [2/3] time 0.901 (1.023) data 0.000 (0.125) loss 0.2520 (0.3500) acc 90.6250 (89.0625) lr 4.6417e-04 eta 0:03:11
epoch [138/200] batch [3/3] time 0.898 (0.982) data 0.000 (0.083) loss 0.7080 (0.4693) acc 84.3750 (87.5000) lr 4.5098e-04 eta 0:03:02
epoch [139/200] batch [1/3] time 1.153 (1.153) data 0.258 (0.258) loss 0.3093 (0.3093) acc 93.7500 (93.7500) lr 4.5098e-04 eta 0:03:33
epoch [139/200] batch [2/3] time 0.900 (1.027) data 0.000 (0.129) loss 0.4070 (0.3582) acc 90.6250 (92.1875) lr 4.5098e-04 eta 0:03:08
epoch [139/200] batch [3/3] time 0.889 (0.981) data 0.000 (0.086) loss 0.1823 (0.2995) acc 93.7500 (92.7083) lr 4.3792e-04 eta 0:02:59
epoch [140/200] batch [1/3] time 1.153 (1.153) data 0.258 (0.258) loss 0.3926 (0.3926) acc 90.6250 (90.6250) lr 4.3792e-04 eta 0:03:29
epoch [140/200] batch [2/3] time 0.901 (1.027) data 0.000 (0.129) loss 0.1608 (0.2767) acc 96.8750 (93.7500) lr 4.3792e-04 eta 0:03:05
epoch [140/200] batch [3/3] time 0.899 (0.984) data 0.000 (0.086) loss 0.2333 (0.2622) acc 93.7500 (93.7500) lr 4.2499e-04 eta 0:02:57
epoch [141/200] batch [1/3] time 1.148 (1.148) data 0.253 (0.253) loss 0.1637 (0.1637) acc 100.0000 (100.0000) lr 4.2499e-04 eta 0:03:25
epoch [141/200] batch [2/3] time 0.897 (1.023) data 0.000 (0.126) loss 0.2406 (0.2021) acc 93.7500 (96.8750) lr 4.2499e-04 eta 0:03:02
epoch [141/200] batch [3/3] time 0.900 (0.982) data 0.000 (0.084) loss 0.3213 (0.2419) acc 96.8750 (96.8750) lr 4.1221e-04 eta 0:02:53
epoch [142/200] batch [1/3] time 1.147 (1.147) data 0.251 (0.251) loss 0.2854 (0.2854) acc 93.7500 (93.7500) lr 4.1221e-04 eta 0:03:21
epoch [142/200] batch [2/3] time 0.899 (1.023) data 0.000 (0.125) loss 0.2373 (0.2614) acc 90.6250 (92.1875) lr 4.1221e-04 eta 0:02:59
epoch [142/200] batch [3/3] time 0.902 (0.983) data 0.000 (0.084) loss 0.2761 (0.2663) acc 87.5000 (90.6250) lr 3.9958e-04 eta 0:02:50
epoch [143/200] batch [1/3] time 1.139 (1.139) data 0.240 (0.240) loss 0.3669 (0.3669) acc 90.6250 (90.6250) lr 3.9958e-04 eta 0:03:17
epoch [143/200] batch [2/3] time 0.898 (1.018) data 0.000 (0.120) loss 0.1985 (0.2827) acc 93.7500 (92.1875) lr 3.9958e-04 eta 0:02:55
epoch [143/200] batch [3/3] time 0.901 (0.979) data 0.000 (0.080) loss 0.3909 (0.3188) acc 90.6250 (91.6667) lr 3.8709e-04 eta 0:02:47
epoch [144/200] batch [1/3] time 1.149 (1.149) data 0.251 (0.251) loss 0.4763 (0.4763) acc 87.5000 (87.5000) lr 3.8709e-04 eta 0:03:15
epoch [144/200] batch [2/3] time 0.899 (1.024) data 0.000 (0.125) loss 0.1249 (0.3006) acc 96.8750 (92.1875) lr 3.8709e-04 eta 0:02:53
epoch [144/200] batch [3/3] time 0.901 (0.983) data 0.000 (0.084) loss 0.3135 (0.3049) acc 93.7500 (92.7083) lr 3.7476e-04 eta 0:02:45
epoch [145/200] batch [1/3] time 1.153 (1.153) data 0.254 (0.254) loss 0.4260 (0.4260) acc 93.7500 (93.7500) lr 3.7476e-04 eta 0:03:12
epoch [145/200] batch [2/3] time 0.899 (1.026) data 0.000 (0.127) loss 0.4011 (0.4136) acc 90.6250 (92.1875) lr 3.7476e-04 eta 0:02:50
epoch [145/200] batch [3/3] time 0.897 (0.983) data 0.000 (0.085) loss 0.4097 (0.4123) acc 87.5000 (90.6250) lr 3.6258e-04 eta 0:02:42
epoch [146/200] batch [1/3] time 1.133 (1.133) data 0.236 (0.236) loss 0.3145 (0.3145) acc 93.7500 (93.7500) lr 3.6258e-04 eta 0:03:05
epoch [146/200] batch [2/3] time 0.892 (1.012) data 0.000 (0.118) loss 0.1375 (0.2260) acc 100.0000 (96.8750) lr 3.6258e-04 eta 0:02:45
epoch [146/200] batch [3/3] time 0.900 (0.975) data 0.000 (0.079) loss 0.5889 (0.3469) acc 81.2500 (91.6667) lr 3.5055e-04 eta 0:02:37
epoch [147/200] batch [1/3] time 1.145 (1.145) data 0.245 (0.245) loss 0.4712 (0.4712) acc 87.5000 (87.5000) lr 3.5055e-04 eta 0:03:04
epoch [147/200] batch [2/3] time 0.897 (1.021) data 0.000 (0.123) loss 0.1798 (0.3255) acc 93.7500 (90.6250) lr 3.5055e-04 eta 0:02:43
epoch [147/200] batch [3/3] time 0.899 (0.980) data 0.000 (0.082) loss 0.3127 (0.3212) acc 93.7500 (91.6667) lr 3.3869e-04 eta 0:02:35
epoch [148/200] batch [1/3] time 1.135 (1.135) data 0.235 (0.235) loss 0.4390 (0.4390) acc 90.6250 (90.6250) lr 3.3869e-04 eta 0:02:59
epoch [148/200] batch [2/3] time 0.899 (1.017) data 0.000 (0.118) loss 0.2372 (0.3381) acc 90.6250 (90.6250) lr 3.3869e-04 eta 0:02:39
epoch [148/200] batch [3/3] time 0.898 (0.977) data 0.000 (0.079) loss 0.2812 (0.3191) acc 90.6250 (90.6250) lr 3.2699e-04 eta 0:02:32
epoch [149/200] batch [1/3] time 1.144 (1.144) data 0.245 (0.245) loss 0.4878 (0.4878) acc 87.5000 (87.5000) lr 3.2699e-04 eta 0:02:57
epoch [149/200] batch [2/3] time 0.899 (1.021) data 0.000 (0.123) loss 0.2168 (0.3523) acc 96.8750 (92.1875) lr 3.2699e-04 eta 0:02:37
epoch [149/200] batch [3/3] time 0.892 (0.978) data 0.000 (0.082) loss 0.1318 (0.2788) acc 96.8750 (93.7500) lr 3.1545e-04 eta 0:02:29
epoch [150/200] batch [1/3] time 1.134 (1.134) data 0.235 (0.235) loss 0.2188 (0.2188) acc 93.7500 (93.7500) lr 3.1545e-04 eta 0:02:52
epoch [150/200] batch [2/3] time 0.899 (1.016) data 0.000 (0.117) loss 0.5464 (0.3826) acc 90.6250 (92.1875) lr 3.1545e-04 eta 0:02:33
epoch [150/200] batch [3/3] time 0.889 (0.974) data 0.000 (0.078) loss 0.5425 (0.4359) acc 87.5000 (90.6250) lr 3.0409e-04 eta 0:02:26
epoch [151/200] batch [1/3] time 1.148 (1.148) data 0.251 (0.251) loss 0.1544 (0.1544) acc 96.8750 (96.8750) lr 3.0409e-04 eta 0:02:51
epoch [151/200] batch [2/3] time 0.900 (1.024) data 0.000 (0.125) loss 0.2032 (0.1788) acc 96.8750 (96.8750) lr 3.0409e-04 eta 0:02:31
epoch [151/200] batch [3/3] time 0.895 (0.981) data 0.000 (0.084) loss 0.4902 (0.2826) acc 93.7500 (95.8333) lr 2.9289e-04 eta 0:02:24
epoch [152/200] batch [1/3] time 1.145 (1.145) data 0.250 (0.250) loss 0.7661 (0.7661) acc 87.5000 (87.5000) lr 2.9289e-04 eta 0:02:47
epoch [152/200] batch [2/3] time 0.899 (1.022) data 0.000 (0.125) loss 0.1163 (0.4412) acc 96.8750 (92.1875) lr 2.9289e-04 eta 0:02:28
epoch [152/200] batch [3/3] time 0.899 (0.981) data 0.000 (0.083) loss 0.4641 (0.4488) acc 90.6250 (91.6667) lr 2.8187e-04 eta 0:02:21
epoch [153/200] batch [1/3] time 1.146 (1.146) data 0.253 (0.253) loss 0.1482 (0.1482) acc 100.0000 (100.0000) lr 2.8187e-04 eta 0:02:43
epoch [153/200] batch [2/3] time 0.898 (1.022) data 0.000 (0.127) loss 0.1458 (0.1470) acc 100.0000 (100.0000) lr 2.8187e-04 eta 0:02:25
epoch [153/200] batch [3/3] time 0.898 (0.980) data 0.000 (0.085) loss 0.4429 (0.2456) acc 93.7500 (97.9167) lr 2.7103e-04 eta 0:02:18
epoch [154/200] batch [1/3] time 1.140 (1.140) data 0.243 (0.243) loss 0.5078 (0.5078) acc 87.5000 (87.5000) lr 2.7103e-04 eta 0:02:39
epoch [154/200] batch [2/3] time 0.900 (1.020) data 0.000 (0.122) loss 0.1118 (0.3098) acc 100.0000 (93.7500) lr 2.7103e-04 eta 0:02:21
epoch [154/200] batch [3/3] time 0.898 (0.979) data 0.000 (0.081) loss 0.3462 (0.3219) acc 90.6250 (92.7083) lr 2.6037e-04 eta 0:02:15
epoch [155/200] batch [1/3] time 1.134 (1.134) data 0.236 (0.236) loss 0.1965 (0.1965) acc 96.8750 (96.8750) lr 2.6037e-04 eta 0:02:35
epoch [155/200] batch [2/3] time 0.899 (1.017) data 0.000 (0.118) loss 0.2133 (0.2049) acc 93.7500 (95.3125) lr 2.6037e-04 eta 0:02:18
epoch [155/200] batch [3/3] time 0.900 (0.978) data 0.000 (0.079) loss 0.7959 (0.4019) acc 78.1250 (89.5833) lr 2.4989e-04 eta 0:02:12
epoch [156/200] batch [1/3] time 1.148 (1.148) data 0.249 (0.249) loss 0.2651 (0.2651) acc 96.8750 (96.8750) lr 2.4989e-04 eta 0:02:33
epoch [156/200] batch [2/3] time 0.901 (1.024) data 0.000 (0.125) loss 0.3259 (0.2955) acc 93.7500 (95.3125) lr 2.4989e-04 eta 0:02:16
epoch [156/200] batch [3/3] time 0.899 (0.983) data 0.000 (0.083) loss 0.2815 (0.2909) acc 96.8750 (95.8333) lr 2.3959e-04 eta 0:02:09
epoch [157/200] batch [1/3] time 1.138 (1.138) data 0.239 (0.239) loss 0.2747 (0.2747) acc 90.6250 (90.6250) lr 2.3959e-04 eta 0:02:29
epoch [157/200] batch [2/3] time 0.889 (1.014) data 0.000 (0.120) loss 0.4443 (0.3595) acc 90.6250 (90.6250) lr 2.3959e-04 eta 0:02:11
epoch [157/200] batch [3/3] time 0.900 (0.976) data 0.000 (0.080) loss 0.2734 (0.3308) acc 96.8750 (92.7083) lr 2.2949e-04 eta 0:02:05
epoch [158/200] batch [1/3] time 1.139 (1.139) data 0.240 (0.240) loss 0.1685 (0.1685) acc 93.7500 (93.7500) lr 2.2949e-04 eta 0:02:25
epoch [158/200] batch [2/3] time 0.899 (1.019) data 0.000 (0.120) loss 0.4614 (0.3149) acc 90.6250 (92.1875) lr 2.2949e-04 eta 0:02:09
epoch [158/200] batch [3/3] time 0.900 (0.979) data 0.000 (0.080) loss 0.4612 (0.3637) acc 90.6250 (91.6667) lr 2.1957e-04 eta 0:02:03
epoch [159/200] batch [1/3] time 1.140 (1.140) data 0.242 (0.242) loss 0.4304 (0.4304) acc 87.5000 (87.5000) lr 2.1957e-04 eta 0:02:22
epoch [159/200] batch [2/3] time 0.903 (1.021) data 0.000 (0.121) loss 0.1901 (0.3102) acc 96.8750 (92.1875) lr 2.1957e-04 eta 0:02:06
epoch [159/200] batch [3/3] time 0.899 (0.981) data 0.000 (0.081) loss 0.2222 (0.2809) acc 96.8750 (93.7500) lr 2.0984e-04 eta 0:02:00
epoch [160/200] batch [1/3] time 1.147 (1.147) data 0.246 (0.246) loss 0.2435 (0.2435) acc 93.7500 (93.7500) lr 2.0984e-04 eta 0:02:19
epoch [160/200] batch [2/3] time 0.899 (1.023) data 0.000 (0.123) loss 0.1630 (0.2032) acc 93.7500 (93.7500) lr 2.0984e-04 eta 0:02:03
epoch [160/200] batch [3/3] time 0.900 (0.982) data 0.000 (0.082) loss 0.4575 (0.2880) acc 90.6250 (92.7083) lr 2.0032e-04 eta 0:01:57
epoch [161/200] batch [1/3] time 1.142 (1.142) data 0.244 (0.244) loss 0.4199 (0.4199) acc 93.7500 (93.7500) lr 2.0032e-04 eta 0:02:15
epoch [161/200] batch [2/3] time 0.900 (1.021) data 0.000 (0.122) loss 0.1256 (0.2728) acc 96.8750 (95.3125) lr 2.0032e-04 eta 0:02:00
epoch [161/200] batch [3/3] time 0.900 (0.980) data 0.000 (0.081) loss 0.4729 (0.3395) acc 90.6250 (93.7500) lr 1.9098e-04 eta 0:01:54
epoch [162/200] batch [1/3] time 1.134 (1.134) data 0.244 (0.244) loss 0.1379 (0.1379) acc 96.8750 (96.8750) lr 1.9098e-04 eta 0:02:11
epoch [162/200] batch [2/3] time 0.899 (1.017) data 0.000 (0.122) loss 0.2026 (0.1703) acc 96.8750 (96.8750) lr 1.9098e-04 eta 0:01:56
epoch [162/200] batch [3/3] time 0.894 (0.976) data 0.000 (0.081) loss 0.1140 (0.1515) acc 96.8750 (96.8750) lr 1.8185e-04 eta 0:01:51
epoch [163/200] batch [1/3] time 1.133 (1.133) data 0.232 (0.232) loss 0.2488 (0.2488) acc 96.8750 (96.8750) lr 1.8185e-04 eta 0:02:07
epoch [163/200] batch [2/3] time 0.898 (1.015) data 0.000 (0.116) loss 0.5352 (0.3920) acc 84.3750 (90.6250) lr 1.8185e-04 eta 0:01:53
epoch [163/200] batch [3/3] time 0.899 (0.977) data 0.000 (0.077) loss 0.2969 (0.3603) acc 90.6250 (90.6250) lr 1.7292e-04 eta 0:01:48
epoch [164/200] batch [1/3] time 1.131 (1.131) data 0.236 (0.236) loss 0.1276 (0.1276) acc 100.0000 (100.0000) lr 1.7292e-04 eta 0:02:04
epoch [164/200] batch [2/3] time 0.899 (1.015) data 0.000 (0.118) loss 0.5977 (0.3626) acc 90.6250 (95.3125) lr 1.7292e-04 eta 0:01:50
epoch [164/200] batch [3/3] time 0.900 (0.976) data 0.000 (0.079) loss 0.5571 (0.4274) acc 87.5000 (92.7083) lr 1.6419e-04 eta 0:01:45
epoch [165/200] batch [1/3] time 1.140 (1.140) data 0.241 (0.241) loss 0.7339 (0.7339) acc 84.3750 (84.3750) lr 1.6419e-04 eta 0:02:01
epoch [165/200] batch [2/3] time 0.899 (1.019) data 0.000 (0.121) loss 0.5635 (0.6487) acc 84.3750 (84.3750) lr 1.6419e-04 eta 0:01:48
epoch [165/200] batch [3/3] time 0.898 (0.979) data 0.000 (0.080) loss 0.4290 (0.5754) acc 90.6250 (86.4583) lr 1.5567e-04 eta 0:01:42
epoch [166/200] batch [1/3] time 1.136 (1.136) data 0.237 (0.237) loss 0.5713 (0.5713) acc 84.3750 (84.3750) lr 1.5567e-04 eta 0:01:58
epoch [166/200] batch [2/3] time 0.900 (1.018) data 0.000 (0.119) loss 0.2229 (0.3971) acc 93.7500 (89.0625) lr 1.5567e-04 eta 0:01:44
epoch [166/200] batch [3/3] time 0.900 (0.979) data 0.000 (0.079) loss 0.2147 (0.3363) acc 93.7500 (90.6250) lr 1.4736e-04 eta 0:01:39
epoch [167/200] batch [1/3] time 1.131 (1.131) data 0.233 (0.233) loss 0.2544 (0.2544) acc 96.8750 (96.8750) lr 1.4736e-04 eta 0:01:54
epoch [167/200] batch [2/3] time 0.901 (1.016) data 0.000 (0.116) loss 0.2683 (0.2614) acc 93.7500 (95.3125) lr 1.4736e-04 eta 0:01:41
epoch [167/200] batch [3/3] time 0.899 (0.977) data 0.000 (0.078) loss 0.4365 (0.3197) acc 93.7500 (94.7917) lr 1.3926e-04 eta 0:01:36
epoch [168/200] batch [1/3] time 1.143 (1.143) data 0.245 (0.245) loss 0.3799 (0.3799) acc 90.6250 (90.6250) lr 1.3926e-04 eta 0:01:51
epoch [168/200] batch [2/3] time 0.899 (1.021) data 0.000 (0.123) loss 0.1611 (0.2705) acc 96.8750 (93.7500) lr 1.3926e-04 eta 0:01:39
epoch [168/200] batch [3/3] time 0.895 (0.979) data 0.000 (0.082) loss 0.1060 (0.2157) acc 100.0000 (95.8333) lr 1.3137e-04 eta 0:01:33
epoch [169/200] batch [1/3] time 1.145 (1.145) data 0.248 (0.248) loss 0.3933 (0.3933) acc 87.5000 (87.5000) lr 1.3137e-04 eta 0:01:48
epoch [169/200] batch [2/3] time 0.899 (1.022) data 0.000 (0.124) loss 0.2905 (0.3419) acc 93.7500 (90.6250) lr 1.3137e-04 eta 0:01:36
epoch [169/200] batch [3/3] time 0.897 (0.980) data 0.000 (0.083) loss 0.2040 (0.2959) acc 96.8750 (92.7083) lr 1.2369e-04 eta 0:01:31
epoch [170/200] batch [1/3] time 1.151 (1.151) data 0.257 (0.257) loss 0.2328 (0.2328) acc 93.7500 (93.7500) lr 1.2369e-04 eta 0:01:45
epoch [170/200] batch [2/3] time 0.899 (1.025) data 0.000 (0.128) loss 0.0517 (0.1422) acc 100.0000 (96.8750) lr 1.2369e-04 eta 0:01:33
epoch [170/200] batch [3/3] time 0.899 (0.983) data 0.000 (0.086) loss 0.3718 (0.2188) acc 93.7500 (95.8333) lr 1.1623e-04 eta 0:01:28
epoch [171/200] batch [1/3] time 1.147 (1.147) data 0.252 (0.252) loss 0.2642 (0.2642) acc 87.5000 (87.5000) lr 1.1623e-04 eta 0:01:42
epoch [171/200] batch [2/3] time 0.898 (1.023) data 0.000 (0.126) loss 0.3193 (0.2917) acc 90.6250 (89.0625) lr 1.1623e-04 eta 0:01:29
epoch [171/200] batch [3/3] time 0.900 (0.982) data 0.000 (0.084) loss 0.3293 (0.3043) acc 96.8750 (91.6667) lr 1.0899e-04 eta 0:01:25
epoch [172/200] batch [1/3] time 1.134 (1.134) data 0.239 (0.239) loss 0.0636 (0.0636) acc 100.0000 (100.0000) lr 1.0899e-04 eta 0:01:37
epoch [172/200] batch [2/3] time 0.899 (1.017) data 0.000 (0.119) loss 0.2903 (0.1769) acc 96.8750 (98.4375) lr 1.0899e-04 eta 0:01:26
epoch [172/200] batch [3/3] time 0.899 (0.977) data 0.000 (0.080) loss 0.2158 (0.1899) acc 96.8750 (97.9167) lr 1.0197e-04 eta 0:01:22
epoch [173/200] batch [1/3] time 1.132 (1.132) data 0.236 (0.236) loss 0.0558 (0.0558) acc 100.0000 (100.0000) lr 1.0197e-04 eta 0:01:33
epoch [173/200] batch [2/3] time 0.899 (1.015) data 0.000 (0.118) loss 0.1915 (0.1237) acc 96.8750 (98.4375) lr 1.0197e-04 eta 0:01:23
epoch [173/200] batch [3/3] time 0.898 (0.976) data 0.000 (0.079) loss 0.4407 (0.2293) acc 87.5000 (94.7917) lr 9.5173e-05 eta 0:01:19
epoch [174/200] batch [1/3] time 1.133 (1.133) data 0.238 (0.238) loss 0.2568 (0.2568) acc 96.8750 (96.8750) lr 9.5173e-05 eta 0:01:30
epoch [174/200] batch [2/3] time 0.898 (1.016) data 0.000 (0.119) loss 0.1666 (0.2117) acc 96.8750 (96.8750) lr 9.5173e-05 eta 0:01:20
epoch [174/200] batch [3/3] time 0.898 (0.976) data 0.000 (0.079) loss 0.2377 (0.2204) acc 93.7500 (95.8333) lr 8.8597e-05 eta 0:01:16
epoch [175/200] batch [1/3] time 1.138 (1.138) data 0.239 (0.239) loss 0.1631 (0.1631) acc 96.8750 (96.8750) lr 8.8597e-05 eta 0:01:27
epoch [175/200] batch [2/3] time 0.897 (1.017) data 0.000 (0.120) loss 0.2059 (0.1845) acc 87.5000 (92.1875) lr 8.8597e-05 eta 0:01:17
epoch [175/200] batch [3/3] time 0.900 (0.978) data 0.000 (0.080) loss 0.2629 (0.2107) acc 90.6250 (91.6667) lr 8.2245e-05 eta 0:01:13
epoch [176/200] batch [1/3] time 1.151 (1.151) data 0.252 (0.252) loss 0.3052 (0.3052) acc 90.6250 (90.6250) lr 8.2245e-05 eta 0:01:25
epoch [176/200] batch [2/3] time 0.898 (1.025) data 0.000 (0.126) loss 0.3591 (0.3322) acc 87.5000 (89.0625) lr 8.2245e-05 eta 0:01:14
epoch [176/200] batch [3/3] time 0.900 (0.983) data 0.000 (0.084) loss 0.3782 (0.3475) acc 96.8750 (91.6667) lr 7.6120e-05 eta 0:01:10
epoch [177/200] batch [1/3] time 1.142 (1.142) data 0.245 (0.245) loss 0.2671 (0.2671) acc 93.7500 (93.7500) lr 7.6120e-05 eta 0:01:21
epoch [177/200] batch [2/3] time 0.899 (1.021) data 0.000 (0.123) loss 0.3452 (0.3062) acc 90.6250 (92.1875) lr 7.6120e-05 eta 0:01:11
epoch [177/200] batch [3/3] time 0.896 (0.979) data 0.000 (0.082) loss 0.0801 (0.2308) acc 96.8750 (93.7500) lr 7.0224e-05 eta 0:01:07
epoch [178/200] batch [1/3] time 1.153 (1.153) data 0.254 (0.254) loss 0.2391 (0.2391) acc 96.8750 (96.8750) lr 7.0224e-05 eta 0:01:18
epoch [178/200] batch [2/3] time 0.898 (1.025) data 0.000 (0.127) loss 0.1075 (0.1733) acc 96.8750 (96.8750) lr 7.0224e-05 eta 0:01:08
epoch [178/200] batch [3/3] time 0.898 (0.983) data 0.000 (0.085) loss 0.4841 (0.2769) acc 90.6250 (94.7917) lr 6.4556e-05 eta 0:01:04
epoch [179/200] batch [1/3] time 1.133 (1.133) data 0.234 (0.234) loss 0.1556 (0.1556) acc 96.8750 (96.8750) lr 6.4556e-05 eta 0:01:13
epoch [179/200] batch [2/3] time 0.898 (1.016) data 0.000 (0.117) loss 0.3767 (0.2662) acc 93.7500 (95.3125) lr 6.4556e-05 eta 0:01:05
epoch [179/200] batch [3/3] time 0.900 (0.977) data 0.000 (0.078) loss 0.1971 (0.2432) acc 100.0000 (96.8750) lr 5.9119e-05 eta 0:01:01
epoch [180/200] batch [1/3] time 1.148 (1.148) data 0.247 (0.247) loss 0.2285 (0.2285) acc 93.7500 (93.7500) lr 5.9119e-05 eta 0:01:11
epoch [180/200] batch [2/3] time 0.898 (1.023) data 0.000 (0.123) loss 0.2366 (0.2325) acc 93.7500 (93.7500) lr 5.9119e-05 eta 0:01:02
epoch [180/200] batch [3/3] time 0.900 (0.982) data 0.000 (0.082) loss 0.3711 (0.2787) acc 90.6250 (92.7083) lr 5.3915e-05 eta 0:00:58
epoch [181/200] batch [1/3] time 1.151 (1.151) data 0.253 (0.253) loss 0.1478 (0.1478) acc 96.8750 (96.8750) lr 5.3915e-05 eta 0:01:07
epoch [181/200] batch [2/3] time 0.900 (1.025) data 0.000 (0.127) loss 0.1650 (0.1564) acc 96.8750 (96.8750) lr 5.3915e-05 eta 0:00:59
epoch [181/200] batch [3/3] time 0.891 (0.981) data 0.000 (0.084) loss 0.3174 (0.2101) acc 90.6250 (94.7917) lr 4.8943e-05 eta 0:00:55
epoch [182/200] batch [1/3] time 1.150 (1.150) data 0.253 (0.253) loss 0.4006 (0.4006) acc 96.8750 (96.8750) lr 4.8943e-05 eta 0:01:04
epoch [182/200] batch [2/3] time 0.901 (1.026) data 0.000 (0.126) loss 0.1602 (0.2804) acc 93.7500 (95.3125) lr 4.8943e-05 eta 0:00:56
epoch [182/200] batch [3/3] time 0.892 (0.981) data 0.000 (0.084) loss 0.2167 (0.2592) acc 100.0000 (96.8750) lr 4.4207e-05 eta 0:00:52
epoch [183/200] batch [1/3] time 1.135 (1.135) data 0.237 (0.237) loss 0.7354 (0.7354) acc 87.5000 (87.5000) lr 4.4207e-05 eta 0:01:00
epoch [183/200] batch [2/3] time 0.898 (1.016) data 0.000 (0.119) loss 0.4370 (0.5862) acc 84.3750 (85.9375) lr 4.4207e-05 eta 0:00:52
epoch [183/200] batch [3/3] time 0.892 (0.975) data 0.000 (0.079) loss 0.4543 (0.5422) acc 84.3750 (85.4167) lr 3.9706e-05 eta 0:00:49
epoch [184/200] batch [1/3] time 1.152 (1.152) data 0.252 (0.252) loss 0.3325 (0.3325) acc 90.6250 (90.6250) lr 3.9706e-05 eta 0:00:57
epoch [184/200] batch [2/3] time 0.899 (1.025) data 0.000 (0.126) loss 0.3833 (0.3579) acc 90.6250 (90.6250) lr 3.9706e-05 eta 0:00:50
epoch [184/200] batch [3/3] time 0.894 (0.981) data 0.000 (0.084) loss 0.2043 (0.3067) acc 96.8750 (92.7083) lr 3.5443e-05 eta 0:00:47
epoch [185/200] batch [1/3] time 1.155 (1.155) data 0.257 (0.257) loss 0.3772 (0.3772) acc 90.6250 (90.6250) lr 3.5443e-05 eta 0:00:54
epoch [185/200] batch [2/3] time 0.899 (1.027) data 0.000 (0.129) loss 0.0825 (0.2298) acc 96.8750 (93.7500) lr 3.5443e-05 eta 0:00:47
epoch [185/200] batch [3/3] time 0.896 (0.984) data 0.000 (0.086) loss 0.2725 (0.2440) acc 96.8750 (94.7917) lr 3.1417e-05 eta 0:00:44
epoch [186/200] batch [1/3] time 1.133 (1.133) data 0.235 (0.235) loss 0.5093 (0.5093) acc 87.5000 (87.5000) lr 3.1417e-05 eta 0:00:49
epoch [186/200] batch [2/3] time 0.899 (1.016) data 0.000 (0.118) loss 0.3938 (0.4515) acc 93.7500 (90.6250) lr 3.1417e-05 eta 0:00:43
epoch [186/200] batch [3/3] time 0.900 (0.977) data 0.000 (0.078) loss 0.3059 (0.4030) acc 93.7500 (91.6667) lr 2.7630e-05 eta 0:00:41
epoch [187/200] batch [1/3] time 1.135 (1.135) data 0.238 (0.238) loss 0.0538 (0.0538) acc 100.0000 (100.0000) lr 2.7630e-05 eta 0:00:46
epoch [187/200] batch [2/3] time 0.897 (1.016) data 0.000 (0.119) loss 0.0752 (0.0645) acc 100.0000 (100.0000) lr 2.7630e-05 eta 0:00:40
epoch [187/200] batch [3/3] time 0.898 (0.977) data 0.000 (0.079) loss 0.3628 (0.1639) acc 90.6250 (96.8750) lr 2.4083e-05 eta 0:00:38
epoch [188/200] batch [1/3] time 1.146 (1.146) data 0.246 (0.246) loss 0.5220 (0.5220) acc 90.6250 (90.6250) lr 2.4083e-05 eta 0:00:43
epoch [188/200] batch [2/3] time 0.899 (1.022) data 0.000 (0.123) loss 0.1932 (0.3576) acc 100.0000 (95.3125) lr 2.4083e-05 eta 0:00:37
epoch [188/200] batch [3/3] time 0.900 (0.982) data 0.000 (0.082) loss 0.2183 (0.3112) acc 93.7500 (94.7917) lr 2.0777e-05 eta 0:00:35
epoch [189/200] batch [1/3] time 1.148 (1.148) data 0.252 (0.252) loss 0.3545 (0.3545) acc 90.6250 (90.6250) lr 2.0777e-05 eta 0:00:40
epoch [189/200] batch [2/3] time 0.897 (1.022) data 0.000 (0.126) loss 0.1427 (0.2486) acc 96.8750 (93.7500) lr 2.0777e-05 eta 0:00:34
epoch [189/200] batch [3/3] time 0.899 (0.981) data 0.000 (0.084) loss 0.1394 (0.2122) acc 96.8750 (94.7917) lr 1.7713e-05 eta 0:00:32
epoch [190/200] batch [1/3] time 1.144 (1.144) data 0.244 (0.244) loss 0.3057 (0.3057) acc 90.6250 (90.6250) lr 1.7713e-05 eta 0:00:36
epoch [190/200] batch [2/3] time 0.888 (1.016) data 0.000 (0.122) loss 0.5630 (0.4343) acc 84.3750 (87.5000) lr 1.7713e-05 eta 0:00:31
epoch [190/200] batch [3/3] time 0.900 (0.977) data 0.000 (0.081) loss 0.3826 (0.4171) acc 93.7500 (89.5833) lr 1.4891e-05 eta 0:00:29
epoch [191/200] batch [1/3] time 1.150 (1.150) data 0.251 (0.251) loss 0.2515 (0.2515) acc 96.8750 (96.8750) lr 1.4891e-05 eta 0:00:33
epoch [191/200] batch [2/3] time 0.893 (1.022) data 0.000 (0.126) loss 0.7080 (0.4797) acc 87.5000 (92.1875) lr 1.4891e-05 eta 0:00:28
epoch [191/200] batch [3/3] time 0.898 (0.980) data 0.000 (0.084) loss 0.4873 (0.4823) acc 87.5000 (90.6250) lr 1.2312e-05 eta 0:00:26
epoch [192/200] batch [1/3] time 1.151 (1.151) data 0.250 (0.250) loss 0.2255 (0.2255) acc 93.7500 (93.7500) lr 1.2312e-05 eta 0:00:29
epoch [192/200] batch [2/3] time 0.901 (1.026) data 0.000 (0.125) loss 0.6377 (0.4316) acc 84.3750 (89.0625) lr 1.2312e-05 eta 0:00:25
epoch [192/200] batch [3/3] time 0.900 (0.984) data 0.000 (0.083) loss 0.3345 (0.3992) acc 90.6250 (89.5833) lr 9.9763e-06 eta 0:00:23
epoch [193/200] batch [1/3] time 1.149 (1.149) data 0.252 (0.252) loss 0.0792 (0.0792) acc 100.0000 (100.0000) lr 9.9763e-06 eta 0:00:26
epoch [193/200] batch [2/3] time 0.898 (1.024) data 0.000 (0.126) loss 0.1237 (0.1015) acc 100.0000 (100.0000) lr 9.9763e-06 eta 0:00:22
epoch [193/200] batch [3/3] time 0.902 (0.983) data 0.000 (0.084) loss 0.2629 (0.1553) acc 93.7500 (97.9167) lr 7.8853e-06 eta 0:00:20
epoch [194/200] batch [1/3] time 1.127 (1.127) data 0.235 (0.235) loss 0.1643 (0.1643) acc 93.7500 (93.7500) lr 7.8853e-06 eta 0:00:22
epoch [194/200] batch [2/3] time 0.899 (1.013) data 0.000 (0.117) loss 0.2057 (0.1850) acc 93.7500 (93.7500) lr 7.8853e-06 eta 0:00:19
epoch [194/200] batch [3/3] time 0.900 (0.976) data 0.000 (0.078) loss 0.2030 (0.1910) acc 96.8750 (94.7917) lr 6.0390e-06 eta 0:00:17
epoch [195/200] batch [1/3] time 1.139 (1.139) data 0.251 (0.251) loss 0.4883 (0.4883) acc 87.5000 (87.5000) lr 6.0390e-06 eta 0:00:19
epoch [195/200] batch [2/3] time 0.899 (1.019) data 0.000 (0.125) loss 0.1101 (0.2992) acc 96.8750 (92.1875) lr 6.0390e-06 eta 0:00:16
epoch [195/200] batch [3/3] time 0.899 (0.979) data 0.000 (0.084) loss 0.1230 (0.2405) acc 100.0000 (94.7917) lr 4.4380e-06 eta 0:00:14
epoch [196/200] batch [1/3] time 1.147 (1.147) data 0.252 (0.252) loss 0.0346 (0.0346) acc 100.0000 (100.0000) lr 4.4380e-06 eta 0:00:16
epoch [196/200] batch [2/3] time 0.897 (1.022) data 0.000 (0.126) loss 0.1135 (0.0741) acc 100.0000 (100.0000) lr 4.4380e-06 eta 0:00:13
epoch [196/200] batch [3/3] time 0.900 (0.982) data 0.000 (0.084) loss 0.2510 (0.1330) acc 93.7500 (97.9167) lr 3.0827e-06 eta 0:00:11
epoch [197/200] batch [1/3] time 1.127 (1.127) data 0.238 (0.238) loss 0.5181 (0.5181) acc 84.3750 (84.3750) lr 3.0827e-06 eta 0:00:12
epoch [197/200] batch [2/3] time 0.899 (1.013) data 0.000 (0.119) loss 0.2028 (0.3604) acc 96.8750 (90.6250) lr 3.0827e-06 eta 0:00:10
epoch [197/200] batch [3/3] time 0.899 (0.975) data 0.000 (0.079) loss 0.5967 (0.4392) acc 78.1250 (86.4583) lr 1.9733e-06 eta 0:00:08
epoch [198/200] batch [1/3] time 1.134 (1.134) data 0.244 (0.244) loss 0.3413 (0.3413) acc 87.5000 (87.5000) lr 1.9733e-06 eta 0:00:09
epoch [198/200] batch [2/3] time 0.899 (1.016) data 0.000 (0.122) loss 0.4604 (0.4009) acc 81.2500 (84.3750) lr 1.9733e-06 eta 0:00:07
epoch [198/200] batch [3/3] time 0.900 (0.977) data 0.000 (0.081) loss 0.0626 (0.2881) acc 100.0000 (89.5833) lr 1.1101e-06 eta 0:00:05
epoch [199/200] batch [1/3] time 1.135 (1.135) data 0.235 (0.235) loss 0.3391 (0.3391) acc 87.5000 (87.5000) lr 1.1101e-06 eta 0:00:05
epoch [199/200] batch [2/3] time 0.898 (1.016) data 0.000 (0.118) loss 0.5205 (0.4298) acc 81.2500 (84.3750) lr 1.1101e-06 eta 0:00:04
epoch [199/200] batch [3/3] time 0.899 (0.977) data 0.000 (0.079) loss 0.5034 (0.4543) acc 90.6250 (86.4583) lr 4.9344e-07 eta 0:00:02
epoch [200/200] batch [1/3] time 1.148 (1.148) data 0.251 (0.251) loss 0.3069 (0.3069) acc 93.7500 (93.7500) lr 4.9344e-07 eta 0:00:02
epoch [200/200] batch [2/3] time 0.901 (1.025) data 0.000 (0.125) loss 0.2646 (0.2858) acc 93.7500 (93.7500) lr 4.9344e-07 eta 0:00:01
epoch [200/200] batch [3/3] time 0.897 (0.982) data 0.000 (0.084) loss 0.2256 (0.2657) acc 93.7500 (93.7500) lr 1.2337e-07 eta 0:00:00
Checkpoint saved to output/Caltech/1/prompt_learner/model.pth.tar-200
Finish training
Deploy the last-epoch model
Evaluate on the *test* set
=> result
* total: 2,465
* correct: 2,002
* accuracy: 81.2%
* error: 18.8%
* macro_f1: 75.8%
Elapsed: 0:10:37
args2: backbone=, config_file=configs/trainers/CoOp/rn101.yaml, dataset_config_file=configs/datasets/caltech101.yaml, eval_only=False, head=, load_epoch=None, model_dir=, no_train=False,  opts: ['TRAINER.COOP.N_CTX', '16', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '1'], output_dir=output/Caltech/1/2, resume=, root=/home/brandnerkasper/Uni/MP/MP_CustomCoOp/data, seed=2, source_domains=None, target_domains=None, trainer=CoOp, transforms=None
Setting fixed seed: 2
***************
** Arguments **
***************
config_file: configs/trainers/CoOp/rn101.yaml
csc: False
ctp: end
dataset_config_file: configs/datasets/caltech101.yaml
n_ctx: 16
opts: ['TRAINER.COOP.N_CTX', '16', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '1']
output_dir: output/Caltech/1/2
root: /home/brandnerkasper/Uni/MP/MP_CustomCoOp/data
seed: 2
shots: 1
trainer: CoOp
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 4
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: Caltech101
  NUM_LABELED: -1
  NUM_SHOTS: 1
  ROOT: /home/brandnerkasper/Uni/MP/MP_CustomCoOp/data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN101
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 200
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/Caltech/1/2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: 
    N_CTX: 16
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: 
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: CoOp
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 2.0.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 22.04.3 LTS (x86_64)
GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.35

Python version: 3.10.12 (main, Jul  5 2023, 18:54:27) [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-6.2.0-32-generic-x86_64-with-glibc2.35
Is CUDA available: True
CUDA runtime version: 11.5.119
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA GeForce GTX 970
Nvidia driver version: 525.125.06
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Architecture:                       x86_64
CPU op-mode(s):                     32-bit, 64-bit
Address sizes:                      39 bits physical, 48 bits virtual
Byte Order:                         Little Endian
CPU(s):                             4
On-line CPU(s) list:                0-3
Vendor ID:                          GenuineIntel
Model name:                         Intel(R) Xeon(R) CPU E3-1225 v3 @ 3.20GHz
CPU family:                         6
Model:                              60
Thread(s) per core:                 1
Core(s) per socket:                 4
Socket(s):                          1
Stepping:                           3
CPU max MHz:                        3600,0000
CPU min MHz:                        800,0000
BogoMIPS:                           6397.95
Flags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm cpuid_fault invpcid_single pti ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid xsaveopt dtherm ida arat pln pts md_clear flush_l1d
Virtualization:                     VT-x
L1d cache:                          128 KiB (4 instances)
L1i cache:                          128 KiB (4 instances)
L2 cache:                           1 MiB (4 instances)
L3 cache:                           8 MiB (1 instance)
NUMA node(s):                       1
NUMA node0 CPU(s):                  0-3
Vulnerability Gather data sampling: Not affected
Vulnerability Itlb multihit:        KVM: Mitigation: VMX disabled
Vulnerability L1tf:                 Mitigation; PTE Inversion; VMX conditional cache flushes, SMT disabled
Vulnerability Mds:                  Mitigation; Clear CPU buffers; SMT disabled
Vulnerability Meltdown:             Mitigation; PTI
Vulnerability Mmio stale data:      Unknown: No mitigations
Vulnerability Retbleed:             Not affected
Vulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl
Vulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization
Vulnerability Spectre v2:           Mitigation; Retpolines, IBPB conditional, IBRS_FW, STIBP disabled, RSB filling, PBRSB-eIBRS Not affected
Vulnerability Srbds:                Mitigation; Microcode
Vulnerability Tsx async abort:      Not affected

Versions of relevant libraries:
[pip3] numpy==1.25.2
[pip3] open-clip-torch==2.20.0
[pip3] torch==2.0.1
[pip3] torchaudio==2.0.2
[pip3] torchvision==0.15.2
[conda] blas                      1.0                         mkl  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] mkl                       2023.1.0         h213fc3f_46343  
[conda] mkl-service               2.4.0           py310h5eee18b_1  
[conda] mkl_fft                   1.3.6           py310h1128e8f_1  
[conda] mkl_random                1.2.2           py310h1128e8f_1  
[conda] numpy                     1.25.2          py310h5f9d8c6_0  
[conda] numpy-base                1.25.2          py310hb5e798b_0  
[conda] open-clip-torch           2.20.0                   pypi_0    pypi
[conda] pytorch                   2.0.1           py3.10_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchaudio                2.0.2               py310_cu117    pytorch
[conda] torchtriton               2.0.0                     py310    pytorch
[conda] torchvision               0.15.2              py310_cu117    pytorch
        Pillow (9.4.0)

Loading trainer: CoOp
Loading dataset: Caltech101
Reading split from /home/brandnerkasper/Uni/MP/MP_CustomCoOp/data/caltech-101/split_zhou_Caltech101.json
Creating a 1-shot dataset
Creating a 1-shot dataset
Saving preprocessed few-shot data to /home/brandnerkasper/Uni/MP/MP_CustomCoOp/data/caltech-101/split_fewshot/shot_1-seed_2.pkl
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ----------
Dataset    Caltech101
# classes  100
# train_x  100
# val      100
# test     2,465
---------  ----------
Loading CLIP (backbone: RN101)
CLIP(
  (visual): ModifiedResNet(
    (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu1): ReLU(inplace=True)
    (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu2): ReLU(inplace=True)
    (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu3): ReLU(inplace=True)
    (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (layer1): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
        (downsample): Sequential(
          (-1): AvgPool2d(kernel_size=1, stride=1, padding=0)
          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
    )
    (layer2): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
        (downsample): Sequential(
          (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
    )
    (layer3): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
        (downsample): Sequential(
          (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
      (6): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
      (7): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
      (8): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
      (9): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
      (10): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
      (11): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
      (12): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
      (13): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
      (14): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
      (15): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
      (16): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
      (17): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
      (18): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
      (19): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
      (20): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
      (21): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
      (22): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
    )
    (layer4): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
        (downsample): Sequential(
          (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
    )
    (attnpool): AttentionPool2d(
      (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
      (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
      (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
      (c_proj): Linear(in_features=2048, out_features=512, bias=True)
    )
  )
  (transformer): Transformer(
    (resblocks): Sequential(
      (0): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (6): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (7): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (8): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (9): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (10): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (11): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (token_embedding): Embedding(49408, 512)
  (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
Turning off gradients in both the image and the text encoder
Loading evaluator: Classification
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/Caltech/1/2/tensorboard)
epoch [1/200] batch [1/3] time 1.086 (1.086) data 0.269 (0.269) loss 2.9258 (2.9258) acc 59.3750 (59.3750) lr 1.0000e-05 eta 0:10:50
epoch [1/200] batch [2/3] time 0.794 (0.940) data 0.000 (0.135) loss 2.9492 (2.9375) acc 43.7500 (51.5625) lr 1.0000e-05 eta 0:09:21
epoch [1/200] batch [3/3] time 0.792 (0.890) data 0.000 (0.090) loss 3.0820 (2.9857) acc 46.8750 (50.0000) lr 2.0000e-03 eta 0:08:51
epoch [2/200] batch [1/3] time 1.054 (1.054) data 0.260 (0.260) loss 2.6250 (2.6250) acc 65.6250 (65.6250) lr 2.0000e-03 eta 0:10:27
epoch [2/200] batch [2/3] time 0.792 (0.923) data 0.000 (0.130) loss 2.0215 (2.3232) acc 56.2500 (60.9375) lr 2.0000e-03 eta 0:09:09
epoch [2/200] batch [3/3] time 0.793 (0.879) data 0.000 (0.087) loss 1.5361 (2.0609) acc 59.3750 (60.4167) lr 1.9999e-03 eta 0:08:42
epoch [3/200] batch [1/3] time 1.056 (1.056) data 0.261 (0.261) loss 1.4473 (1.4473) acc 68.7500 (68.7500) lr 1.9999e-03 eta 0:10:26
epoch [3/200] batch [2/3] time 0.795 (0.925) data 0.001 (0.131) loss 1.3428 (1.3950) acc 68.7500 (68.7500) lr 1.9999e-03 eta 0:09:07
epoch [3/200] batch [3/3] time 0.793 (0.881) data 0.000 (0.087) loss 2.1719 (1.6540) acc 40.6250 (59.3750) lr 1.9995e-03 eta 0:08:40
epoch [4/200] batch [1/3] time 1.064 (1.064) data 0.274 (0.274) loss 0.6772 (0.6772) acc 78.1250 (78.1250) lr 1.9995e-03 eta 0:10:27
epoch [4/200] batch [2/3] time 0.796 (0.930) data 0.000 (0.137) loss 1.2432 (0.9602) acc 65.6250 (71.8750) lr 1.9995e-03 eta 0:09:07
epoch [4/200] batch [3/3] time 0.796 (0.885) data 0.000 (0.091) loss 0.7344 (0.8849) acc 81.2500 (75.0000) lr 1.9989e-03 eta 0:08:40
epoch [5/200] batch [1/3] time 1.061 (1.061) data 0.267 (0.267) loss 1.0264 (1.0264) acc 75.0000 (75.0000) lr 1.9989e-03 eta 0:10:22
epoch [5/200] batch [2/3] time 0.794 (0.927) data 0.000 (0.134) loss 1.4736 (1.2500) acc 62.5000 (68.7500) lr 1.9989e-03 eta 0:09:03
epoch [5/200] batch [3/3] time 0.794 (0.883) data 0.000 (0.089) loss 1.0391 (1.1797) acc 65.6250 (67.7083) lr 1.9980e-03 eta 0:08:36
epoch [6/200] batch [1/3] time 1.072 (1.072) data 0.278 (0.278) loss 0.4529 (0.4529) acc 87.5000 (87.5000) lr 1.9980e-03 eta 0:10:26
epoch [6/200] batch [2/3] time 0.796 (0.934) data 0.000 (0.139) loss 0.7964 (0.6246) acc 71.8750 (79.6875) lr 1.9980e-03 eta 0:09:04
epoch [6/200] batch [3/3] time 0.794 (0.887) data 0.000 (0.093) loss 1.4834 (0.9109) acc 71.8750 (77.0833) lr 1.9969e-03 eta 0:08:36
epoch [7/200] batch [1/3] time 1.071 (1.071) data 0.276 (0.276) loss 1.3516 (1.3516) acc 59.3750 (59.3750) lr 1.9969e-03 eta 0:10:22
epoch [7/200] batch [2/3] time 0.796 (0.934) data 0.000 (0.138) loss 1.1514 (1.2515) acc 68.7500 (64.0625) lr 1.9969e-03 eta 0:09:01
epoch [7/200] batch [3/3] time 0.794 (0.887) data 0.000 (0.092) loss 1.0625 (1.1885) acc 68.7500 (65.6250) lr 1.9956e-03 eta 0:08:33
epoch [8/200] batch [1/3] time 1.051 (1.051) data 0.257 (0.257) loss 1.3213 (1.3213) acc 65.6250 (65.6250) lr 1.9956e-03 eta 0:10:07
epoch [8/200] batch [2/3] time 0.796 (0.923) data 0.000 (0.129) loss 1.2598 (1.2905) acc 68.7500 (67.1875) lr 1.9956e-03 eta 0:08:52
epoch [8/200] batch [3/3] time 0.796 (0.881) data 0.000 (0.086) loss 1.2041 (1.2617) acc 62.5000 (65.6250) lr 1.9940e-03 eta 0:08:27
epoch [9/200] batch [1/3] time 1.057 (1.057) data 0.265 (0.265) loss 1.7832 (1.7832) acc 50.0000 (50.0000) lr 1.9940e-03 eta 0:10:07
epoch [9/200] batch [2/3] time 0.796 (0.926) data 0.000 (0.133) loss 1.0654 (1.4243) acc 71.8750 (60.9375) lr 1.9940e-03 eta 0:08:51
epoch [9/200] batch [3/3] time 0.795 (0.882) data 0.000 (0.089) loss 0.7285 (1.1924) acc 75.0000 (65.6250) lr 1.9921e-03 eta 0:08:25
epoch [10/200] batch [1/3] time 1.052 (1.052) data 0.257 (0.257) loss 1.0029 (1.0029) acc 68.7500 (68.7500) lr 1.9921e-03 eta 0:10:01
epoch [10/200] batch [2/3] time 0.794 (0.923) data 0.000 (0.129) loss 1.4482 (1.2256) acc 62.5000 (65.6250) lr 1.9921e-03 eta 0:08:47
epoch [10/200] batch [3/3] time 0.796 (0.881) data 0.000 (0.086) loss 0.6406 (1.0306) acc 87.5000 (72.9167) lr 1.9900e-03 eta 0:08:22
epoch [11/200] batch [1/3] time 1.059 (1.059) data 0.266 (0.266) loss 1.1943 (1.1943) acc 71.8750 (71.8750) lr 1.9900e-03 eta 0:10:02
epoch [11/200] batch [2/3] time 0.794 (0.927) data 0.000 (0.133) loss 1.0381 (1.1162) acc 68.7500 (70.3125) lr 1.9900e-03 eta 0:08:46
epoch [11/200] batch [3/3] time 0.794 (0.883) data 0.000 (0.089) loss 0.6777 (0.9701) acc 78.1250 (72.9167) lr 1.9877e-03 eta 0:08:20
epoch [12/200] batch [1/3] time 1.065 (1.065) data 0.272 (0.272) loss 0.7534 (0.7534) acc 75.0000 (75.0000) lr 1.9877e-03 eta 0:10:02
epoch [12/200] batch [2/3] time 0.796 (0.930) data 0.000 (0.136) loss 1.0625 (0.9080) acc 68.7500 (71.8750) lr 1.9877e-03 eta 0:08:45
epoch [12/200] batch [3/3] time 0.794 (0.885) data 0.000 (0.091) loss 0.6860 (0.8340) acc 81.2500 (75.0000) lr 1.9851e-03 eta 0:08:19
epoch [13/200] batch [1/3] time 1.065 (1.065) data 0.270 (0.270) loss 0.5649 (0.5649) acc 84.3750 (84.3750) lr 1.9851e-03 eta 0:09:59
epoch [13/200] batch [2/3] time 0.795 (0.930) data 0.000 (0.135) loss 0.9189 (0.7419) acc 78.1250 (81.2500) lr 1.9851e-03 eta 0:08:42
epoch [13/200] batch [3/3] time 0.797 (0.886) data 0.000 (0.090) loss 0.8403 (0.7747) acc 71.8750 (78.1250) lr 1.9823e-03 eta 0:08:16
epoch [14/200] batch [1/3] time 1.065 (1.065) data 0.275 (0.275) loss 1.1045 (1.1045) acc 75.0000 (75.0000) lr 1.9823e-03 eta 0:09:56
epoch [14/200] batch [2/3] time 0.795 (0.930) data 0.000 (0.137) loss 1.0420 (1.0732) acc 78.1250 (76.5625) lr 1.9823e-03 eta 0:08:39
epoch [14/200] batch [3/3] time 0.795 (0.885) data 0.000 (0.092) loss 0.8413 (0.9959) acc 81.2500 (78.1250) lr 1.9792e-03 eta 0:08:13
epoch [15/200] batch [1/3] time 1.061 (1.061) data 0.267 (0.267) loss 0.7827 (0.7827) acc 87.5000 (87.5000) lr 1.9792e-03 eta 0:09:50
epoch [15/200] batch [2/3] time 0.795 (0.928) data 0.000 (0.134) loss 0.7563 (0.7695) acc 75.0000 (81.2500) lr 1.9792e-03 eta 0:08:35
epoch [15/200] batch [3/3] time 0.795 (0.883) data 0.000 (0.089) loss 1.4473 (0.9954) acc 65.6250 (76.0417) lr 1.9759e-03 eta 0:08:10
epoch [16/200] batch [1/3] time 1.056 (1.056) data 0.265 (0.265) loss 0.7935 (0.7935) acc 78.1250 (78.1250) lr 1.9759e-03 eta 0:09:45
epoch [16/200] batch [2/3] time 0.794 (0.925) data 0.000 (0.133) loss 1.1318 (0.9626) acc 71.8750 (75.0000) lr 1.9759e-03 eta 0:08:31
epoch [16/200] batch [3/3] time 0.795 (0.882) data 0.000 (0.088) loss 0.6709 (0.8654) acc 81.2500 (77.0833) lr 1.9724e-03 eta 0:08:06
epoch [17/200] batch [1/3] time 1.062 (1.062) data 0.266 (0.266) loss 0.9551 (0.9551) acc 78.1250 (78.1250) lr 1.9724e-03 eta 0:09:45
epoch [17/200] batch [2/3] time 0.797 (0.929) data 0.000 (0.133) loss 0.9946 (0.9749) acc 71.8750 (75.0000) lr 1.9724e-03 eta 0:08:31
epoch [17/200] batch [3/3] time 0.795 (0.884) data 0.000 (0.089) loss 0.8174 (0.9224) acc 75.0000 (75.0000) lr 1.9686e-03 eta 0:08:05
epoch [18/200] batch [1/3] time 1.067 (1.067) data 0.274 (0.274) loss 0.8228 (0.8228) acc 78.1250 (78.1250) lr 1.9686e-03 eta 0:09:44
epoch [18/200] batch [2/3] time 0.795 (0.931) data 0.000 (0.137) loss 1.2090 (1.0159) acc 65.6250 (71.8750) lr 1.9686e-03 eta 0:08:29
epoch [18/200] batch [3/3] time 0.795 (0.885) data 0.000 (0.091) loss 1.3975 (1.1431) acc 68.7500 (70.8333) lr 1.9646e-03 eta 0:08:03
epoch [19/200] batch [1/3] time 1.052 (1.052) data 0.258 (0.258) loss 1.3975 (1.3975) acc 68.7500 (68.7500) lr 1.9646e-03 eta 0:09:33
epoch [19/200] batch [2/3] time 0.794 (0.923) data 0.000 (0.129) loss 0.9219 (1.1597) acc 68.7500 (68.7500) lr 1.9646e-03 eta 0:08:22
epoch [19/200] batch [3/3] time 0.795 (0.880) data 0.000 (0.086) loss 1.1279 (1.1491) acc 71.8750 (69.7917) lr 1.9603e-03 eta 0:07:57
epoch [20/200] batch [1/3] time 1.068 (1.068) data 0.275 (0.275) loss 1.1729 (1.1729) acc 68.7500 (68.7500) lr 1.9603e-03 eta 0:09:39
epoch [20/200] batch [2/3] time 0.795 (0.932) data 0.000 (0.137) loss 0.6313 (0.9021) acc 81.2500 (75.0000) lr 1.9603e-03 eta 0:08:24
epoch [20/200] batch [3/3] time 0.796 (0.887) data 0.000 (0.092) loss 1.0059 (0.9367) acc 75.0000 (75.0000) lr 1.9558e-03 eta 0:07:58
epoch [21/200] batch [1/3] time 1.060 (1.060) data 0.268 (0.268) loss 0.5439 (0.5439) acc 81.2500 (81.2500) lr 1.9558e-03 eta 0:09:31
epoch [21/200] batch [2/3] time 0.795 (0.927) data 0.000 (0.134) loss 1.3213 (0.9326) acc 68.7500 (75.0000) lr 1.9558e-03 eta 0:08:18
epoch [21/200] batch [3/3] time 0.794 (0.883) data 0.000 (0.089) loss 0.5522 (0.8058) acc 87.5000 (79.1667) lr 1.9511e-03 eta 0:07:54
epoch [22/200] batch [1/3] time 1.053 (1.053) data 0.258 (0.258) loss 0.7373 (0.7373) acc 78.1250 (78.1250) lr 1.9511e-03 eta 0:09:24
epoch [22/200] batch [2/3] time 0.794 (0.924) data 0.000 (0.129) loss 0.5928 (0.6650) acc 81.2500 (79.6875) lr 1.9511e-03 eta 0:08:14
epoch [22/200] batch [3/3] time 0.794 (0.880) data 0.000 (0.086) loss 0.4875 (0.6059) acc 84.3750 (81.2500) lr 1.9461e-03 eta 0:07:50
epoch [23/200] batch [1/3] time 1.063 (1.063) data 0.267 (0.267) loss 0.8696 (0.8696) acc 75.0000 (75.0000) lr 1.9461e-03 eta 0:09:26
epoch [23/200] batch [2/3] time 0.795 (0.929) data 0.000 (0.134) loss 0.6411 (0.7554) acc 84.3750 (79.6875) lr 1.9461e-03 eta 0:08:14
epoch [23/200] batch [3/3] time 0.797 (0.885) data 0.000 (0.089) loss 1.0859 (0.8656) acc 68.7500 (76.0417) lr 1.9409e-03 eta 0:07:49
epoch [24/200] batch [1/3] time 1.049 (1.049) data 0.258 (0.258) loss 1.2314 (1.2314) acc 65.6250 (65.6250) lr 1.9409e-03 eta 0:09:16
epoch [24/200] batch [2/3] time 0.797 (0.923) data 0.000 (0.129) loss 0.5557 (0.8936) acc 84.3750 (75.0000) lr 1.9409e-03 eta 0:08:08
epoch [24/200] batch [3/3] time 0.794 (0.880) data 0.000 (0.086) loss 0.5562 (0.7811) acc 81.2500 (77.0833) lr 1.9354e-03 eta 0:07:44
epoch [25/200] batch [1/3] time 1.068 (1.068) data 0.275 (0.275) loss 1.1914 (1.1914) acc 68.7500 (68.7500) lr 1.9354e-03 eta 0:09:22
epoch [25/200] batch [2/3] time 0.794 (0.931) data 0.000 (0.137) loss 1.0527 (1.1221) acc 78.1250 (73.4375) lr 1.9354e-03 eta 0:08:09
epoch [25/200] batch [3/3] time 0.797 (0.886) data 0.000 (0.092) loss 1.2705 (1.1715) acc 68.7500 (71.8750) lr 1.9298e-03 eta 0:07:45
epoch [26/200] batch [1/3] time 1.056 (1.056) data 0.265 (0.265) loss 0.8813 (0.8813) acc 71.8750 (71.8750) lr 1.9298e-03 eta 0:09:13
epoch [26/200] batch [2/3] time 0.795 (0.926) data 0.000 (0.132) loss 0.9243 (0.9028) acc 68.7500 (70.3125) lr 1.9298e-03 eta 0:08:04
epoch [26/200] batch [3/3] time 0.795 (0.882) data 0.000 (0.088) loss 0.7690 (0.8582) acc 75.0000 (71.8750) lr 1.9239e-03 eta 0:07:40
epoch [27/200] batch [1/3] time 1.064 (1.064) data 0.268 (0.268) loss 0.6729 (0.6729) acc 81.2500 (81.2500) lr 1.9239e-03 eta 0:09:14
epoch [27/200] batch [2/3] time 0.794 (0.929) data 0.000 (0.134) loss 0.5645 (0.6187) acc 84.3750 (82.8125) lr 1.9239e-03 eta 0:08:03
epoch [27/200] batch [3/3] time 0.795 (0.885) data 0.000 (0.089) loss 0.7705 (0.6693) acc 75.0000 (80.2083) lr 1.9178e-03 eta 0:07:39
epoch [28/200] batch [1/3] time 1.063 (1.063) data 0.266 (0.266) loss 1.3574 (1.3574) acc 65.6250 (65.6250) lr 1.9178e-03 eta 0:09:10
epoch [28/200] batch [2/3] time 0.795 (0.929) data 0.000 (0.133) loss 0.8213 (1.0894) acc 75.0000 (70.3125) lr 1.9178e-03 eta 0:08:00
epoch [28/200] batch [3/3] time 0.794 (0.884) data 0.000 (0.089) loss 0.5171 (0.8986) acc 87.5000 (76.0417) lr 1.9114e-03 eta 0:07:36
epoch [29/200] batch [1/3] time 1.059 (1.059) data 0.265 (0.265) loss 0.9849 (0.9849) acc 68.7500 (68.7500) lr 1.9114e-03 eta 0:09:05
epoch [29/200] batch [2/3] time 0.795 (0.927) data 0.000 (0.133) loss 0.8555 (0.9202) acc 75.0000 (71.8750) lr 1.9114e-03 eta 0:07:56
epoch [29/200] batch [3/3] time 0.795 (0.883) data 0.000 (0.088) loss 0.8784 (0.9062) acc 81.2500 (75.0000) lr 1.9048e-03 eta 0:07:32
epoch [30/200] batch [1/3] time 1.061 (1.061) data 0.267 (0.267) loss 0.6533 (0.6533) acc 84.3750 (84.3750) lr 1.9048e-03 eta 0:09:03
epoch [30/200] batch [2/3] time 0.795 (0.928) data 0.000 (0.134) loss 1.1309 (0.8921) acc 71.8750 (78.1250) lr 1.9048e-03 eta 0:07:54
epoch [30/200] batch [3/3] time 0.795 (0.884) data 0.000 (0.089) loss 0.5801 (0.7881) acc 81.2500 (79.1667) lr 1.8980e-03 eta 0:07:30
epoch [31/200] batch [1/3] time 1.053 (1.053) data 0.261 (0.261) loss 0.5728 (0.5728) acc 84.3750 (84.3750) lr 1.8980e-03 eta 0:08:55
epoch [31/200] batch [2/3] time 0.795 (0.924) data 0.000 (0.131) loss 0.6006 (0.5867) acc 87.5000 (85.9375) lr 1.8980e-03 eta 0:07:49
epoch [31/200] batch [3/3] time 0.795 (0.881) data 0.000 (0.087) loss 0.7324 (0.6353) acc 81.2500 (84.3750) lr 1.8910e-03 eta 0:07:26
epoch [32/200] batch [1/3] time 1.059 (1.059) data 0.265 (0.265) loss 0.9766 (0.9766) acc 81.2500 (81.2500) lr 1.8910e-03 eta 0:08:55
epoch [32/200] batch [2/3] time 0.795 (0.927) data 0.000 (0.133) loss 0.4648 (0.7207) acc 87.5000 (84.3750) lr 1.8910e-03 eta 0:07:48
epoch [32/200] batch [3/3] time 0.795 (0.883) data 0.000 (0.089) loss 0.9565 (0.7993) acc 81.2500 (83.3333) lr 1.8838e-03 eta 0:07:24
epoch [33/200] batch [1/3] time 1.063 (1.063) data 0.267 (0.267) loss 0.3396 (0.3396) acc 90.6250 (90.6250) lr 1.8838e-03 eta 0:08:54
epoch [33/200] batch [2/3] time 0.795 (0.929) data 0.000 (0.134) loss 1.2109 (0.7753) acc 71.8750 (81.2500) lr 1.8838e-03 eta 0:07:46
epoch [33/200] batch [3/3] time 0.796 (0.885) data 0.000 (0.089) loss 0.9897 (0.8468) acc 71.8750 (78.1250) lr 1.8763e-03 eta 0:07:23
epoch [34/200] batch [1/3] time 1.051 (1.051) data 0.260 (0.260) loss 0.6533 (0.6533) acc 78.1250 (78.1250) lr 1.8763e-03 eta 0:08:45
epoch [34/200] batch [2/3] time 0.794 (0.923) data 0.000 (0.130) loss 0.8467 (0.7500) acc 81.2500 (79.6875) lr 1.8763e-03 eta 0:07:40
epoch [34/200] batch [3/3] time 0.795 (0.880) data 0.000 (0.087) loss 0.4568 (0.6523) acc 87.5000 (82.2917) lr 1.8686e-03 eta 0:07:18
epoch [35/200] batch [1/3] time 1.071 (1.071) data 0.276 (0.276) loss 0.7979 (0.7979) acc 75.0000 (75.0000) lr 1.8686e-03 eta 0:08:52
epoch [35/200] batch [2/3] time 0.795 (0.933) data 0.000 (0.138) loss 0.8154 (0.8066) acc 75.0000 (75.0000) lr 1.8686e-03 eta 0:07:42
epoch [35/200] batch [3/3] time 0.794 (0.887) data 0.000 (0.092) loss 0.3994 (0.6709) acc 87.5000 (79.1667) lr 1.8607e-03 eta 0:07:18
epoch [36/200] batch [1/3] time 1.069 (1.069) data 0.278 (0.278) loss 0.6582 (0.6582) acc 84.3750 (84.3750) lr 1.8607e-03 eta 0:08:47
epoch [36/200] batch [2/3] time 0.797 (0.933) data 0.000 (0.139) loss 0.7075 (0.6829) acc 81.2500 (82.8125) lr 1.8607e-03 eta 0:07:39
epoch [36/200] batch [3/3] time 0.795 (0.887) data 0.000 (0.093) loss 0.7607 (0.7088) acc 81.2500 (82.2917) lr 1.8526e-03 eta 0:07:16
epoch [37/200] batch [1/3] time 1.066 (1.066) data 0.271 (0.271) loss 0.5991 (0.5991) acc 75.0000 (75.0000) lr 1.8526e-03 eta 0:08:43
epoch [37/200] batch [2/3] time 0.796 (0.931) data 0.000 (0.135) loss 0.6255 (0.6123) acc 81.2500 (78.1250) lr 1.8526e-03 eta 0:07:36
epoch [37/200] batch [3/3] time 0.797 (0.887) data 0.000 (0.090) loss 0.7183 (0.6476) acc 84.3750 (80.2083) lr 1.8443e-03 eta 0:07:13
epoch [38/200] batch [1/3] time 1.062 (1.062) data 0.267 (0.267) loss 0.8291 (0.8291) acc 81.2500 (81.2500) lr 1.8443e-03 eta 0:08:38
epoch [38/200] batch [2/3] time 0.794 (0.928) data 0.000 (0.133) loss 0.4016 (0.6154) acc 93.7500 (87.5000) lr 1.8443e-03 eta 0:07:32
epoch [38/200] batch [3/3] time 0.797 (0.885) data 0.000 (0.089) loss 0.7427 (0.6578) acc 78.1250 (84.3750) lr 1.8358e-03 eta 0:07:09
epoch [39/200] batch [1/3] time 1.069 (1.069) data 0.275 (0.275) loss 0.2649 (0.2649) acc 93.7500 (93.7500) lr 1.8358e-03 eta 0:08:38
epoch [39/200] batch [2/3] time 0.795 (0.932) data 0.000 (0.138) loss 0.7935 (0.5292) acc 84.3750 (89.0625) lr 1.8358e-03 eta 0:07:30
epoch [39/200] batch [3/3] time 0.795 (0.886) data 0.000 (0.092) loss 0.7817 (0.6134) acc 78.1250 (85.4167) lr 1.8271e-03 eta 0:07:07
epoch [40/200] batch [1/3] time 1.068 (1.068) data 0.274 (0.274) loss 0.6992 (0.6992) acc 84.3750 (84.3750) lr 1.8271e-03 eta 0:08:34
epoch [40/200] batch [2/3] time 0.795 (0.931) data 0.000 (0.137) loss 0.7368 (0.7180) acc 75.0000 (79.6875) lr 1.8271e-03 eta 0:07:27
epoch [40/200] batch [3/3] time 0.795 (0.886) data 0.000 (0.091) loss 0.6021 (0.6794) acc 84.3750 (81.2500) lr 1.8181e-03 eta 0:07:05
epoch [41/200] batch [1/3] time 1.068 (1.068) data 0.275 (0.275) loss 0.8350 (0.8350) acc 81.2500 (81.2500) lr 1.8181e-03 eta 0:08:31
epoch [41/200] batch [2/3] time 0.795 (0.932) data 0.000 (0.137) loss 0.5820 (0.7085) acc 81.2500 (81.2500) lr 1.8181e-03 eta 0:07:25
epoch [41/200] batch [3/3] time 0.795 (0.886) data 0.000 (0.092) loss 0.4507 (0.6226) acc 93.7500 (85.4167) lr 1.8090e-03 eta 0:07:02
epoch [42/200] batch [1/3] time 1.054 (1.054) data 0.258 (0.258) loss 0.6553 (0.6553) acc 81.2500 (81.2500) lr 1.8090e-03 eta 0:08:21
epoch [42/200] batch [2/3] time 0.795 (0.925) data 0.000 (0.129) loss 0.5591 (0.6072) acc 87.5000 (84.3750) lr 1.8090e-03 eta 0:07:19
epoch [42/200] batch [3/3] time 0.795 (0.881) data 0.000 (0.086) loss 0.6064 (0.6069) acc 87.5000 (85.4167) lr 1.7997e-03 eta 0:06:57
epoch [43/200] batch [1/3] time 1.049 (1.049) data 0.258 (0.258) loss 0.6582 (0.6582) acc 81.2500 (81.2500) lr 1.7997e-03 eta 0:08:16
epoch [43/200] batch [2/3] time 0.794 (0.921) data 0.000 (0.129) loss 0.4111 (0.5347) acc 96.8750 (89.0625) lr 1.7997e-03 eta 0:07:14
epoch [43/200] batch [3/3] time 0.795 (0.879) data 0.000 (0.086) loss 0.4668 (0.5120) acc 93.7500 (90.6250) lr 1.7902e-03 eta 0:06:54
epoch [44/200] batch [1/3] time 1.067 (1.067) data 0.273 (0.273) loss 0.8394 (0.8394) acc 78.1250 (78.1250) lr 1.7902e-03 eta 0:08:21
epoch [44/200] batch [2/3] time 0.795 (0.931) data 0.000 (0.136) loss 1.0654 (0.9524) acc 78.1250 (78.1250) lr 1.7902e-03 eta 0:07:16
epoch [44/200] batch [3/3] time 0.798 (0.887) data 0.000 (0.091) loss 1.1279 (1.0109) acc 78.1250 (78.1250) lr 1.7804e-03 eta 0:06:54
epoch [45/200] batch [1/3] time 1.068 (1.068) data 0.274 (0.274) loss 0.7437 (0.7437) acc 78.1250 (78.1250) lr 1.7804e-03 eta 0:08:18
epoch [45/200] batch [2/3] time 0.794 (0.931) data 0.000 (0.137) loss 0.3752 (0.5594) acc 93.7500 (85.9375) lr 1.7804e-03 eta 0:07:13
epoch [45/200] batch [3/3] time 0.795 (0.886) data 0.000 (0.091) loss 1.2432 (0.7874) acc 65.6250 (79.1667) lr 1.7705e-03 eta 0:06:51
epoch [46/200] batch [1/3] time 1.055 (1.055) data 0.260 (0.260) loss 0.5527 (0.5527) acc 87.5000 (87.5000) lr 1.7705e-03 eta 0:08:09
epoch [46/200] batch [2/3] time 0.795 (0.925) data 0.000 (0.130) loss 0.4451 (0.4989) acc 87.5000 (87.5000) lr 1.7705e-03 eta 0:07:08
epoch [46/200] batch [3/3] time 0.795 (0.881) data 0.000 (0.087) loss 0.8955 (0.6311) acc 75.0000 (83.3333) lr 1.7604e-03 eta 0:06:47
epoch [47/200] batch [1/3] time 1.054 (1.054) data 0.260 (0.260) loss 0.5898 (0.5898) acc 84.3750 (84.3750) lr 1.7604e-03 eta 0:08:06
epoch [47/200] batch [2/3] time 0.795 (0.925) data 0.000 (0.130) loss 0.7485 (0.6692) acc 78.1250 (81.2500) lr 1.7604e-03 eta 0:07:05
epoch [47/200] batch [3/3] time 0.796 (0.882) data 0.000 (0.087) loss 0.8174 (0.7186) acc 84.3750 (82.2917) lr 1.7501e-03 eta 0:06:44
epoch [48/200] batch [1/3] time 1.061 (1.061) data 0.269 (0.269) loss 0.6538 (0.6538) acc 81.2500 (81.2500) lr 1.7501e-03 eta 0:08:05
epoch [48/200] batch [2/3] time 0.795 (0.928) data 0.000 (0.134) loss 0.8262 (0.7400) acc 78.1250 (79.6875) lr 1.7501e-03 eta 0:07:03
epoch [48/200] batch [3/3] time 0.795 (0.883) data 0.000 (0.090) loss 0.7788 (0.7529) acc 81.2500 (80.2083) lr 1.7396e-03 eta 0:06:42
epoch [49/200] batch [1/3] time 1.071 (1.071) data 0.275 (0.275) loss 0.6157 (0.6157) acc 78.1250 (78.1250) lr 1.7396e-03 eta 0:08:07
epoch [49/200] batch [2/3] time 0.795 (0.933) data 0.000 (0.138) loss 0.5796 (0.5977) acc 84.3750 (81.2500) lr 1.7396e-03 eta 0:07:03
epoch [49/200] batch [3/3] time 0.795 (0.887) data 0.000 (0.092) loss 0.5688 (0.5881) acc 87.5000 (83.3333) lr 1.7290e-03 eta 0:06:41
epoch [50/200] batch [1/3] time 1.055 (1.055) data 0.261 (0.261) loss 1.0537 (1.0537) acc 71.8750 (71.8750) lr 1.7290e-03 eta 0:07:56
epoch [50/200] batch [2/3] time 0.795 (0.925) data 0.000 (0.130) loss 0.5483 (0.8010) acc 87.5000 (79.6875) lr 1.7290e-03 eta 0:06:57
epoch [50/200] batch [3/3] time 0.795 (0.882) data 0.000 (0.087) loss 0.6162 (0.7394) acc 87.5000 (82.2917) lr 1.7181e-03 eta 0:06:36
epoch [51/200] batch [1/3] time 1.053 (1.053) data 0.259 (0.259) loss 0.6743 (0.6743) acc 90.6250 (90.6250) lr 1.7181e-03 eta 0:07:52
epoch [51/200] batch [2/3] time 0.794 (0.924) data 0.000 (0.130) loss 0.4719 (0.5731) acc 90.6250 (90.6250) lr 1.7181e-03 eta 0:06:53
epoch [51/200] batch [3/3] time 0.795 (0.881) data 0.000 (0.086) loss 0.9102 (0.6855) acc 81.2500 (87.5000) lr 1.7071e-03 eta 0:06:33
epoch [52/200] batch [1/3] time 1.054 (1.054) data 0.258 (0.258) loss 0.4900 (0.4900) acc 84.3750 (84.3750) lr 1.7071e-03 eta 0:07:50
epoch [52/200] batch [2/3] time 0.795 (0.924) data 0.000 (0.129) loss 0.3770 (0.4335) acc 90.6250 (87.5000) lr 1.7071e-03 eta 0:06:51
epoch [52/200] batch [3/3] time 0.796 (0.882) data 0.000 (0.086) loss 0.6055 (0.4908) acc 87.5000 (87.5000) lr 1.6959e-03 eta 0:06:31
epoch [53/200] batch [1/3] time 1.052 (1.052) data 0.261 (0.261) loss 0.2522 (0.2522) acc 90.6250 (90.6250) lr 1.6959e-03 eta 0:07:46
epoch [53/200] batch [2/3] time 0.796 (0.924) data 0.000 (0.130) loss 0.4316 (0.3419) acc 90.6250 (90.6250) lr 1.6959e-03 eta 0:06:48
epoch [53/200] batch [3/3] time 0.795 (0.881) data 0.000 (0.087) loss 0.2864 (0.3234) acc 93.7500 (91.6667) lr 1.6845e-03 eta 0:06:28
epoch [54/200] batch [1/3] time 1.056 (1.056) data 0.260 (0.260) loss 0.4766 (0.4766) acc 87.5000 (87.5000) lr 1.6845e-03 eta 0:07:44
epoch [54/200] batch [2/3] time 0.797 (0.926) data 0.000 (0.130) loss 0.7241 (0.6003) acc 81.2500 (84.3750) lr 1.6845e-03 eta 0:06:46
epoch [54/200] batch [3/3] time 0.796 (0.883) data 0.000 (0.087) loss 0.3645 (0.5217) acc 93.7500 (87.5000) lr 1.6730e-03 eta 0:06:26
epoch [55/200] batch [1/3] time 1.060 (1.060) data 0.267 (0.267) loss 0.4702 (0.4702) acc 81.2500 (81.2500) lr 1.6730e-03 eta 0:07:43
epoch [55/200] batch [2/3] time 0.795 (0.928) data 0.000 (0.133) loss 0.7983 (0.6343) acc 84.3750 (82.8125) lr 1.6730e-03 eta 0:06:44
epoch [55/200] batch [3/3] time 0.795 (0.883) data 0.000 (0.089) loss 0.4338 (0.5675) acc 93.7500 (86.4583) lr 1.6613e-03 eta 0:06:24
epoch [56/200] batch [1/3] time 1.070 (1.070) data 0.276 (0.276) loss 0.4197 (0.4197) acc 90.6250 (90.6250) lr 1.6613e-03 eta 0:07:44
epoch [56/200] batch [2/3] time 0.794 (0.932) data 0.000 (0.138) loss 0.6938 (0.5568) acc 81.2500 (85.9375) lr 1.6613e-03 eta 0:06:43
epoch [56/200] batch [3/3] time 0.795 (0.886) data 0.000 (0.092) loss 0.3093 (0.4743) acc 90.6250 (87.5000) lr 1.6494e-03 eta 0:06:22
epoch [57/200] batch [1/3] time 1.067 (1.067) data 0.273 (0.273) loss 0.6665 (0.6665) acc 87.5000 (87.5000) lr 1.6494e-03 eta 0:07:40
epoch [57/200] batch [2/3] time 0.794 (0.931) data 0.000 (0.137) loss 0.4622 (0.5643) acc 90.6250 (89.0625) lr 1.6494e-03 eta 0:06:40
epoch [57/200] batch [3/3] time 0.795 (0.885) data 0.000 (0.091) loss 0.7373 (0.6220) acc 75.0000 (84.3750) lr 1.6374e-03 eta 0:06:19
epoch [58/200] batch [1/3] time 1.071 (1.071) data 0.278 (0.278) loss 0.5708 (0.5708) acc 87.5000 (87.5000) lr 1.6374e-03 eta 0:07:38
epoch [58/200] batch [2/3] time 0.797 (0.934) data 0.000 (0.139) loss 0.3416 (0.4562) acc 93.7500 (90.6250) lr 1.6374e-03 eta 0:06:38
epoch [58/200] batch [3/3] time 0.795 (0.887) data 0.000 (0.093) loss 1.1875 (0.7000) acc 84.3750 (88.5417) lr 1.6252e-03 eta 0:06:17
epoch [59/200] batch [1/3] time 1.069 (1.069) data 0.275 (0.275) loss 0.8687 (0.8687) acc 84.3750 (84.3750) lr 1.6252e-03 eta 0:07:34
epoch [59/200] batch [2/3] time 0.794 (0.931) data 0.000 (0.137) loss 0.5527 (0.7107) acc 84.3750 (84.3750) lr 1.6252e-03 eta 0:06:34
epoch [59/200] batch [3/3] time 0.797 (0.887) data 0.000 (0.092) loss 0.5771 (0.6662) acc 84.3750 (84.3750) lr 1.6129e-03 eta 0:06:15
epoch [60/200] batch [1/3] time 1.068 (1.068) data 0.276 (0.276) loss 0.7876 (0.7876) acc 84.3750 (84.3750) lr 1.6129e-03 eta 0:07:30
epoch [60/200] batch [2/3] time 0.796 (0.932) data 0.000 (0.138) loss 0.5581 (0.6729) acc 87.5000 (85.9375) lr 1.6129e-03 eta 0:06:32
epoch [60/200] batch [3/3] time 0.795 (0.886) data 0.000 (0.092) loss 0.6646 (0.6701) acc 84.3750 (85.4167) lr 1.6004e-03 eta 0:06:12
epoch [61/200] batch [1/3] time 1.066 (1.066) data 0.275 (0.275) loss 0.4124 (0.4124) acc 84.3750 (84.3750) lr 1.6004e-03 eta 0:07:26
epoch [61/200] batch [2/3] time 0.794 (0.930) data 0.000 (0.137) loss 0.6924 (0.5524) acc 84.3750 (84.3750) lr 1.6004e-03 eta 0:06:28
epoch [61/200] batch [3/3] time 0.796 (0.885) data 0.000 (0.092) loss 0.4592 (0.5213) acc 87.5000 (85.4167) lr 1.5878e-03 eta 0:06:09
epoch [62/200] batch [1/3] time 1.055 (1.055) data 0.260 (0.260) loss 0.4729 (0.4729) acc 90.6250 (90.6250) lr 1.5878e-03 eta 0:07:18
epoch [62/200] batch [2/3] time 0.795 (0.925) data 0.000 (0.130) loss 0.7671 (0.6200) acc 78.1250 (84.3750) lr 1.5878e-03 eta 0:06:23
epoch [62/200] batch [3/3] time 0.795 (0.881) data 0.000 (0.087) loss 0.4246 (0.5549) acc 84.3750 (84.3750) lr 1.5750e-03 eta 0:06:04
epoch [63/200] batch [1/3] time 1.054 (1.054) data 0.260 (0.260) loss 0.2109 (0.2109) acc 100.0000 (100.0000) lr 1.5750e-03 eta 0:07:15
epoch [63/200] batch [2/3] time 0.795 (0.924) data 0.000 (0.130) loss 0.6548 (0.4329) acc 81.2500 (90.6250) lr 1.5750e-03 eta 0:06:20
epoch [63/200] batch [3/3] time 0.794 (0.881) data 0.000 (0.087) loss 0.4690 (0.4449) acc 87.5000 (89.5833) lr 1.5621e-03 eta 0:06:02
epoch [64/200] batch [1/3] time 1.061 (1.061) data 0.268 (0.268) loss 0.6987 (0.6987) acc 84.3750 (84.3750) lr 1.5621e-03 eta 0:07:15
epoch [64/200] batch [2/3] time 0.795 (0.928) data 0.000 (0.134) loss 0.5166 (0.6077) acc 90.6250 (87.5000) lr 1.5621e-03 eta 0:06:19
epoch [64/200] batch [3/3] time 0.795 (0.884) data 0.000 (0.089) loss 0.4158 (0.5437) acc 90.6250 (88.5417) lr 1.5490e-03 eta 0:06:00
epoch [65/200] batch [1/3] time 1.066 (1.066) data 0.275 (0.275) loss 0.6099 (0.6099) acc 81.2500 (81.2500) lr 1.5490e-03 eta 0:07:13
epoch [65/200] batch [2/3] time 0.795 (0.930) data 0.000 (0.138) loss 0.2239 (0.4169) acc 93.7500 (87.5000) lr 1.5490e-03 eta 0:06:17
epoch [65/200] batch [3/3] time 0.795 (0.885) data 0.000 (0.092) loss 1.0195 (0.6178) acc 75.0000 (83.3333) lr 1.5358e-03 eta 0:05:58
epoch [66/200] batch [1/3] time 1.061 (1.061) data 0.267 (0.267) loss 0.4194 (0.4194) acc 93.7500 (93.7500) lr 1.5358e-03 eta 0:07:08
epoch [66/200] batch [2/3] time 0.795 (0.928) data 0.000 (0.133) loss 0.8545 (0.6370) acc 81.2500 (87.5000) lr 1.5358e-03 eta 0:06:13
epoch [66/200] batch [3/3] time 0.794 (0.883) data 0.000 (0.089) loss 0.7456 (0.6732) acc 84.3750 (86.4583) lr 1.5225e-03 eta 0:05:55
epoch [67/200] batch [1/3] time 1.052 (1.052) data 0.258 (0.258) loss 0.5796 (0.5796) acc 84.3750 (84.3750) lr 1.5225e-03 eta 0:07:01
epoch [67/200] batch [2/3] time 0.794 (0.923) data 0.000 (0.129) loss 0.3142 (0.4469) acc 93.7500 (89.0625) lr 1.5225e-03 eta 0:06:09
epoch [67/200] batch [3/3] time 0.796 (0.881) data 0.000 (0.086) loss 0.6631 (0.5190) acc 84.3750 (87.5000) lr 1.5090e-03 eta 0:05:51
epoch [68/200] batch [1/3] time 1.052 (1.052) data 0.258 (0.258) loss 0.6973 (0.6973) acc 84.3750 (84.3750) lr 1.5090e-03 eta 0:06:58
epoch [68/200] batch [2/3] time 0.797 (0.925) data 0.000 (0.129) loss 0.3857 (0.5415) acc 90.6250 (87.5000) lr 1.5090e-03 eta 0:06:07
epoch [68/200] batch [3/3] time 0.794 (0.881) data 0.000 (0.086) loss 0.3269 (0.4700) acc 93.7500 (89.5833) lr 1.4955e-03 eta 0:05:48
epoch [69/200] batch [1/3] time 1.071 (1.071) data 0.277 (0.277) loss 0.9351 (0.9351) acc 78.1250 (78.1250) lr 1.4955e-03 eta 0:07:02
epoch [69/200] batch [2/3] time 0.795 (0.933) data 0.000 (0.139) loss 0.1810 (0.5580) acc 96.8750 (87.5000) lr 1.4955e-03 eta 0:06:07
epoch [69/200] batch [3/3] time 0.794 (0.887) data 0.000 (0.092) loss 0.3535 (0.4899) acc 90.6250 (88.5417) lr 1.4818e-03 eta 0:05:48
epoch [70/200] batch [1/3] time 1.059 (1.059) data 0.267 (0.267) loss 0.2739 (0.2739) acc 93.7500 (93.7500) lr 1.4818e-03 eta 0:06:54
epoch [70/200] batch [2/3] time 0.795 (0.927) data 0.000 (0.134) loss 0.5615 (0.4177) acc 84.3750 (89.0625) lr 1.4818e-03 eta 0:06:02
epoch [70/200] batch [3/3] time 0.795 (0.883) data 0.000 (0.089) loss 0.6548 (0.4967) acc 90.6250 (89.5833) lr 1.4679e-03 eta 0:05:44
epoch [71/200] batch [1/3] time 1.069 (1.069) data 0.273 (0.273) loss 0.8936 (0.8936) acc 81.2500 (81.2500) lr 1.4679e-03 eta 0:06:55
epoch [71/200] batch [2/3] time 0.795 (0.932) data 0.000 (0.137) loss 1.0410 (0.9673) acc 75.0000 (78.1250) lr 1.4679e-03 eta 0:06:01
epoch [71/200] batch [3/3] time 0.795 (0.886) data 0.000 (0.091) loss 0.5903 (0.8416) acc 93.7500 (83.3333) lr 1.4540e-03 eta 0:05:42
epoch [72/200] batch [1/3] time 1.055 (1.055) data 0.259 (0.259) loss 0.2277 (0.2277) acc 93.7500 (93.7500) lr 1.4540e-03 eta 0:06:47
epoch [72/200] batch [2/3] time 0.795 (0.925) data 0.000 (0.129) loss 0.3193 (0.2735) acc 87.5000 (90.6250) lr 1.4540e-03 eta 0:05:55
epoch [72/200] batch [3/3] time 0.794 (0.881) data 0.000 (0.086) loss 0.7495 (0.4322) acc 84.3750 (88.5417) lr 1.4399e-03 eta 0:05:38
epoch [73/200] batch [1/3] time 1.064 (1.064) data 0.267 (0.267) loss 0.5728 (0.5728) acc 90.6250 (90.6250) lr 1.4399e-03 eta 0:06:47
epoch [73/200] batch [2/3] time 0.796 (0.930) data 0.000 (0.133) loss 0.4377 (0.5052) acc 90.6250 (90.6250) lr 1.4399e-03 eta 0:05:55
epoch [73/200] batch [3/3] time 0.797 (0.885) data 0.000 (0.089) loss 0.4294 (0.4800) acc 90.6250 (90.6250) lr 1.4258e-03 eta 0:05:37
epoch [74/200] batch [1/3] time 1.066 (1.066) data 0.271 (0.271) loss 0.4485 (0.4485) acc 90.6250 (90.6250) lr 1.4258e-03 eta 0:06:44
epoch [74/200] batch [2/3] time 0.797 (0.931) data 0.000 (0.135) loss 0.7295 (0.5890) acc 81.2500 (85.9375) lr 1.4258e-03 eta 0:05:52
epoch [74/200] batch [3/3] time 0.797 (0.886) data 0.000 (0.090) loss 0.6299 (0.6026) acc 84.3750 (85.4167) lr 1.4115e-03 eta 0:05:35
epoch [75/200] batch [1/3] time 1.068 (1.068) data 0.274 (0.274) loss 0.4502 (0.4502) acc 87.5000 (87.5000) lr 1.4115e-03 eta 0:06:42
epoch [75/200] batch [2/3] time 0.796 (0.932) data 0.000 (0.137) loss 0.4717 (0.4609) acc 84.3750 (85.9375) lr 1.4115e-03 eta 0:05:50
epoch [75/200] batch [3/3] time 0.797 (0.887) data 0.000 (0.091) loss 1.0352 (0.6523) acc 71.8750 (81.2500) lr 1.3971e-03 eta 0:05:32
epoch [76/200] batch [1/3] time 1.061 (1.061) data 0.268 (0.268) loss 0.7588 (0.7588) acc 81.2500 (81.2500) lr 1.3971e-03 eta 0:06:36
epoch [76/200] batch [2/3] time 0.795 (0.928) data 0.000 (0.134) loss 0.3271 (0.5430) acc 87.5000 (84.3750) lr 1.3971e-03 eta 0:05:46
epoch [76/200] batch [3/3] time 0.796 (0.884) data 0.000 (0.089) loss 0.5571 (0.5477) acc 81.2500 (83.3333) lr 1.3827e-03 eta 0:05:28
epoch [77/200] batch [1/3] time 1.054 (1.054) data 0.257 (0.257) loss 0.3508 (0.3508) acc 93.7500 (93.7500) lr 1.3827e-03 eta 0:06:30
epoch [77/200] batch [2/3] time 0.797 (0.925) data 0.000 (0.129) loss 0.4563 (0.4036) acc 90.6250 (92.1875) lr 1.3827e-03 eta 0:05:42
epoch [77/200] batch [3/3] time 0.795 (0.882) data 0.000 (0.086) loss 0.5776 (0.4616) acc 78.1250 (87.5000) lr 1.3681e-03 eta 0:05:25
epoch [78/200] batch [1/3] time 1.057 (1.057) data 0.261 (0.261) loss 0.5610 (0.5610) acc 87.5000 (87.5000) lr 1.3681e-03 eta 0:06:28
epoch [78/200] batch [2/3] time 0.797 (0.927) data 0.000 (0.131) loss 0.3096 (0.4353) acc 90.6250 (89.0625) lr 1.3681e-03 eta 0:05:40
epoch [78/200] batch [3/3] time 0.797 (0.883) data 0.000 (0.087) loss 0.3003 (0.3903) acc 93.7500 (90.6250) lr 1.3535e-03 eta 0:05:23
epoch [79/200] batch [1/3] time 1.062 (1.062) data 0.266 (0.266) loss 0.2739 (0.2739) acc 93.7500 (93.7500) lr 1.3535e-03 eta 0:06:27
epoch [79/200] batch [2/3] time 0.797 (0.929) data 0.000 (0.133) loss 0.4417 (0.3578) acc 93.7500 (93.7500) lr 1.3535e-03 eta 0:05:38
epoch [79/200] batch [3/3] time 0.797 (0.885) data 0.000 (0.089) loss 0.4231 (0.3796) acc 90.6250 (92.7083) lr 1.3387e-03 eta 0:05:21
epoch [80/200] batch [1/3] time 1.060 (1.060) data 0.267 (0.267) loss 0.5923 (0.5923) acc 87.5000 (87.5000) lr 1.3387e-03 eta 0:06:23
epoch [80/200] batch [2/3] time 0.797 (0.928) data 0.000 (0.134) loss 0.6719 (0.6321) acc 87.5000 (87.5000) lr 1.3387e-03 eta 0:05:35
epoch [80/200] batch [3/3] time 0.797 (0.884) data 0.000 (0.089) loss 0.1478 (0.4707) acc 96.8750 (90.6250) lr 1.3239e-03 eta 0:05:18
epoch [81/200] batch [1/3] time 1.054 (1.054) data 0.257 (0.257) loss 0.3276 (0.3276) acc 96.8750 (96.8750) lr 1.3239e-03 eta 0:06:18
epoch [81/200] batch [2/3] time 0.795 (0.924) data 0.000 (0.129) loss 0.3696 (0.3486) acc 87.5000 (92.1875) lr 1.3239e-03 eta 0:05:30
epoch [81/200] batch [3/3] time 0.796 (0.882) data 0.000 (0.086) loss 0.5088 (0.4020) acc 84.3750 (89.5833) lr 1.3090e-03 eta 0:05:14
epoch [82/200] batch [1/3] time 1.051 (1.051) data 0.257 (0.257) loss 0.5747 (0.5747) acc 87.5000 (87.5000) lr 1.3090e-03 eta 0:06:14
epoch [82/200] batch [2/3] time 0.798 (0.924) data 0.000 (0.129) loss 0.4302 (0.5024) acc 90.6250 (89.0625) lr 1.3090e-03 eta 0:05:28
epoch [82/200] batch [3/3] time 0.795 (0.881) data 0.000 (0.086) loss 0.2045 (0.4031) acc 96.8750 (91.6667) lr 1.2940e-03 eta 0:05:11
epoch [83/200] batch [1/3] time 1.060 (1.060) data 0.265 (0.265) loss 0.2025 (0.2025) acc 96.8750 (96.8750) lr 1.2940e-03 eta 0:06:14
epoch [83/200] batch [2/3] time 0.797 (0.929) data 0.000 (0.133) loss 0.2908 (0.2466) acc 96.8750 (96.8750) lr 1.2940e-03 eta 0:05:26
epoch [83/200] batch [3/3] time 0.796 (0.884) data 0.000 (0.089) loss 0.1896 (0.2276) acc 96.8750 (96.8750) lr 1.2790e-03 eta 0:05:10
epoch [84/200] batch [1/3] time 1.056 (1.056) data 0.260 (0.260) loss 0.4712 (0.4712) acc 87.5000 (87.5000) lr 1.2790e-03 eta 0:06:09
epoch [84/200] batch [2/3] time 0.797 (0.926) data 0.000 (0.130) loss 0.5054 (0.4883) acc 84.3750 (85.9375) lr 1.2790e-03 eta 0:05:23
epoch [84/200] batch [3/3] time 0.797 (0.883) data 0.000 (0.087) loss 0.1663 (0.3809) acc 93.7500 (88.5417) lr 1.2639e-03 eta 0:05:07
epoch [85/200] batch [1/3] time 1.061 (1.061) data 0.265 (0.265) loss 0.7285 (0.7285) acc 87.5000 (87.5000) lr 1.2639e-03 eta 0:06:08
epoch [85/200] batch [2/3] time 0.797 (0.929) data 0.000 (0.133) loss 0.5312 (0.6299) acc 81.2500 (84.3750) lr 1.2639e-03 eta 0:05:21
epoch [85/200] batch [3/3] time 0.797 (0.885) data 0.000 (0.088) loss 0.6094 (0.6230) acc 84.3750 (84.3750) lr 1.2487e-03 eta 0:05:05
epoch [86/200] batch [1/3] time 1.056 (1.056) data 0.262 (0.262) loss 0.3987 (0.3987) acc 87.5000 (87.5000) lr 1.2487e-03 eta 0:06:03
epoch [86/200] batch [2/3] time 0.794 (0.925) data 0.000 (0.131) loss 0.8989 (0.6488) acc 78.1250 (82.8125) lr 1.2487e-03 eta 0:05:17
epoch [86/200] batch [3/3] time 0.797 (0.883) data 0.000 (0.088) loss 0.1490 (0.4822) acc 100.0000 (88.5417) lr 1.2334e-03 eta 0:05:01
epoch [87/200] batch [1/3] time 1.054 (1.054) data 0.259 (0.259) loss 0.5376 (0.5376) acc 87.5000 (87.5000) lr 1.2334e-03 eta 0:05:59
epoch [87/200] batch [2/3] time 0.796 (0.925) data 0.000 (0.129) loss 0.5938 (0.5657) acc 81.2500 (84.3750) lr 1.2334e-03 eta 0:05:14
epoch [87/200] batch [3/3] time 0.795 (0.882) data 0.000 (0.086) loss 0.5874 (0.5729) acc 87.5000 (85.4167) lr 1.2181e-03 eta 0:04:58
epoch [88/200] batch [1/3] time 1.061 (1.061) data 0.267 (0.267) loss 0.8105 (0.8105) acc 81.2500 (81.2500) lr 1.2181e-03 eta 0:05:58
epoch [88/200] batch [2/3] time 0.797 (0.929) data 0.000 (0.134) loss 0.1876 (0.4991) acc 96.8750 (89.0625) lr 1.2181e-03 eta 0:05:13
epoch [88/200] batch [3/3] time 0.795 (0.884) data 0.000 (0.089) loss 0.3887 (0.4623) acc 93.7500 (90.6250) lr 1.2028e-03 eta 0:04:57
epoch [89/200] batch [1/3] time 1.065 (1.065) data 0.272 (0.272) loss 0.4846 (0.4846) acc 84.3750 (84.3750) lr 1.2028e-03 eta 0:05:56
epoch [89/200] batch [2/3] time 0.795 (0.930) data 0.000 (0.136) loss 0.1664 (0.3255) acc 100.0000 (92.1875) lr 1.2028e-03 eta 0:05:10
epoch [89/200] batch [3/3] time 0.796 (0.885) data 0.000 (0.091) loss 0.6519 (0.4343) acc 87.5000 (90.6250) lr 1.1874e-03 eta 0:04:54
epoch [90/200] batch [1/3] time 1.065 (1.065) data 0.272 (0.272) loss 0.4314 (0.4314) acc 90.6250 (90.6250) lr 1.1874e-03 eta 0:05:53
epoch [90/200] batch [2/3] time 0.795 (0.930) data 0.000 (0.136) loss 0.5220 (0.4767) acc 84.3750 (87.5000) lr 1.1874e-03 eta 0:05:07
epoch [90/200] batch [3/3] time 0.794 (0.885) data 0.000 (0.091) loss 0.4438 (0.4657) acc 87.5000 (87.5000) lr 1.1719e-03 eta 0:04:52
epoch [91/200] batch [1/3] time 1.072 (1.072) data 0.276 (0.276) loss 0.3853 (0.3853) acc 87.5000 (87.5000) lr 1.1719e-03 eta 0:05:52
epoch [91/200] batch [2/3] time 0.794 (0.933) data 0.000 (0.138) loss 0.4673 (0.4263) acc 87.5000 (87.5000) lr 1.1719e-03 eta 0:05:05
epoch [91/200] batch [3/3] time 0.797 (0.888) data 0.000 (0.092) loss 0.5879 (0.4801) acc 87.5000 (87.5000) lr 1.1564e-03 eta 0:04:50
epoch [92/200] batch [1/3] time 1.050 (1.050) data 0.259 (0.259) loss 0.7944 (0.7944) acc 84.3750 (84.3750) lr 1.1564e-03 eta 0:05:42
epoch [92/200] batch [2/3] time 0.795 (0.922) data 0.000 (0.129) loss 0.2104 (0.5024) acc 96.8750 (90.6250) lr 1.1564e-03 eta 0:04:59
epoch [92/200] batch [3/3] time 0.795 (0.880) data 0.000 (0.086) loss 0.1908 (0.3986) acc 96.8750 (92.7083) lr 1.1409e-03 eta 0:04:45
epoch [93/200] batch [1/3] time 1.056 (1.056) data 0.260 (0.260) loss 0.4836 (0.4836) acc 90.6250 (90.6250) lr 1.1409e-03 eta 0:05:40
epoch [93/200] batch [2/3] time 0.795 (0.925) data 0.000 (0.130) loss 0.6357 (0.5597) acc 87.5000 (89.0625) lr 1.1409e-03 eta 0:04:57
epoch [93/200] batch [3/3] time 0.802 (0.884) data 0.000 (0.087) loss 0.4751 (0.5315) acc 84.3750 (87.5000) lr 1.1253e-03 eta 0:04:43
epoch [94/200] batch [1/3] time 1.058 (1.058) data 0.263 (0.263) loss 0.3330 (0.3330) acc 90.6250 (90.6250) lr 1.1253e-03 eta 0:05:38
epoch [94/200] batch [2/3] time 0.795 (0.927) data 0.000 (0.131) loss 0.2434 (0.2882) acc 96.8750 (93.7500) lr 1.1253e-03 eta 0:04:55
epoch [94/200] batch [3/3] time 0.795 (0.883) data 0.000 (0.088) loss 0.2429 (0.2731) acc 93.7500 (93.7500) lr 1.1097e-03 eta 0:04:40
epoch [95/200] batch [1/3] time 1.052 (1.052) data 0.259 (0.259) loss 0.4768 (0.4768) acc 87.5000 (87.5000) lr 1.1097e-03 eta 0:05:33
epoch [95/200] batch [2/3] time 0.795 (0.923) data 0.000 (0.129) loss 0.4968 (0.4868) acc 87.5000 (87.5000) lr 1.1097e-03 eta 0:04:51
epoch [95/200] batch [3/3] time 0.795 (0.881) data 0.000 (0.086) loss 0.2480 (0.4072) acc 90.6250 (88.5417) lr 1.0941e-03 eta 0:04:37
epoch [96/200] batch [1/3] time 1.065 (1.065) data 0.269 (0.269) loss 0.3818 (0.3818) acc 90.6250 (90.6250) lr 1.0941e-03 eta 0:05:34
epoch [96/200] batch [2/3] time 0.795 (0.930) data 0.000 (0.135) loss 0.3018 (0.3418) acc 90.6250 (90.6250) lr 1.0941e-03 eta 0:04:51
epoch [96/200] batch [3/3] time 0.794 (0.885) data 0.000 (0.090) loss 0.4521 (0.3786) acc 90.6250 (90.6250) lr 1.0785e-03 eta 0:04:36
epoch [97/200] batch [1/3] time 1.059 (1.059) data 0.266 (0.266) loss 0.3281 (0.3281) acc 90.6250 (90.6250) lr 1.0785e-03 eta 0:05:29
epoch [97/200] batch [2/3] time 0.796 (0.928) data 0.000 (0.133) loss 0.7256 (0.5269) acc 84.3750 (87.5000) lr 1.0785e-03 eta 0:04:47
epoch [97/200] batch [3/3] time 0.796 (0.884) data 0.000 (0.089) loss 0.4272 (0.4937) acc 87.5000 (87.5000) lr 1.0628e-03 eta 0:04:33
epoch [98/200] batch [1/3] time 1.057 (1.057) data 0.260 (0.260) loss 0.4023 (0.4023) acc 87.5000 (87.5000) lr 1.0628e-03 eta 0:05:25
epoch [98/200] batch [2/3] time 0.797 (0.927) data 0.000 (0.130) loss 0.3584 (0.3804) acc 93.7500 (90.6250) lr 1.0628e-03 eta 0:04:44
epoch [98/200] batch [3/3] time 0.797 (0.884) data 0.000 (0.087) loss 0.5557 (0.4388) acc 87.5000 (89.5833) lr 1.0471e-03 eta 0:04:30
epoch [99/200] batch [1/3] time 1.063 (1.063) data 0.267 (0.267) loss 0.2632 (0.2632) acc 90.6250 (90.6250) lr 1.0471e-03 eta 0:05:24
epoch [99/200] batch [2/3] time 0.797 (0.930) data 0.000 (0.134) loss 0.4297 (0.3464) acc 84.3750 (87.5000) lr 1.0471e-03 eta 0:04:42
epoch [99/200] batch [3/3] time 0.795 (0.885) data 0.000 (0.089) loss 0.6616 (0.4515) acc 90.6250 (88.5417) lr 1.0314e-03 eta 0:04:28
epoch [100/200] batch [1/3] time 1.059 (1.059) data 0.266 (0.266) loss 0.3125 (0.3125) acc 96.8750 (96.8750) lr 1.0314e-03 eta 0:05:19
epoch [100/200] batch [2/3] time 0.795 (0.927) data 0.000 (0.133) loss 0.2654 (0.2889) acc 93.7500 (95.3125) lr 1.0314e-03 eta 0:04:39
epoch [100/200] batch [3/3] time 0.796 (0.884) data 0.000 (0.089) loss 0.7744 (0.4508) acc 84.3750 (91.6667) lr 1.0157e-03 eta 0:04:25
epoch [101/200] batch [1/3] time 1.064 (1.064) data 0.269 (0.269) loss 0.6074 (0.6074) acc 81.2500 (81.2500) lr 1.0157e-03 eta 0:05:18
epoch [101/200] batch [2/3] time 0.796 (0.930) data 0.000 (0.135) loss 0.1951 (0.4012) acc 93.7500 (87.5000) lr 1.0157e-03 eta 0:04:37
epoch [101/200] batch [3/3] time 0.796 (0.885) data 0.000 (0.090) loss 0.4060 (0.4028) acc 90.6250 (88.5417) lr 1.0000e-03 eta 0:04:22
epoch [102/200] batch [1/3] time 1.062 (1.062) data 0.267 (0.267) loss 0.3923 (0.3923) acc 87.5000 (87.5000) lr 1.0000e-03 eta 0:05:14
epoch [102/200] batch [2/3] time 0.796 (0.929) data 0.000 (0.133) loss 0.8564 (0.6244) acc 81.2500 (84.3750) lr 1.0000e-03 eta 0:04:34
epoch [102/200] batch [3/3] time 0.795 (0.884) data 0.000 (0.089) loss 0.3506 (0.5331) acc 93.7500 (87.5000) lr 9.8429e-04 eta 0:04:20
epoch [103/200] batch [1/3] time 1.068 (1.068) data 0.275 (0.275) loss 0.5791 (0.5791) acc 84.3750 (84.3750) lr 9.8429e-04 eta 0:05:13
epoch [103/200] batch [2/3] time 0.795 (0.932) data 0.000 (0.138) loss 0.1388 (0.3589) acc 96.8750 (90.6250) lr 9.8429e-04 eta 0:04:32
epoch [103/200] batch [3/3] time 0.795 (0.886) data 0.000 (0.092) loss 0.2534 (0.3238) acc 93.7500 (91.6667) lr 9.6859e-04 eta 0:04:17
epoch [104/200] batch [1/3] time 1.060 (1.060) data 0.266 (0.266) loss 0.3225 (0.3225) acc 93.7500 (93.7500) lr 9.6859e-04 eta 0:05:07
epoch [104/200] batch [2/3] time 0.796 (0.928) data 0.000 (0.133) loss 0.3696 (0.3461) acc 90.6250 (92.1875) lr 9.6859e-04 eta 0:04:28
epoch [104/200] batch [3/3] time 0.795 (0.884) data 0.000 (0.089) loss 0.2311 (0.3077) acc 93.7500 (92.7083) lr 9.5289e-04 eta 0:04:14
epoch [105/200] batch [1/3] time 1.066 (1.066) data 0.273 (0.273) loss 0.2837 (0.2837) acc 90.6250 (90.6250) lr 9.5289e-04 eta 0:05:06
epoch [105/200] batch [2/3] time 0.795 (0.931) data 0.000 (0.136) loss 0.5317 (0.4077) acc 84.3750 (87.5000) lr 9.5289e-04 eta 0:04:26
epoch [105/200] batch [3/3] time 0.800 (0.887) data 0.000 (0.091) loss 0.5864 (0.4673) acc 81.2500 (85.4167) lr 9.3721e-04 eta 0:04:12
epoch [106/200] batch [1/3] time 1.062 (1.062) data 0.267 (0.267) loss 0.4626 (0.4626) acc 84.3750 (84.3750) lr 9.3721e-04 eta 0:05:01
epoch [106/200] batch [2/3] time 0.797 (0.929) data 0.000 (0.134) loss 0.1804 (0.3215) acc 96.8750 (90.6250) lr 9.3721e-04 eta 0:04:23
epoch [106/200] batch [3/3] time 0.795 (0.885) data 0.000 (0.089) loss 0.1685 (0.2705) acc 93.7500 (91.6667) lr 9.2154e-04 eta 0:04:09
epoch [107/200] batch [1/3] time 1.061 (1.061) data 0.266 (0.266) loss 0.3826 (0.3826) acc 87.5000 (87.5000) lr 9.2154e-04 eta 0:04:58
epoch [107/200] batch [2/3] time 0.797 (0.929) data 0.000 (0.133) loss 0.6216 (0.5021) acc 81.2500 (84.3750) lr 9.2154e-04 eta 0:04:20
epoch [107/200] batch [3/3] time 0.797 (0.885) data 0.000 (0.089) loss 0.2084 (0.4042) acc 96.8750 (88.5417) lr 9.0589e-04 eta 0:04:06
epoch [108/200] batch [1/3] time 1.051 (1.051) data 0.257 (0.257) loss 1.0205 (1.0205) acc 81.2500 (81.2500) lr 9.0589e-04 eta 0:04:52
epoch [108/200] batch [2/3] time 0.795 (0.923) data 0.000 (0.129) loss 0.4053 (0.7129) acc 93.7500 (87.5000) lr 9.0589e-04 eta 0:04:15
epoch [108/200] batch [3/3] time 0.797 (0.881) data 0.000 (0.086) loss 0.2666 (0.5641) acc 93.7500 (89.5833) lr 8.9027e-04 eta 0:04:03
epoch [109/200] batch [1/3] time 1.053 (1.053) data 0.259 (0.259) loss 0.3174 (0.3174) acc 96.8750 (96.8750) lr 8.9027e-04 eta 0:04:49
epoch [109/200] batch [2/3] time 0.796 (0.925) data 0.000 (0.130) loss 0.3245 (0.3209) acc 93.7500 (95.3125) lr 8.9027e-04 eta 0:04:13
epoch [109/200] batch [3/3] time 0.797 (0.882) data 0.000 (0.086) loss 0.6504 (0.4307) acc 84.3750 (91.6667) lr 8.7467e-04 eta 0:04:00
epoch [110/200] batch [1/3] time 1.053 (1.053) data 0.259 (0.259) loss 0.8096 (0.8096) acc 84.3750 (84.3750) lr 8.7467e-04 eta 0:04:46
epoch [110/200] batch [2/3] time 0.795 (0.924) data 0.000 (0.130) loss 0.2188 (0.5142) acc 93.7500 (89.0625) lr 8.7467e-04 eta 0:04:10
epoch [110/200] batch [3/3] time 0.797 (0.882) data 0.000 (0.087) loss 0.3206 (0.4496) acc 96.8750 (91.6667) lr 8.5910e-04 eta 0:03:58
epoch [111/200] batch [1/3] time 1.050 (1.050) data 0.259 (0.259) loss 0.3560 (0.3560) acc 90.6250 (90.6250) lr 8.5910e-04 eta 0:04:42
epoch [111/200] batch [2/3] time 0.797 (0.924) data 0.000 (0.129) loss 0.2957 (0.3258) acc 93.7500 (92.1875) lr 8.5910e-04 eta 0:04:07
epoch [111/200] batch [3/3] time 0.795 (0.881) data 0.000 (0.086) loss 0.6294 (0.4270) acc 84.3750 (89.5833) lr 8.4357e-04 eta 0:03:55
epoch [112/200] batch [1/3] time 1.061 (1.061) data 0.265 (0.265) loss 0.6299 (0.6299) acc 84.3750 (84.3750) lr 8.4357e-04 eta 0:04:42
epoch [112/200] batch [2/3] time 0.797 (0.929) data 0.000 (0.133) loss 0.3157 (0.4728) acc 93.7500 (89.0625) lr 8.4357e-04 eta 0:04:06
epoch [112/200] batch [3/3] time 0.795 (0.884) data 0.000 (0.088) loss 0.5093 (0.4849) acc 90.6250 (89.5833) lr 8.2807e-04 eta 0:03:53
epoch [113/200] batch [1/3] time 1.071 (1.071) data 0.276 (0.276) loss 0.2194 (0.2194) acc 93.7500 (93.7500) lr 8.2807e-04 eta 0:04:41
epoch [113/200] batch [2/3] time 0.795 (0.933) data 0.000 (0.138) loss 0.1487 (0.1840) acc 96.8750 (95.3125) lr 8.2807e-04 eta 0:04:04
epoch [113/200] batch [3/3] time 0.797 (0.888) data 0.000 (0.092) loss 0.3984 (0.2555) acc 93.7500 (94.7917) lr 8.1262e-04 eta 0:03:51
epoch [114/200] batch [1/3] time 1.067 (1.067) data 0.274 (0.274) loss 0.5840 (0.5840) acc 87.5000 (87.5000) lr 8.1262e-04 eta 0:04:37
epoch [114/200] batch [2/3] time 0.796 (0.931) data 0.000 (0.137) loss 0.2496 (0.4168) acc 96.8750 (92.1875) lr 8.1262e-04 eta 0:04:01
epoch [114/200] batch [3/3] time 0.795 (0.886) data 0.000 (0.091) loss 0.5181 (0.4506) acc 81.2500 (88.5417) lr 7.9721e-04 eta 0:03:48
epoch [115/200] batch [1/3] time 1.056 (1.056) data 0.261 (0.261) loss 0.1447 (0.1447) acc 96.8750 (96.8750) lr 7.9721e-04 eta 0:04:31
epoch [115/200] batch [2/3] time 0.795 (0.926) data 0.000 (0.131) loss 0.4402 (0.2924) acc 87.5000 (92.1875) lr 7.9721e-04 eta 0:03:56
epoch [115/200] batch [3/3] time 0.795 (0.882) data 0.000 (0.087) loss 0.1439 (0.2429) acc 96.8750 (93.7500) lr 7.8186e-04 eta 0:03:44
epoch [116/200] batch [1/3] time 1.070 (1.070) data 0.277 (0.277) loss 0.2080 (0.2080) acc 90.6250 (90.6250) lr 7.8186e-04 eta 0:04:31
epoch [116/200] batch [2/3] time 0.797 (0.934) data 0.000 (0.139) loss 0.5454 (0.3767) acc 87.5000 (89.0625) lr 7.8186e-04 eta 0:03:56
epoch [116/200] batch [3/3] time 0.798 (0.888) data 0.000 (0.092) loss 0.5688 (0.4408) acc 93.7500 (90.6250) lr 7.6655e-04 eta 0:03:43
epoch [117/200] batch [1/3] time 1.053 (1.053) data 0.257 (0.257) loss 0.4407 (0.4407) acc 87.5000 (87.5000) lr 7.6655e-04 eta 0:04:24
epoch [117/200] batch [2/3] time 0.795 (0.924) data 0.000 (0.129) loss 0.6860 (0.5634) acc 78.1250 (82.8125) lr 7.6655e-04 eta 0:03:50
epoch [117/200] batch [3/3] time 0.798 (0.882) data 0.000 (0.086) loss 0.3918 (0.5062) acc 84.3750 (83.3333) lr 7.5131e-04 eta 0:03:39
epoch [118/200] batch [1/3] time 1.065 (1.065) data 0.268 (0.268) loss 0.2871 (0.2871) acc 90.6250 (90.6250) lr 7.5131e-04 eta 0:04:24
epoch [118/200] batch [2/3] time 0.796 (0.931) data 0.000 (0.134) loss 0.2229 (0.2550) acc 96.8750 (93.7500) lr 7.5131e-04 eta 0:03:49
epoch [118/200] batch [3/3] time 0.796 (0.886) data 0.000 (0.090) loss 0.3384 (0.2828) acc 100.0000 (95.8333) lr 7.3613e-04 eta 0:03:37
epoch [119/200] batch [1/3] time 1.053 (1.053) data 0.258 (0.258) loss 0.1986 (0.1986) acc 96.8750 (96.8750) lr 7.3613e-04 eta 0:04:18
epoch [119/200] batch [2/3] time 0.797 (0.925) data 0.000 (0.129) loss 0.4561 (0.3273) acc 90.6250 (93.7500) lr 7.3613e-04 eta 0:03:45
epoch [119/200] batch [3/3] time 0.796 (0.882) data 0.000 (0.086) loss 0.6372 (0.4306) acc 87.5000 (91.6667) lr 7.2101e-04 eta 0:03:34
epoch [120/200] batch [1/3] time 1.058 (1.058) data 0.261 (0.261) loss 0.5327 (0.5327) acc 87.5000 (87.5000) lr 7.2101e-04 eta 0:04:15
epoch [120/200] batch [2/3] time 0.797 (0.928) data 0.000 (0.131) loss 0.3376 (0.4352) acc 93.7500 (90.6250) lr 7.2101e-04 eta 0:03:43
epoch [120/200] batch [3/3] time 0.798 (0.884) data 0.000 (0.087) loss 0.1853 (0.3519) acc 96.8750 (92.7083) lr 7.0596e-04 eta 0:03:32
epoch [121/200] batch [1/3] time 1.051 (1.051) data 0.257 (0.257) loss 0.4272 (0.4272) acc 90.6250 (90.6250) lr 7.0596e-04 eta 0:04:11
epoch [121/200] batch [2/3] time 0.797 (0.924) data 0.000 (0.129) loss 0.3560 (0.3916) acc 93.7500 (92.1875) lr 7.0596e-04 eta 0:03:39
epoch [121/200] batch [3/3] time 0.797 (0.882) data 0.000 (0.086) loss 0.2864 (0.3565) acc 93.7500 (92.7083) lr 6.9098e-04 eta 0:03:28
epoch [122/200] batch [1/3] time 1.056 (1.056) data 0.260 (0.260) loss 0.3818 (0.3818) acc 90.6250 (90.6250) lr 6.9098e-04 eta 0:04:09
epoch [122/200] batch [2/3] time 0.795 (0.925) data 0.000 (0.130) loss 0.3623 (0.3721) acc 93.7500 (92.1875) lr 6.9098e-04 eta 0:03:37
epoch [122/200] batch [3/3] time 0.798 (0.883) data 0.000 (0.087) loss 0.2432 (0.3291) acc 93.7500 (92.7083) lr 6.7608e-04 eta 0:03:26
epoch [123/200] batch [1/3] time 1.058 (1.058) data 0.266 (0.266) loss 0.5229 (0.5229) acc 87.5000 (87.5000) lr 6.7608e-04 eta 0:04:06
epoch [123/200] batch [2/3] time 0.797 (0.927) data 0.000 (0.133) loss 0.3303 (0.4266) acc 96.8750 (92.1875) lr 6.7608e-04 eta 0:03:35
epoch [123/200] batch [3/3] time 0.795 (0.883) data 0.000 (0.089) loss 0.3379 (0.3971) acc 93.7500 (92.7083) lr 6.6126e-04 eta 0:03:24
epoch [124/200] batch [1/3] time 1.052 (1.052) data 0.259 (0.259) loss 0.2896 (0.2896) acc 90.6250 (90.6250) lr 6.6126e-04 eta 0:04:02
epoch [124/200] batch [2/3] time 0.796 (0.924) data 0.000 (0.129) loss 0.5342 (0.4119) acc 84.3750 (87.5000) lr 6.6126e-04 eta 0:03:31
epoch [124/200] batch [3/3] time 0.797 (0.882) data 0.000 (0.086) loss 0.1785 (0.3341) acc 93.7500 (89.5833) lr 6.4653e-04 eta 0:03:21
epoch [125/200] batch [1/3] time 1.055 (1.055) data 0.261 (0.261) loss 0.1882 (0.1882) acc 96.8750 (96.8750) lr 6.4653e-04 eta 0:03:59
epoch [125/200] batch [2/3] time 0.797 (0.926) data 0.000 (0.130) loss 0.3557 (0.2720) acc 93.7500 (95.3125) lr 6.4653e-04 eta 0:03:29
epoch [125/200] batch [3/3] time 0.798 (0.883) data 0.000 (0.087) loss 0.1274 (0.2238) acc 100.0000 (96.8750) lr 6.3188e-04 eta 0:03:18
epoch [126/200] batch [1/3] time 1.057 (1.057) data 0.261 (0.261) loss 0.3533 (0.3533) acc 96.8750 (96.8750) lr 6.3188e-04 eta 0:03:56
epoch [126/200] batch [2/3] time 0.796 (0.927) data 0.000 (0.131) loss 0.1307 (0.2420) acc 100.0000 (98.4375) lr 6.3188e-04 eta 0:03:26
epoch [126/200] batch [3/3] time 0.795 (0.883) data 0.000 (0.087) loss 0.0505 (0.1782) acc 100.0000 (98.9583) lr 6.1732e-04 eta 0:03:15
epoch [127/200] batch [1/3] time 1.069 (1.069) data 0.276 (0.276) loss 0.1497 (0.1497) acc 96.8750 (96.8750) lr 6.1732e-04 eta 0:03:56
epoch [127/200] batch [2/3] time 0.794 (0.932) data 0.000 (0.138) loss 0.4111 (0.2804) acc 90.6250 (93.7500) lr 6.1732e-04 eta 0:03:25
epoch [127/200] batch [3/3] time 0.795 (0.886) data 0.000 (0.092) loss 0.4651 (0.3420) acc 87.5000 (91.6667) lr 6.0285e-04 eta 0:03:14
epoch [128/200] batch [1/3] time 1.066 (1.066) data 0.275 (0.275) loss 0.1724 (0.1724) acc 96.8750 (96.8750) lr 6.0285e-04 eta 0:03:52
epoch [128/200] batch [2/3] time 0.795 (0.930) data 0.000 (0.138) loss 0.1421 (0.1572) acc 96.8750 (96.8750) lr 6.0285e-04 eta 0:03:21
epoch [128/200] batch [3/3] time 0.795 (0.885) data 0.000 (0.092) loss 0.5342 (0.2829) acc 93.7500 (95.8333) lr 5.8849e-04 eta 0:03:11
epoch [129/200] batch [1/3] time 1.071 (1.071) data 0.275 (0.275) loss 0.2942 (0.2942) acc 93.7500 (93.7500) lr 5.8849e-04 eta 0:03:50
epoch [129/200] batch [2/3] time 0.795 (0.933) data 0.000 (0.138) loss 0.4160 (0.3551) acc 87.5000 (90.6250) lr 5.8849e-04 eta 0:03:19
epoch [129/200] batch [3/3] time 0.795 (0.887) data 0.000 (0.092) loss 0.2666 (0.3256) acc 93.7500 (91.6667) lr 5.7422e-04 eta 0:03:08
epoch [130/200] batch [1/3] time 1.055 (1.055) data 0.261 (0.261) loss 0.3772 (0.3772) acc 87.5000 (87.5000) lr 5.7422e-04 eta 0:03:43
epoch [130/200] batch [2/3] time 0.794 (0.925) data 0.000 (0.131) loss 0.3767 (0.3770) acc 90.6250 (89.0625) lr 5.7422e-04 eta 0:03:15
epoch [130/200] batch [3/3] time 0.795 (0.881) data 0.000 (0.087) loss 0.5356 (0.4299) acc 87.5000 (88.5417) lr 5.6006e-04 eta 0:03:05
epoch [131/200] batch [1/3] time 1.067 (1.067) data 0.273 (0.273) loss 0.4985 (0.4985) acc 93.7500 (93.7500) lr 5.6006e-04 eta 0:03:43
epoch [131/200] batch [2/3] time 0.794 (0.931) data 0.000 (0.137) loss 0.4258 (0.4622) acc 84.3750 (89.0625) lr 5.6006e-04 eta 0:03:13
epoch [131/200] batch [3/3] time 0.794 (0.885) data 0.000 (0.091) loss 0.3403 (0.4215) acc 90.6250 (89.5833) lr 5.4601e-04 eta 0:03:03
epoch [132/200] batch [1/3] time 1.063 (1.063) data 0.267 (0.267) loss 0.5513 (0.5513) acc 81.2500 (81.2500) lr 5.4601e-04 eta 0:03:39
epoch [132/200] batch [2/3] time 0.796 (0.930) data 0.000 (0.134) loss 0.6914 (0.6213) acc 81.2500 (81.2500) lr 5.4601e-04 eta 0:03:10
epoch [132/200] batch [3/3] time 0.797 (0.886) data 0.000 (0.089) loss 0.1667 (0.4698) acc 96.8750 (86.4583) lr 5.3207e-04 eta 0:03:00
epoch [133/200] batch [1/3] time 1.052 (1.052) data 0.257 (0.257) loss 0.4626 (0.4626) acc 87.5000 (87.5000) lr 5.3207e-04 eta 0:03:33
epoch [133/200] batch [2/3] time 0.796 (0.924) data 0.000 (0.129) loss 0.2869 (0.3748) acc 90.6250 (89.0625) lr 5.3207e-04 eta 0:03:06
epoch [133/200] batch [3/3] time 0.795 (0.881) data 0.000 (0.086) loss 0.3889 (0.3795) acc 90.6250 (89.5833) lr 5.1825e-04 eta 0:02:57
epoch [134/200] batch [1/3] time 1.070 (1.070) data 0.275 (0.275) loss 0.3281 (0.3281) acc 90.6250 (90.6250) lr 5.1825e-04 eta 0:03:34
epoch [134/200] batch [2/3] time 0.797 (0.934) data 0.000 (0.137) loss 0.3352 (0.3317) acc 90.6250 (90.6250) lr 5.1825e-04 eta 0:03:05
epoch [134/200] batch [3/3] time 0.796 (0.888) data 0.000 (0.092) loss 0.3550 (0.3394) acc 90.6250 (90.6250) lr 5.0454e-04 eta 0:02:55
epoch [135/200] batch [1/3] time 1.065 (1.065) data 0.269 (0.269) loss 0.3989 (0.3989) acc 93.7500 (93.7500) lr 5.0454e-04 eta 0:03:29
epoch [135/200] batch [2/3] time 0.796 (0.930) data 0.000 (0.134) loss 0.3833 (0.3911) acc 93.7500 (93.7500) lr 5.0454e-04 eta 0:03:02
epoch [135/200] batch [3/3] time 0.798 (0.886) data 0.000 (0.090) loss 0.3152 (0.3658) acc 93.7500 (93.7500) lr 4.9096e-04 eta 0:02:52
epoch [136/200] batch [1/3] time 1.073 (1.073) data 0.275 (0.275) loss 0.3374 (0.3374) acc 93.7500 (93.7500) lr 4.9096e-04 eta 0:03:28
epoch [136/200] batch [2/3] time 0.797 (0.935) data 0.000 (0.138) loss 0.4512 (0.3943) acc 93.7500 (93.7500) lr 4.9096e-04 eta 0:03:00
epoch [136/200] batch [3/3] time 0.798 (0.889) data 0.000 (0.092) loss 0.3950 (0.3945) acc 93.7500 (93.7500) lr 4.7750e-04 eta 0:02:50
epoch [137/200] batch [1/3] time 1.064 (1.064) data 0.268 (0.268) loss 0.4885 (0.4885) acc 90.6250 (90.6250) lr 4.7750e-04 eta 0:03:23
epoch [137/200] batch [2/3] time 0.796 (0.930) data 0.000 (0.134) loss 0.3809 (0.4347) acc 90.6250 (90.6250) lr 4.7750e-04 eta 0:02:56
epoch [137/200] batch [3/3] time 0.796 (0.886) data 0.000 (0.089) loss 0.1780 (0.3491) acc 96.8750 (92.7083) lr 4.6417e-04 eta 0:02:47
epoch [138/200] batch [1/3] time 1.070 (1.070) data 0.277 (0.277) loss 0.4858 (0.4858) acc 84.3750 (84.3750) lr 4.6417e-04 eta 0:03:21
epoch [138/200] batch [2/3] time 0.797 (0.934) data 0.000 (0.138) loss 0.3274 (0.4066) acc 93.7500 (89.0625) lr 4.6417e-04 eta 0:02:54
epoch [138/200] batch [3/3] time 0.797 (0.888) data 0.000 (0.092) loss 0.2844 (0.3659) acc 90.6250 (89.5833) lr 4.5098e-04 eta 0:02:45
epoch [139/200] batch [1/3] time 1.070 (1.070) data 0.276 (0.276) loss 0.3110 (0.3110) acc 93.7500 (93.7500) lr 4.5098e-04 eta 0:03:17
epoch [139/200] batch [2/3] time 0.795 (0.932) data 0.000 (0.138) loss 0.7041 (0.5076) acc 81.2500 (87.5000) lr 4.5098e-04 eta 0:02:51
epoch [139/200] batch [3/3] time 0.797 (0.887) data 0.000 (0.092) loss 0.1276 (0.3809) acc 100.0000 (91.6667) lr 4.3792e-04 eta 0:02:42
epoch [140/200] batch [1/3] time 1.058 (1.058) data 0.266 (0.266) loss 0.7026 (0.7026) acc 84.3750 (84.3750) lr 4.3792e-04 eta 0:03:12
epoch [140/200] batch [2/3] time 0.795 (0.926) data 0.000 (0.133) loss 0.7080 (0.7053) acc 81.2500 (82.8125) lr 4.3792e-04 eta 0:02:47
epoch [140/200] batch [3/3] time 0.795 (0.883) data 0.000 (0.089) loss 0.2355 (0.5487) acc 93.7500 (86.4583) lr 4.2499e-04 eta 0:02:38
epoch [141/200] batch [1/3] time 1.064 (1.064) data 0.268 (0.268) loss 0.2910 (0.2910) acc 90.6250 (90.6250) lr 4.2499e-04 eta 0:03:10
epoch [141/200] batch [2/3] time 0.795 (0.929) data 0.000 (0.134) loss 0.5894 (0.4402) acc 84.3750 (87.5000) lr 4.2499e-04 eta 0:02:45
epoch [141/200] batch [3/3] time 0.795 (0.885) data 0.000 (0.089) loss 0.1832 (0.3545) acc 90.6250 (88.5417) lr 4.1221e-04 eta 0:02:36
epoch [142/200] batch [1/3] time 1.060 (1.060) data 0.266 (0.266) loss 0.2494 (0.2494) acc 93.7500 (93.7500) lr 4.1221e-04 eta 0:03:06
epoch [142/200] batch [2/3] time 0.798 (0.929) data 0.000 (0.133) loss 0.2401 (0.2448) acc 87.5000 (90.6250) lr 4.1221e-04 eta 0:02:42
epoch [142/200] batch [3/3] time 0.795 (0.884) data 0.000 (0.089) loss 0.1176 (0.2024) acc 100.0000 (93.7500) lr 3.9958e-04 eta 0:02:33
epoch [143/200] batch [1/3] time 1.064 (1.064) data 0.268 (0.268) loss 0.2703 (0.2703) acc 93.7500 (93.7500) lr 3.9958e-04 eta 0:03:04
epoch [143/200] batch [2/3] time 0.797 (0.930) data 0.000 (0.134) loss 0.3708 (0.3206) acc 84.3750 (89.0625) lr 3.9958e-04 eta 0:02:40
epoch [143/200] batch [3/3] time 0.797 (0.886) data 0.000 (0.089) loss 0.2084 (0.2832) acc 96.8750 (91.6667) lr 3.8709e-04 eta 0:02:31
epoch [144/200] batch [1/3] time 1.065 (1.065) data 0.269 (0.269) loss 0.3616 (0.3616) acc 90.6250 (90.6250) lr 3.8709e-04 eta 0:03:01
epoch [144/200] batch [2/3] time 0.797 (0.931) data 0.000 (0.135) loss 0.2023 (0.2819) acc 96.8750 (93.7500) lr 3.8709e-04 eta 0:02:37
epoch [144/200] batch [3/3] time 0.797 (0.887) data 0.000 (0.090) loss 0.2395 (0.2678) acc 93.7500 (93.7500) lr 3.7476e-04 eta 0:02:28
epoch [145/200] batch [1/3] time 1.052 (1.052) data 0.258 (0.258) loss 0.3030 (0.3030) acc 87.5000 (87.5000) lr 3.7476e-04 eta 0:02:55
epoch [145/200] batch [2/3] time 0.796 (0.924) data 0.000 (0.129) loss 0.5513 (0.4271) acc 87.5000 (87.5000) lr 3.7476e-04 eta 0:02:33
epoch [145/200] batch [3/3] time 0.795 (0.881) data 0.000 (0.086) loss 0.6626 (0.5056) acc 84.3750 (86.4583) lr 3.6258e-04 eta 0:02:25
epoch [146/200] batch [1/3] time 1.056 (1.056) data 0.261 (0.261) loss 0.2150 (0.2150) acc 100.0000 (100.0000) lr 3.6258e-04 eta 0:02:53
epoch [146/200] batch [2/3] time 0.794 (0.925) data 0.000 (0.131) loss 0.6348 (0.4249) acc 87.5000 (93.7500) lr 3.6258e-04 eta 0:02:30
epoch [146/200] batch [3/3] time 0.797 (0.883) data 0.000 (0.087) loss 0.2996 (0.3831) acc 93.7500 (93.7500) lr 3.5055e-04 eta 0:02:22
epoch [147/200] batch [1/3] time 1.066 (1.066) data 0.275 (0.275) loss 0.2534 (0.2534) acc 96.8750 (96.8750) lr 3.5055e-04 eta 0:02:51
epoch [147/200] batch [2/3] time 0.795 (0.930) data 0.000 (0.138) loss 0.3489 (0.3011) acc 93.7500 (95.3125) lr 3.5055e-04 eta 0:02:28
epoch [147/200] batch [3/3] time 0.795 (0.885) data 0.000 (0.092) loss 0.7979 (0.4667) acc 81.2500 (90.6250) lr 3.3869e-04 eta 0:02:20
epoch [148/200] batch [1/3] time 1.072 (1.072) data 0.275 (0.275) loss 0.3516 (0.3516) acc 96.8750 (96.8750) lr 3.3869e-04 eta 0:02:49
epoch [148/200] batch [2/3] time 0.796 (0.934) data 0.000 (0.138) loss 0.2917 (0.3217) acc 96.8750 (96.8750) lr 3.3869e-04 eta 0:02:26
epoch [148/200] batch [3/3] time 0.797 (0.889) data 0.000 (0.092) loss 0.3147 (0.3193) acc 90.6250 (94.7917) lr 3.2699e-04 eta 0:02:18
epoch [149/200] batch [1/3] time 1.060 (1.060) data 0.265 (0.265) loss 0.6782 (0.6782) acc 78.1250 (78.1250) lr 3.2699e-04 eta 0:02:44
epoch [149/200] batch [2/3] time 0.795 (0.928) data 0.000 (0.133) loss 0.2529 (0.4656) acc 96.8750 (87.5000) lr 3.2699e-04 eta 0:02:22
epoch [149/200] batch [3/3] time 0.795 (0.883) data 0.000 (0.088) loss 0.1962 (0.3758) acc 96.8750 (90.6250) lr 3.1545e-04 eta 0:02:15
epoch [150/200] batch [1/3] time 1.057 (1.057) data 0.261 (0.261) loss 0.2681 (0.2681) acc 93.7500 (93.7500) lr 3.1545e-04 eta 0:02:40
epoch [150/200] batch [2/3] time 0.798 (0.927) data 0.000 (0.131) loss 0.0569 (0.1625) acc 100.0000 (96.8750) lr 3.1545e-04 eta 0:02:20
epoch [150/200] batch [3/3] time 0.796 (0.884) data 0.000 (0.087) loss 0.1216 (0.1489) acc 96.8750 (96.8750) lr 3.0409e-04 eta 0:02:12
epoch [151/200] batch [1/3] time 1.063 (1.063) data 0.267 (0.267) loss 0.6069 (0.6069) acc 84.3750 (84.3750) lr 3.0409e-04 eta 0:02:38
epoch [151/200] batch [2/3] time 0.797 (0.930) data 0.000 (0.133) loss 0.5190 (0.5630) acc 87.5000 (85.9375) lr 3.0409e-04 eta 0:02:17
epoch [151/200] batch [3/3] time 0.796 (0.885) data 0.000 (0.089) loss 0.3591 (0.4950) acc 90.6250 (87.5000) lr 2.9289e-04 eta 0:02:10
epoch [152/200] batch [1/3] time 1.058 (1.058) data 0.264 (0.264) loss 0.1892 (0.1892) acc 96.8750 (96.8750) lr 2.9289e-04 eta 0:02:34
epoch [152/200] batch [2/3] time 0.797 (0.927) data 0.000 (0.132) loss 0.3931 (0.2911) acc 87.5000 (92.1875) lr 2.9289e-04 eta 0:02:14
epoch [152/200] batch [3/3] time 0.797 (0.884) data 0.000 (0.088) loss 0.3032 (0.2952) acc 90.6250 (91.6667) lr 2.8187e-04 eta 0:02:07
epoch [153/200] batch [1/3] time 1.067 (1.067) data 0.272 (0.272) loss 0.5010 (0.5010) acc 84.3750 (84.3750) lr 2.8187e-04 eta 0:02:32
epoch [153/200] batch [2/3] time 0.795 (0.931) data 0.000 (0.136) loss 0.5366 (0.5188) acc 87.5000 (85.9375) lr 2.8187e-04 eta 0:02:12
epoch [153/200] batch [3/3] time 0.797 (0.887) data 0.000 (0.091) loss 0.5723 (0.5366) acc 84.3750 (85.4167) lr 2.7103e-04 eta 0:02:05
epoch [154/200] batch [1/3] time 1.051 (1.051) data 0.260 (0.260) loss 0.3054 (0.3054) acc 90.6250 (90.6250) lr 2.7103e-04 eta 0:02:27
epoch [154/200] batch [2/3] time 0.796 (0.923) data 0.000 (0.130) loss 0.3655 (0.3354) acc 90.6250 (90.6250) lr 2.7103e-04 eta 0:02:08
epoch [154/200] batch [3/3] time 0.795 (0.881) data 0.000 (0.087) loss 0.4993 (0.3901) acc 90.6250 (90.6250) lr 2.6037e-04 eta 0:02:01
epoch [155/200] batch [1/3] time 1.068 (1.068) data 0.273 (0.273) loss 0.3533 (0.3533) acc 87.5000 (87.5000) lr 2.6037e-04 eta 0:02:26
epoch [155/200] batch [2/3] time 0.796 (0.932) data 0.000 (0.137) loss 0.0917 (0.2225) acc 96.8750 (92.1875) lr 2.6037e-04 eta 0:02:06
epoch [155/200] batch [3/3] time 0.796 (0.887) data 0.000 (0.091) loss 0.2981 (0.2477) acc 93.7500 (92.7083) lr 2.4989e-04 eta 0:01:59
epoch [156/200] batch [1/3] time 1.065 (1.065) data 0.269 (0.269) loss 0.1486 (0.1486) acc 93.7500 (93.7500) lr 2.4989e-04 eta 0:02:22
epoch [156/200] batch [2/3] time 0.797 (0.931) data 0.000 (0.135) loss 0.3604 (0.2545) acc 90.6250 (92.1875) lr 2.4989e-04 eta 0:02:03
epoch [156/200] batch [3/3] time 0.796 (0.886) data 0.000 (0.090) loss 0.2386 (0.2492) acc 93.7500 (92.7083) lr 2.3959e-04 eta 0:01:56
epoch [157/200] batch [1/3] time 1.062 (1.062) data 0.271 (0.271) loss 0.2795 (0.2795) acc 93.7500 (93.7500) lr 2.3959e-04 eta 0:02:19
epoch [157/200] batch [2/3] time 0.797 (0.930) data 0.000 (0.135) loss 0.5205 (0.4000) acc 84.3750 (89.0625) lr 2.3959e-04 eta 0:02:00
epoch [157/200] batch [3/3] time 0.795 (0.885) data 0.000 (0.090) loss 0.2969 (0.3656) acc 87.5000 (88.5417) lr 2.2949e-04 eta 0:01:54
epoch [158/200] batch [1/3] time 1.063 (1.063) data 0.267 (0.267) loss 0.2312 (0.2312) acc 96.8750 (96.8750) lr 2.2949e-04 eta 0:02:16
epoch [158/200] batch [2/3] time 0.795 (0.929) data 0.000 (0.134) loss 0.6958 (0.4635) acc 81.2500 (89.0625) lr 2.2949e-04 eta 0:01:57
epoch [158/200] batch [3/3] time 0.795 (0.884) data 0.000 (0.089) loss 0.2083 (0.3784) acc 93.7500 (90.6250) lr 2.1957e-04 eta 0:01:51
epoch [159/200] batch [1/3] time 1.050 (1.050) data 0.258 (0.258) loss 0.3367 (0.3367) acc 90.6250 (90.6250) lr 2.1957e-04 eta 0:02:11
epoch [159/200] batch [2/3] time 0.795 (0.922) data 0.000 (0.129) loss 0.5312 (0.4340) acc 84.3750 (87.5000) lr 2.1957e-04 eta 0:01:54
epoch [159/200] batch [3/3] time 0.795 (0.880) data 0.000 (0.086) loss 0.1869 (0.3516) acc 96.8750 (90.6250) lr 2.0984e-04 eta 0:01:48
epoch [160/200] batch [1/3] time 1.058 (1.058) data 0.262 (0.262) loss 0.6763 (0.6763) acc 87.5000 (87.5000) lr 2.0984e-04 eta 0:02:09
epoch [160/200] batch [2/3] time 0.795 (0.927) data 0.000 (0.131) loss 0.3281 (0.5022) acc 93.7500 (90.6250) lr 2.0984e-04 eta 0:01:52
epoch [160/200] batch [3/3] time 0.795 (0.883) data 0.000 (0.087) loss 0.1440 (0.3828) acc 96.8750 (92.7083) lr 2.0032e-04 eta 0:01:45
epoch [161/200] batch [1/3] time 1.060 (1.060) data 0.267 (0.267) loss 0.2017 (0.2017) acc 93.7500 (93.7500) lr 2.0032e-04 eta 0:02:06
epoch [161/200] batch [2/3] time 0.794 (0.927) data 0.000 (0.133) loss 0.2135 (0.2076) acc 96.8750 (95.3125) lr 2.0032e-04 eta 0:01:49
epoch [161/200] batch [3/3] time 0.795 (0.883) data 0.000 (0.089) loss 0.3899 (0.2684) acc 90.6250 (93.7500) lr 1.9098e-04 eta 0:01:43
epoch [162/200] batch [1/3] time 1.074 (1.074) data 0.279 (0.279) loss 0.1388 (0.1388) acc 96.8750 (96.8750) lr 1.9098e-04 eta 0:02:04
epoch [162/200] batch [2/3] time 0.797 (0.936) data 0.000 (0.139) loss 0.1886 (0.1637) acc 96.8750 (96.8750) lr 1.9098e-04 eta 0:01:47
epoch [162/200] batch [3/3] time 0.797 (0.889) data 0.000 (0.093) loss 0.1501 (0.1592) acc 96.8750 (96.8750) lr 1.8185e-04 eta 0:01:41
epoch [163/200] batch [1/3] time 1.073 (1.073) data 0.277 (0.277) loss 0.5024 (0.5024) acc 87.5000 (87.5000) lr 1.8185e-04 eta 0:02:01
epoch [163/200] batch [2/3] time 0.796 (0.935) data 0.000 (0.139) loss 0.4917 (0.4971) acc 90.6250 (89.0625) lr 1.8185e-04 eta 0:01:44
epoch [163/200] batch [3/3] time 0.797 (0.889) data 0.000 (0.093) loss 0.3027 (0.4323) acc 90.6250 (89.5833) lr 1.7292e-04 eta 0:01:38
epoch [164/200] batch [1/3] time 1.064 (1.064) data 0.270 (0.270) loss 0.4941 (0.4941) acc 93.7500 (93.7500) lr 1.7292e-04 eta 0:01:57
epoch [164/200] batch [2/3] time 0.798 (0.931) data 0.000 (0.135) loss 0.1859 (0.3400) acc 96.8750 (95.3125) lr 1.7292e-04 eta 0:01:41
epoch [164/200] batch [3/3] time 0.797 (0.886) data 0.000 (0.090) loss 0.5659 (0.4153) acc 90.6250 (93.7500) lr 1.6419e-04 eta 0:01:35
epoch [165/200] batch [1/3] time 1.055 (1.055) data 0.261 (0.261) loss 0.1238 (0.1238) acc 96.8750 (96.8750) lr 1.6419e-04 eta 0:01:52
epoch [165/200] batch [2/3] time 0.795 (0.925) data 0.000 (0.131) loss 0.5044 (0.3141) acc 90.6250 (93.7500) lr 1.6419e-04 eta 0:01:38
epoch [165/200] batch [3/3] time 0.797 (0.882) data 0.000 (0.087) loss 0.2695 (0.2993) acc 90.6250 (92.7083) lr 1.5567e-04 eta 0:01:32
epoch [166/200] batch [1/3] time 1.053 (1.053) data 0.261 (0.261) loss 0.5474 (0.5474) acc 81.2500 (81.2500) lr 1.5567e-04 eta 0:01:49
epoch [166/200] batch [2/3] time 0.797 (0.925) data 0.000 (0.130) loss 0.3127 (0.4301) acc 90.6250 (85.9375) lr 1.5567e-04 eta 0:01:35
epoch [166/200] batch [3/3] time 0.795 (0.881) data 0.000 (0.087) loss 0.4851 (0.4484) acc 84.3750 (85.4167) lr 1.4736e-04 eta 0:01:29
epoch [167/200] batch [1/3] time 1.072 (1.072) data 0.277 (0.277) loss 0.1982 (0.1982) acc 93.7500 (93.7500) lr 1.4736e-04 eta 0:01:48
epoch [167/200] batch [2/3] time 0.794 (0.933) data 0.000 (0.139) loss 0.3523 (0.2753) acc 93.7500 (93.7500) lr 1.4736e-04 eta 0:01:33
epoch [167/200] batch [3/3] time 0.795 (0.887) data 0.000 (0.092) loss 0.2151 (0.2552) acc 96.8750 (94.7917) lr 1.3926e-04 eta 0:01:27
epoch [168/200] batch [1/3] time 1.069 (1.069) data 0.274 (0.274) loss 0.3738 (0.3738) acc 90.6250 (90.6250) lr 1.3926e-04 eta 0:01:44
epoch [168/200] batch [2/3] time 0.795 (0.932) data 0.000 (0.137) loss 0.1819 (0.2778) acc 96.8750 (93.7500) lr 1.3926e-04 eta 0:01:30
epoch [168/200] batch [3/3] time 0.797 (0.887) data 0.000 (0.091) loss 0.4644 (0.3400) acc 90.6250 (92.7083) lr 1.3137e-04 eta 0:01:25
epoch [169/200] batch [1/3] time 1.067 (1.067) data 0.273 (0.273) loss 0.3306 (0.3306) acc 90.6250 (90.6250) lr 1.3137e-04 eta 0:01:41
epoch [169/200] batch [2/3] time 0.794 (0.931) data 0.000 (0.137) loss 0.4482 (0.3894) acc 90.6250 (90.6250) lr 1.3137e-04 eta 0:01:27
epoch [169/200] batch [3/3] time 0.794 (0.885) data 0.000 (0.091) loss 0.2637 (0.3475) acc 93.7500 (91.6667) lr 1.2369e-04 eta 0:01:22
epoch [170/200] batch [1/3] time 1.056 (1.056) data 0.261 (0.261) loss 0.0836 (0.0836) acc 100.0000 (100.0000) lr 1.2369e-04 eta 0:01:37
epoch [170/200] batch [2/3] time 0.794 (0.925) data 0.000 (0.130) loss 0.1479 (0.1158) acc 100.0000 (100.0000) lr 1.2369e-04 eta 0:01:24
epoch [170/200] batch [3/3] time 0.795 (0.882) data 0.000 (0.087) loss 0.1847 (0.1387) acc 93.7500 (97.9167) lr 1.1623e-04 eta 0:01:19
epoch [171/200] batch [1/3] time 1.059 (1.059) data 0.267 (0.267) loss 0.1473 (0.1473) acc 96.8750 (96.8750) lr 1.1623e-04 eta 0:01:34
epoch [171/200] batch [2/3] time 0.797 (0.928) data 0.000 (0.134) loss 0.0489 (0.0981) acc 100.0000 (98.4375) lr 1.1623e-04 eta 0:01:21
epoch [171/200] batch [3/3] time 0.795 (0.884) data 0.000 (0.089) loss 0.3411 (0.1791) acc 93.7500 (96.8750) lr 1.0899e-04 eta 0:01:16
epoch [172/200] batch [1/3] time 1.072 (1.072) data 0.276 (0.276) loss 0.2795 (0.2795) acc 93.7500 (93.7500) lr 1.0899e-04 eta 0:01:32
epoch [172/200] batch [2/3] time 0.795 (0.933) data 0.000 (0.138) loss 0.1832 (0.2314) acc 96.8750 (95.3125) lr 1.0899e-04 eta 0:01:19
epoch [172/200] batch [3/3] time 0.795 (0.887) data 0.000 (0.092) loss 0.2196 (0.2275) acc 100.0000 (96.8750) lr 1.0197e-04 eta 0:01:14
epoch [173/200] batch [1/3] time 1.056 (1.056) data 0.262 (0.262) loss 0.7095 (0.7095) acc 84.3750 (84.3750) lr 1.0197e-04 eta 0:01:27
epoch [173/200] batch [2/3] time 0.798 (0.927) data 0.000 (0.131) loss 0.1881 (0.4488) acc 96.8750 (90.6250) lr 1.0197e-04 eta 0:01:15
epoch [173/200] batch [3/3] time 0.795 (0.883) data 0.000 (0.088) loss 0.2927 (0.3968) acc 93.7500 (91.6667) lr 9.5173e-05 eta 0:01:11
epoch [174/200] batch [1/3] time 1.059 (1.059) data 0.261 (0.261) loss 0.5601 (0.5601) acc 87.5000 (87.5000) lr 9.5173e-05 eta 0:01:24
epoch [174/200] batch [2/3] time 0.797 (0.928) data 0.000 (0.131) loss 0.4565 (0.5083) acc 90.6250 (89.0625) lr 9.5173e-05 eta 0:01:13
epoch [174/200] batch [3/3] time 0.797 (0.884) data 0.000 (0.087) loss 0.2131 (0.4099) acc 93.7500 (90.6250) lr 8.8597e-05 eta 0:01:08
epoch [175/200] batch [1/3] time 1.072 (1.072) data 0.276 (0.276) loss 0.1236 (0.1236) acc 100.0000 (100.0000) lr 8.8597e-05 eta 0:01:22
epoch [175/200] batch [2/3] time 0.797 (0.934) data 0.000 (0.138) loss 0.2849 (0.2043) acc 96.8750 (98.4375) lr 8.8597e-05 eta 0:01:11
epoch [175/200] batch [3/3] time 0.797 (0.889) data 0.000 (0.092) loss 0.2739 (0.2275) acc 93.7500 (96.8750) lr 8.2245e-05 eta 0:01:06
epoch [176/200] batch [1/3] time 1.055 (1.055) data 0.259 (0.259) loss 0.5210 (0.5210) acc 90.6250 (90.6250) lr 8.2245e-05 eta 0:01:18
epoch [176/200] batch [2/3] time 0.797 (0.926) data 0.000 (0.130) loss 0.4011 (0.4611) acc 96.8750 (93.7500) lr 8.2245e-05 eta 0:01:07
epoch [176/200] batch [3/3] time 0.797 (0.883) data 0.000 (0.087) loss 0.5537 (0.4919) acc 81.2500 (89.5833) lr 7.6120e-05 eta 0:01:03
epoch [177/200] batch [1/3] time 1.064 (1.064) data 0.269 (0.269) loss 0.1700 (0.1700) acc 93.7500 (93.7500) lr 7.6120e-05 eta 0:01:15
epoch [177/200] batch [2/3] time 0.795 (0.929) data 0.000 (0.134) loss 0.2268 (0.1984) acc 93.7500 (93.7500) lr 7.6120e-05 eta 0:01:05
epoch [177/200] batch [3/3] time 0.797 (0.885) data 0.000 (0.090) loss 0.2028 (0.1999) acc 100.0000 (95.8333) lr 7.0224e-05 eta 0:01:01
epoch [178/200] batch [1/3] time 1.051 (1.051) data 0.259 (0.259) loss 0.2109 (0.2109) acc 90.6250 (90.6250) lr 7.0224e-05 eta 0:01:11
epoch [178/200] batch [2/3] time 0.796 (0.923) data 0.000 (0.130) loss 0.1923 (0.2016) acc 93.7500 (92.1875) lr 7.0224e-05 eta 0:01:01
epoch [178/200] batch [3/3] time 0.795 (0.880) data 0.000 (0.086) loss 0.4529 (0.2854) acc 87.5000 (90.6250) lr 6.4556e-05 eta 0:00:58
epoch [179/200] batch [1/3] time 1.056 (1.056) data 0.260 (0.260) loss 0.3704 (0.3704) acc 90.6250 (90.6250) lr 6.4556e-05 eta 0:01:08
epoch [179/200] batch [2/3] time 0.797 (0.927) data 0.000 (0.130) loss 0.2820 (0.3262) acc 93.7500 (92.1875) lr 6.4556e-05 eta 0:00:59
epoch [179/200] batch [3/3] time 0.796 (0.883) data 0.000 (0.087) loss 0.3210 (0.3245) acc 93.7500 (92.7083) lr 5.9119e-05 eta 0:00:55
epoch [180/200] batch [1/3] time 1.063 (1.063) data 0.267 (0.267) loss 0.3772 (0.3772) acc 93.7500 (93.7500) lr 5.9119e-05 eta 0:01:05
epoch [180/200] batch [2/3] time 0.794 (0.929) data 0.000 (0.134) loss 0.3228 (0.3500) acc 96.8750 (95.3125) lr 5.9119e-05 eta 0:00:56
epoch [180/200] batch [3/3] time 0.797 (0.885) data 0.000 (0.089) loss 0.1274 (0.2758) acc 100.0000 (96.8750) lr 5.3915e-05 eta 0:00:53
epoch [181/200] batch [1/3] time 1.053 (1.053) data 0.258 (0.258) loss 0.3806 (0.3806) acc 93.7500 (93.7500) lr 5.3915e-05 eta 0:01:02
epoch [181/200] batch [2/3] time 0.797 (0.925) data 0.000 (0.129) loss 0.7402 (0.5604) acc 81.2500 (87.5000) lr 5.3915e-05 eta 0:00:53
epoch [181/200] batch [3/3] time 0.798 (0.883) data 0.000 (0.086) loss 0.3313 (0.4840) acc 96.8750 (90.6250) lr 4.8943e-05 eta 0:00:50
epoch [182/200] batch [1/3] time 1.056 (1.056) data 0.260 (0.260) loss 0.1036 (0.1036) acc 96.8750 (96.8750) lr 4.8943e-05 eta 0:00:59
epoch [182/200] batch [2/3] time 0.797 (0.926) data 0.000 (0.130) loss 0.3357 (0.2197) acc 93.7500 (95.3125) lr 4.8943e-05 eta 0:00:50
epoch [182/200] batch [3/3] time 0.797 (0.883) data 0.000 (0.087) loss 0.4199 (0.2864) acc 90.6250 (93.7500) lr 4.4207e-05 eta 0:00:47
epoch [183/200] batch [1/3] time 1.050 (1.050) data 0.258 (0.258) loss 0.3276 (0.3276) acc 90.6250 (90.6250) lr 4.4207e-05 eta 0:00:55
epoch [183/200] batch [2/3] time 0.796 (0.923) data 0.000 (0.129) loss 0.6465 (0.4871) acc 81.2500 (85.9375) lr 4.4207e-05 eta 0:00:47
epoch [183/200] batch [3/3] time 0.797 (0.881) data 0.000 (0.086) loss 0.0868 (0.3536) acc 96.8750 (89.5833) lr 3.9706e-05 eta 0:00:44
epoch [184/200] batch [1/3] time 1.062 (1.062) data 0.268 (0.268) loss 0.2336 (0.2336) acc 93.7500 (93.7500) lr 3.9706e-05 eta 0:00:53
epoch [184/200] batch [2/3] time 0.796 (0.929) data 0.000 (0.134) loss 0.4333 (0.3335) acc 87.5000 (90.6250) lr 3.9706e-05 eta 0:00:45
epoch [184/200] batch [3/3] time 0.797 (0.885) data 0.000 (0.089) loss 0.4111 (0.3594) acc 87.5000 (89.5833) lr 3.5443e-05 eta 0:00:42
epoch [185/200] batch [1/3] time 1.058 (1.058) data 0.260 (0.260) loss 0.3071 (0.3071) acc 93.7500 (93.7500) lr 3.5443e-05 eta 0:00:49
epoch [185/200] batch [2/3] time 0.798 (0.928) data 0.000 (0.130) loss 0.2256 (0.2664) acc 96.8750 (95.3125) lr 3.5443e-05 eta 0:00:42
epoch [185/200] batch [3/3] time 0.796 (0.884) data 0.000 (0.087) loss 0.0570 (0.1966) acc 100.0000 (96.8750) lr 3.1417e-05 eta 0:00:39
epoch [186/200] batch [1/3] time 1.054 (1.054) data 0.259 (0.259) loss 0.2974 (0.2974) acc 96.8750 (96.8750) lr 3.1417e-05 eta 0:00:46
epoch [186/200] batch [2/3] time 0.797 (0.926) data 0.000 (0.130) loss 0.5273 (0.4124) acc 81.2500 (89.0625) lr 3.1417e-05 eta 0:00:39
epoch [186/200] batch [3/3] time 0.798 (0.883) data 0.000 (0.086) loss 0.2786 (0.3678) acc 93.7500 (90.6250) lr 2.7630e-05 eta 0:00:37
epoch [187/200] batch [1/3] time 1.060 (1.060) data 0.266 (0.266) loss 0.4438 (0.4438) acc 87.5000 (87.5000) lr 2.7630e-05 eta 0:00:43
epoch [187/200] batch [2/3] time 0.795 (0.927) data 0.000 (0.133) loss 0.6040 (0.5239) acc 87.5000 (87.5000) lr 2.7630e-05 eta 0:00:37
epoch [187/200] batch [3/3] time 0.797 (0.884) data 0.000 (0.089) loss 0.2922 (0.4467) acc 96.8750 (90.6250) lr 2.4083e-05 eta 0:00:34
epoch [188/200] batch [1/3] time 1.068 (1.068) data 0.277 (0.277) loss 0.3970 (0.3970) acc 90.6250 (90.6250) lr 2.4083e-05 eta 0:00:40
epoch [188/200] batch [2/3] time 0.795 (0.932) data 0.000 (0.139) loss 0.2194 (0.3082) acc 93.7500 (92.1875) lr 2.4083e-05 eta 0:00:34
epoch [188/200] batch [3/3] time 0.795 (0.886) data 0.000 (0.092) loss 0.5449 (0.3871) acc 84.3750 (89.5833) lr 2.0777e-05 eta 0:00:31
epoch [189/200] batch [1/3] time 1.061 (1.061) data 0.267 (0.267) loss 0.3530 (0.3530) acc 87.5000 (87.5000) lr 2.0777e-05 eta 0:00:37
epoch [189/200] batch [2/3] time 0.796 (0.929) data 0.000 (0.134) loss 0.3188 (0.3359) acc 93.7500 (90.6250) lr 2.0777e-05 eta 0:00:31
epoch [189/200] batch [3/3] time 0.795 (0.884) data 0.000 (0.089) loss 0.4780 (0.3833) acc 93.7500 (91.6667) lr 1.7713e-05 eta 0:00:29
epoch [190/200] batch [1/3] time 1.054 (1.054) data 0.260 (0.260) loss 0.4258 (0.4258) acc 84.3750 (84.3750) lr 1.7713e-05 eta 0:00:33
epoch [190/200] batch [2/3] time 0.795 (0.924) data 0.000 (0.130) loss 0.2893 (0.3575) acc 96.8750 (90.6250) lr 1.7713e-05 eta 0:00:28
epoch [190/200] batch [3/3] time 0.797 (0.882) data 0.000 (0.087) loss 0.3550 (0.3567) acc 90.6250 (90.6250) lr 1.4891e-05 eta 0:00:26
epoch [191/200] batch [1/3] time 1.054 (1.054) data 0.258 (0.258) loss 0.1722 (0.1722) acc 93.7500 (93.7500) lr 1.4891e-05 eta 0:00:30
epoch [191/200] batch [2/3] time 0.795 (0.924) data 0.000 (0.129) loss 0.2180 (0.1951) acc 96.8750 (95.3125) lr 1.4891e-05 eta 0:00:25
epoch [191/200] batch [3/3] time 0.795 (0.881) data 0.000 (0.086) loss 0.3411 (0.2438) acc 93.7500 (94.7917) lr 1.2312e-05 eta 0:00:23
epoch [192/200] batch [1/3] time 1.056 (1.056) data 0.261 (0.261) loss 0.3660 (0.3660) acc 93.7500 (93.7500) lr 1.2312e-05 eta 0:00:27
epoch [192/200] batch [2/3] time 0.795 (0.925) data 0.000 (0.131) loss 0.2742 (0.3201) acc 93.7500 (93.7500) lr 1.2312e-05 eta 0:00:23
epoch [192/200] batch [3/3] time 0.795 (0.882) data 0.000 (0.087) loss 0.6519 (0.4307) acc 87.5000 (91.6667) lr 9.9763e-06 eta 0:00:21
epoch [193/200] batch [1/3] time 1.062 (1.062) data 0.268 (0.268) loss 0.3303 (0.3303) acc 96.8750 (96.8750) lr 9.9763e-06 eta 0:00:24
epoch [193/200] batch [2/3] time 0.797 (0.930) data 0.000 (0.134) loss 0.3057 (0.3180) acc 90.6250 (93.7500) lr 9.9763e-06 eta 0:00:20
epoch [193/200] batch [3/3] time 0.796 (0.885) data 0.000 (0.089) loss 0.2581 (0.2980) acc 93.7500 (93.7500) lr 7.8853e-06 eta 0:00:18
epoch [194/200] batch [1/3] time 1.054 (1.054) data 0.257 (0.257) loss 0.2717 (0.2717) acc 93.7500 (93.7500) lr 7.8853e-06 eta 0:00:21
epoch [194/200] batch [2/3] time 0.797 (0.926) data 0.000 (0.129) loss 0.7021 (0.4869) acc 81.2500 (87.5000) lr 7.8853e-06 eta 0:00:17
epoch [194/200] batch [3/3] time 0.797 (0.883) data 0.000 (0.086) loss 0.6841 (0.5527) acc 87.5000 (87.5000) lr 6.0390e-06 eta 0:00:15
epoch [195/200] batch [1/3] time 1.053 (1.053) data 0.260 (0.260) loss 0.3257 (0.3257) acc 90.6250 (90.6250) lr 6.0390e-06 eta 0:00:17
epoch [195/200] batch [2/3] time 0.797 (0.925) data 0.000 (0.130) loss 0.1790 (0.2523) acc 96.8750 (93.7500) lr 6.0390e-06 eta 0:00:14
epoch [195/200] batch [3/3] time 0.795 (0.882) data 0.000 (0.087) loss 0.2130 (0.2392) acc 96.8750 (94.7917) lr 4.4380e-06 eta 0:00:13
epoch [196/200] batch [1/3] time 1.063 (1.063) data 0.268 (0.268) loss 0.1685 (0.1685) acc 96.8750 (96.8750) lr 4.4380e-06 eta 0:00:14
epoch [196/200] batch [2/3] time 0.797 (0.930) data 0.000 (0.134) loss 0.2454 (0.2069) acc 96.8750 (96.8750) lr 4.4380e-06 eta 0:00:12
epoch [196/200] batch [3/3] time 0.797 (0.886) data 0.000 (0.089) loss 0.5752 (0.3297) acc 90.6250 (94.7917) lr 3.0827e-06 eta 0:00:10
epoch [197/200] batch [1/3] time 1.071 (1.071) data 0.276 (0.276) loss 0.3704 (0.3704) acc 90.6250 (90.6250) lr 3.0827e-06 eta 0:00:11
epoch [197/200] batch [2/3] time 0.796 (0.934) data 0.000 (0.138) loss 0.3079 (0.3391) acc 90.6250 (90.6250) lr 3.0827e-06 eta 0:00:09
epoch [197/200] batch [3/3] time 0.797 (0.888) data 0.000 (0.092) loss 0.3367 (0.3383) acc 90.6250 (90.6250) lr 1.9733e-06 eta 0:00:07
epoch [198/200] batch [1/3] time 1.057 (1.057) data 0.261 (0.261) loss 0.2849 (0.2849) acc 90.6250 (90.6250) lr 1.9733e-06 eta 0:00:08
epoch [198/200] batch [2/3] time 0.797 (0.927) data 0.000 (0.131) loss 0.3843 (0.3346) acc 87.5000 (89.0625) lr 1.9733e-06 eta 0:00:06
epoch [198/200] batch [3/3] time 0.797 (0.884) data 0.000 (0.087) loss 0.3167 (0.3286) acc 93.7500 (90.6250) lr 1.1101e-06 eta 0:00:05
epoch [199/200] batch [1/3] time 1.056 (1.056) data 0.260 (0.260) loss 0.1737 (0.1737) acc 96.8750 (96.8750) lr 1.1101e-06 eta 0:00:05
epoch [199/200] batch [2/3] time 0.795 (0.925) data 0.000 (0.130) loss 0.2161 (0.1949) acc 93.7500 (95.3125) lr 1.1101e-06 eta 0:00:03
epoch [199/200] batch [3/3] time 0.797 (0.882) data 0.000 (0.087) loss 0.1553 (0.1817) acc 96.8750 (95.8333) lr 4.9344e-07 eta 0:00:02
epoch [200/200] batch [1/3] time 1.059 (1.059) data 0.268 (0.268) loss 0.1935 (0.1935) acc 93.7500 (93.7500) lr 4.9344e-07 eta 0:00:02
epoch [200/200] batch [2/3] time 0.797 (0.928) data 0.000 (0.134) loss 0.5459 (0.3697) acc 87.5000 (90.6250) lr 4.9344e-07 eta 0:00:00
epoch [200/200] batch [3/3] time 0.795 (0.884) data 0.000 (0.089) loss 0.3145 (0.3513) acc 90.6250 (90.6250) lr 1.2337e-07 eta 0:00:00
Checkpoint saved to output/Caltech/1/2/prompt_learner/model.pth.tar-200
Finish training
Deploy the last-epoch model
Evaluate on the *test* set
=> result
* total: 2,465
* correct: 2,137
* accuracy: 86.7%
* error: 13.3%
* macro_f1: 80.1%
Elapsed: 0:09:29
args2: backbone=, config_file=configs/trainers/CoOp/rn101.yaml, dataset_config_file=configs/datasets/caltech101.yaml, eval_only=False, head=, load_epoch=None, model_dir=, no_train=False,  opts: ['TRAINER.COOP.N_CTX', '16', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '1'], output_dir=output/Caltech/1/2/3, resume=, root=/home/brandnerkasper/Uni/MP/MP_CustomCoOp/data, seed=3, source_domains=None, target_domains=None, trainer=CoOp, transforms=None
Setting fixed seed: 3
***************
** Arguments **
***************
config_file: configs/trainers/CoOp/rn101.yaml
csc: False
ctp: end
dataset_config_file: configs/datasets/caltech101.yaml
n_ctx: 16
opts: ['TRAINER.COOP.N_CTX', '16', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '1']
output_dir: output/Caltech/1/2/3
root: /home/brandnerkasper/Uni/MP/MP_CustomCoOp/data
seed: 3
shots: 1
trainer: CoOp
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 4
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: Caltech101
  NUM_LABELED: -1
  NUM_SHOTS: 1
  ROOT: /home/brandnerkasper/Uni/MP/MP_CustomCoOp/data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN101
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 200
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/Caltech/1/2/3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: 
    N_CTX: 16
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: 
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: CoOp
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 2.0.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 22.04.3 LTS (x86_64)
GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.35

Python version: 3.10.12 (main, Jul  5 2023, 18:54:27) [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-6.2.0-32-generic-x86_64-with-glibc2.35
Is CUDA available: True
CUDA runtime version: 11.5.119
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA GeForce GTX 970
Nvidia driver version: 525.125.06
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Architecture:                       x86_64
CPU op-mode(s):                     32-bit, 64-bit
Address sizes:                      39 bits physical, 48 bits virtual
Byte Order:                         Little Endian
CPU(s):                             4
On-line CPU(s) list:                0-3
Vendor ID:                          GenuineIntel
Model name:                         Intel(R) Xeon(R) CPU E3-1225 v3 @ 3.20GHz
CPU family:                         6
Model:                              60
Thread(s) per core:                 1
Core(s) per socket:                 4
Socket(s):                          1
Stepping:                           3
CPU max MHz:                        3600,0000
CPU min MHz:                        800,0000
BogoMIPS:                           6397.95
Flags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm cpuid_fault invpcid_single pti ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid xsaveopt dtherm ida arat pln pts md_clear flush_l1d
Virtualization:                     VT-x
L1d cache:                          128 KiB (4 instances)
L1i cache:                          128 KiB (4 instances)
L2 cache:                           1 MiB (4 instances)
L3 cache:                           8 MiB (1 instance)
NUMA node(s):                       1
NUMA node0 CPU(s):                  0-3
Vulnerability Gather data sampling: Not affected
Vulnerability Itlb multihit:        KVM: Mitigation: VMX disabled
Vulnerability L1tf:                 Mitigation; PTE Inversion; VMX conditional cache flushes, SMT disabled
Vulnerability Mds:                  Mitigation; Clear CPU buffers; SMT disabled
Vulnerability Meltdown:             Mitigation; PTI
Vulnerability Mmio stale data:      Unknown: No mitigations
Vulnerability Retbleed:             Not affected
Vulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl
Vulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization
Vulnerability Spectre v2:           Mitigation; Retpolines, IBPB conditional, IBRS_FW, STIBP disabled, RSB filling, PBRSB-eIBRS Not affected
Vulnerability Srbds:                Mitigation; Microcode
Vulnerability Tsx async abort:      Not affected

Versions of relevant libraries:
[pip3] numpy==1.25.2
[pip3] open-clip-torch==2.20.0
[pip3] torch==2.0.1
[pip3] torchaudio==2.0.2
[pip3] torchvision==0.15.2
[conda] blas                      1.0                         mkl  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] mkl                       2023.1.0         h213fc3f_46343  
[conda] mkl-service               2.4.0           py310h5eee18b_1  
[conda] mkl_fft                   1.3.6           py310h1128e8f_1  
[conda] mkl_random                1.2.2           py310h1128e8f_1  
[conda] numpy                     1.25.2          py310h5f9d8c6_0  
[conda] numpy-base                1.25.2          py310hb5e798b_0  
[conda] open-clip-torch           2.20.0                   pypi_0    pypi
[conda] pytorch                   2.0.1           py3.10_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchaudio                2.0.2               py310_cu117    pytorch
[conda] torchtriton               2.0.0                     py310    pytorch
[conda] torchvision               0.15.2              py310_cu117    pytorch
        Pillow (9.4.0)

Loading trainer: CoOp
Loading dataset: Caltech101
Reading split from /home/brandnerkasper/Uni/MP/MP_CustomCoOp/data/caltech-101/split_zhou_Caltech101.json
Creating a 1-shot dataset
Creating a 1-shot dataset
Saving preprocessed few-shot data to /home/brandnerkasper/Uni/MP/MP_CustomCoOp/data/caltech-101/split_fewshot/shot_1-seed_3.pkl
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ----------
Dataset    Caltech101
# classes  100
# train_x  100
# val      100
# test     2,465
---------  ----------
Loading CLIP (backbone: RN101)
CLIP(
  (visual): ModifiedResNet(
    (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu1): ReLU(inplace=True)
    (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu2): ReLU(inplace=True)
    (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu3): ReLU(inplace=True)
    (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (layer1): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
        (downsample): Sequential(
          (-1): AvgPool2d(kernel_size=1, stride=1, padding=0)
          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
    )
    (layer2): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
        (downsample): Sequential(
          (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
    )
    (layer3): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
        (downsample): Sequential(
          (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
      (6): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
      (7): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
      (8): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
      (9): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
      (10): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
      (11): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
      (12): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
      (13): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
      (14): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
      (15): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
      (16): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
      (17): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
      (18): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
      (19): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
      (20): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
      (21): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
      (22): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
    )
    (layer4): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
        (downsample): Sequential(
          (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (avgpool): Identity()
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu3): ReLU(inplace=True)
      )
    )
    (attnpool): AttentionPool2d(
      (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
      (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
      (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
      (c_proj): Linear(in_features=2048, out_features=512, bias=True)
    )
  )
  (transformer): Transformer(
    (resblocks): Sequential(
      (0): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (6): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (7): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (8): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (9): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (10): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (11): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (token_embedding): Embedding(49408, 512)
  (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
Turning off gradients in both the image and the text encoder
Loading evaluator: Classification
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/Caltech/1/2/3/tensorboard)
epoch [1/200] batch [1/3] time 1.375 (1.375) data 0.284 (0.284) loss 2.9805 (2.9805) acc 53.1250 (53.1250) lr 1.0000e-05 eta 0:13:43
epoch [1/200] batch [2/3] time 1.027 (1.201) data 0.000 (0.142) loss 2.9043 (2.9424) acc 46.8750 (50.0000) lr 1.0000e-05 eta 0:11:58
epoch [1/200] batch [3/3] time 1.029 (1.144) data 0.000 (0.095) loss 3.0723 (2.9857) acc 37.5000 (45.8333) lr 2.0000e-03 eta 0:11:22
epoch [2/200] batch [1/3] time 1.297 (1.297) data 0.278 (0.278) loss 3.1641 (3.1641) acc 34.3750 (34.3750) lr 2.0000e-03 eta 0:12:53
epoch [2/200] batch [2/3] time 1.023 (1.160) data 0.000 (0.139) loss 1.4180 (2.2910) acc 68.7500 (51.5625) lr 2.0000e-03 eta 0:11:30
epoch [2/200] batch [3/3] time 1.027 (1.116) data 0.000 (0.093) loss 1.3184 (1.9668) acc 59.3750 (54.1667) lr 1.9999e-03 eta 0:11:02
epoch [3/200] batch [1/3] time 1.286 (1.286) data 0.263 (0.263) loss 1.4951 (1.4951) acc 56.2500 (56.2500) lr 1.9999e-03 eta 0:12:42
epoch [3/200] batch [2/3] time 1.021 (1.154) data 0.000 (0.131) loss 0.7842 (1.1396) acc 78.1250 (67.1875) lr 1.9999e-03 eta 0:11:23
epoch [3/200] batch [3/3] time 1.020 (1.109) data 0.000 (0.088) loss 0.8770 (1.0521) acc 68.7500 (67.7083) lr 1.9995e-03 eta 0:10:55
epoch [4/200] batch [1/3] time 1.304 (1.304) data 0.275 (0.275) loss 0.7749 (0.7749) acc 78.1250 (78.1250) lr 1.9995e-03 eta 0:12:49
epoch [4/200] batch [2/3] time 1.017 (1.161) data 0.000 (0.137) loss 0.8784 (0.8267) acc 71.8750 (75.0000) lr 1.9995e-03 eta 0:11:23
epoch [4/200] batch [3/3] time 1.030 (1.117) data 0.000 (0.092) loss 1.1719 (0.9417) acc 65.6250 (71.8750) lr 1.9989e-03 eta 0:10:56
epoch [5/200] batch [1/3] time 1.308 (1.308) data 0.281 (0.281) loss 1.1357 (1.1357) acc 65.6250 (65.6250) lr 1.9989e-03 eta 0:12:47
epoch [5/200] batch [2/3] time 1.031 (1.169) data 0.000 (0.141) loss 0.9028 (1.0193) acc 75.0000 (70.3125) lr 1.9989e-03 eta 0:11:25
epoch [5/200] batch [3/3] time 1.026 (1.122) data 0.000 (0.094) loss 1.0684 (1.0356) acc 75.0000 (71.8750) lr 1.9980e-03 eta 0:10:56
epoch [6/200] batch [1/3] time 1.308 (1.308) data 0.278 (0.278) loss 0.9619 (0.9619) acc 65.6250 (65.6250) lr 1.9980e-03 eta 0:12:43
epoch [6/200] batch [2/3] time 1.031 (1.169) data 0.000 (0.139) loss 0.8521 (0.9070) acc 81.2500 (73.4375) lr 1.9980e-03 eta 0:11:21
epoch [6/200] batch [3/3] time 1.032 (1.123) data 0.000 (0.093) loss 1.0312 (0.9484) acc 71.8750 (72.9167) lr 1.9969e-03 eta 0:10:53
epoch [7/200] batch [1/3] time 1.305 (1.305) data 0.278 (0.278) loss 0.6299 (0.6299) acc 84.3750 (84.3750) lr 1.9969e-03 eta 0:12:37
epoch [7/200] batch [2/3] time 1.029 (1.167) data 0.000 (0.139) loss 0.9595 (0.7947) acc 68.7500 (76.5625) lr 1.9969e-03 eta 0:11:16
epoch [7/200] batch [3/3] time 1.033 (1.122) data 0.000 (0.093) loss 0.8130 (0.8008) acc 78.1250 (77.0833) lr 1.9956e-03 eta 0:10:49
epoch [8/200] batch [1/3] time 1.294 (1.294) data 0.272 (0.272) loss 0.8550 (0.8550) acc 71.8750 (71.8750) lr 1.9956e-03 eta 0:12:27
epoch [8/200] batch [2/3] time 1.028 (1.161) data 0.000 (0.136) loss 0.8525 (0.8538) acc 71.8750 (71.8750) lr 1.9956e-03 eta 0:11:09
epoch [8/200] batch [3/3] time 1.029 (1.117) data 0.000 (0.091) loss 0.4333 (0.7136) acc 93.7500 (79.1667) lr 1.9940e-03 eta 0:10:43
epoch [9/200] batch [1/3] time 1.288 (1.288) data 0.264 (0.264) loss 0.5942 (0.5942) acc 78.1250 (78.1250) lr 1.9940e-03 eta 0:12:20
epoch [9/200] batch [2/3] time 1.022 (1.155) data 0.000 (0.132) loss 0.3079 (0.4510) acc 87.5000 (82.8125) lr 1.9940e-03 eta 0:11:02
epoch [9/200] batch [3/3] time 1.028 (1.112) data 0.000 (0.088) loss 0.7515 (0.5512) acc 81.2500 (82.2917) lr 1.9921e-03 eta 0:10:37
epoch [10/200] batch [1/3] time 1.292 (1.292) data 0.263 (0.263) loss 1.0508 (1.0508) acc 75.0000 (75.0000) lr 1.9921e-03 eta 0:12:19
epoch [10/200] batch [2/3] time 1.016 (1.154) data 0.000 (0.132) loss 0.6943 (0.8726) acc 75.0000 (75.0000) lr 1.9921e-03 eta 0:10:59
epoch [10/200] batch [3/3] time 1.024 (1.111) data 0.000 (0.088) loss 1.3359 (1.0270) acc 71.8750 (73.9583) lr 1.9900e-03 eta 0:10:33
epoch [11/200] batch [1/3] time 1.304 (1.304) data 0.275 (0.275) loss 0.7871 (0.7871) acc 75.0000 (75.0000) lr 1.9900e-03 eta 0:12:21
epoch [11/200] batch [2/3] time 1.027 (1.166) data 0.000 (0.137) loss 0.7524 (0.7698) acc 75.0000 (75.0000) lr 1.9900e-03 eta 0:11:02
epoch [11/200] batch [3/3] time 1.019 (1.117) data 0.000 (0.092) loss 0.7354 (0.7583) acc 81.2500 (77.0833) lr 1.9877e-03 eta 0:10:33
epoch [12/200] batch [1/3] time 1.292 (1.292) data 0.268 (0.268) loss 0.9854 (0.9854) acc 78.1250 (78.1250) lr 1.9877e-03 eta 0:12:11
epoch [12/200] batch [2/3] time 1.031 (1.162) data 0.000 (0.134) loss 0.5835 (0.7844) acc 87.5000 (82.8125) lr 1.9877e-03 eta 0:10:56
epoch [12/200] batch [3/3] time 1.029 (1.117) data 0.000 (0.089) loss 0.8237 (0.7975) acc 78.1250 (81.2500) lr 1.9851e-03 eta 0:10:30
epoch [13/200] batch [1/3] time 1.283 (1.283) data 0.263 (0.263) loss 0.7627 (0.7627) acc 75.0000 (75.0000) lr 1.9851e-03 eta 0:12:02
epoch [13/200] batch [2/3] time 1.026 (1.155) data 0.000 (0.132) loss 0.4697 (0.6162) acc 84.3750 (79.6875) lr 1.9851e-03 eta 0:10:48
epoch [13/200] batch [3/3] time 1.024 (1.111) data 0.000 (0.088) loss 0.5220 (0.5848) acc 84.3750 (81.2500) lr 1.9823e-03 eta 0:10:23
epoch [14/200] batch [1/3] time 1.288 (1.288) data 0.263 (0.263) loss 0.9077 (0.9077) acc 78.1250 (78.1250) lr 1.9823e-03 eta 0:12:01
epoch [14/200] batch [2/3] time 1.023 (1.156) data 0.000 (0.131) loss 0.9097 (0.9087) acc 75.0000 (76.5625) lr 1.9823e-03 eta 0:10:46
epoch [14/200] batch [3/3] time 1.026 (1.113) data 0.000 (0.088) loss 0.6548 (0.8241) acc 81.2500 (78.1250) lr 1.9792e-03 eta 0:10:20
epoch [15/200] batch [1/3] time 1.288 (1.288) data 0.262 (0.262) loss 1.0752 (1.0752) acc 71.8750 (71.8750) lr 1.9792e-03 eta 0:11:57
epoch [15/200] batch [2/3] time 1.019 (1.153) data 0.000 (0.131) loss 0.5991 (0.8372) acc 90.6250 (81.2500) lr 1.9792e-03 eta 0:10:41
epoch [15/200] batch [3/3] time 1.023 (1.110) data 0.000 (0.087) loss 0.5869 (0.7537) acc 81.2500 (81.2500) lr 1.9759e-03 eta 0:10:15
epoch [16/200] batch [1/3] time 1.302 (1.302) data 0.271 (0.271) loss 1.3477 (1.3477) acc 59.3750 (59.3750) lr 1.9759e-03 eta 0:12:01
epoch [16/200] batch [2/3] time 1.032 (1.167) data 0.000 (0.135) loss 0.8979 (1.1228) acc 71.8750 (65.6250) lr 1.9759e-03 eta 0:10:45
epoch [16/200] batch [3/3] time 1.026 (1.120) data 0.000 (0.090) loss 0.5068 (0.9175) acc 84.3750 (71.8750) lr 1.9724e-03 eta 0:10:18
epoch [17/200] batch [1/3] time 1.294 (1.294) data 0.268 (0.268) loss 0.6304 (0.6304) acc 87.5000 (87.5000) lr 1.9724e-03 eta 0:11:53
epoch [17/200] batch [2/3] time 1.029 (1.162) data 0.000 (0.134) loss 0.7446 (0.6875) acc 81.2500 (84.3750) lr 1.9724e-03 eta 0:10:38
epoch [17/200] batch [3/3] time 1.030 (1.118) data 0.000 (0.090) loss 0.8784 (0.7511) acc 78.1250 (82.2917) lr 1.9686e-03 eta 0:10:13
epoch [18/200] batch [1/3] time 1.303 (1.303) data 0.282 (0.282) loss 0.7344 (0.7344) acc 84.3750 (84.3750) lr 1.9686e-03 eta 0:11:54
epoch [18/200] batch [2/3] time 1.030 (1.167) data 0.000 (0.141) loss 0.3699 (0.5521) acc 84.3750 (84.3750) lr 1.9686e-03 eta 0:10:38
epoch [18/200] batch [3/3] time 1.026 (1.120) data 0.000 (0.094) loss 0.6191 (0.5745) acc 84.3750 (84.3750) lr 1.9646e-03 eta 0:10:11
epoch [19/200] batch [1/3] time 1.293 (1.293) data 0.265 (0.265) loss 0.5181 (0.5181) acc 84.3750 (84.3750) lr 1.9646e-03 eta 0:11:44
epoch [19/200] batch [2/3] time 1.028 (1.160) data 0.000 (0.133) loss 0.3809 (0.4495) acc 93.7500 (89.0625) lr 1.9646e-03 eta 0:10:31
epoch [19/200] batch [3/3] time 1.029 (1.117) data 0.000 (0.089) loss 0.5566 (0.4852) acc 84.3750 (87.5000) lr 1.9603e-03 eta 0:10:06
epoch [20/200] batch [1/3] time 1.305 (1.305) data 0.279 (0.279) loss 0.6440 (0.6440) acc 81.2500 (81.2500) lr 1.9603e-03 eta 0:11:47
epoch [20/200] batch [2/3] time 1.031 (1.168) data 0.000 (0.139) loss 0.4482 (0.5461) acc 84.3750 (82.8125) lr 1.9603e-03 eta 0:10:31
epoch [20/200] batch [3/3] time 1.022 (1.119) data 0.000 (0.093) loss 0.9561 (0.6828) acc 68.7500 (78.1250) lr 1.9558e-03 eta 0:10:04
epoch [21/200] batch [1/3] time 1.286 (1.286) data 0.262 (0.262) loss 0.5635 (0.5635) acc 87.5000 (87.5000) lr 1.9558e-03 eta 0:11:33
epoch [21/200] batch [2/3] time 1.024 (1.155) data 0.000 (0.131) loss 0.6943 (0.6289) acc 87.5000 (87.5000) lr 1.9558e-03 eta 0:10:21
epoch [21/200] batch [3/3] time 1.025 (1.112) data 0.000 (0.088) loss 0.3564 (0.5381) acc 93.7500 (89.5833) lr 1.9511e-03 eta 0:09:57
epoch [22/200] batch [1/3] time 1.306 (1.306) data 0.278 (0.278) loss 0.5635 (0.5635) acc 87.5000 (87.5000) lr 1.9511e-03 eta 0:11:39
epoch [22/200] batch [2/3] time 1.027 (1.166) data 0.000 (0.139) loss 0.3330 (0.4482) acc 93.7500 (90.6250) lr 1.9511e-03 eta 0:10:23
epoch [22/200] batch [3/3] time 1.027 (1.120) data 0.000 (0.093) loss 0.5791 (0.4919) acc 84.3750 (88.5417) lr 1.9461e-03 eta 0:09:57
epoch [23/200] batch [1/3] time 1.291 (1.291) data 0.270 (0.270) loss 0.7456 (0.7456) acc 75.0000 (75.0000) lr 1.9461e-03 eta 0:11:28
epoch [23/200] batch [2/3] time 1.027 (1.159) data 0.000 (0.135) loss 0.7476 (0.7466) acc 84.3750 (79.6875) lr 1.9461e-03 eta 0:10:16
epoch [23/200] batch [3/3] time 1.025 (1.114) data 0.000 (0.090) loss 0.8237 (0.7723) acc 81.2500 (80.2083) lr 1.9409e-03 eta 0:09:51
epoch [24/200] batch [1/3] time 1.289 (1.289) data 0.265 (0.265) loss 0.3318 (0.3318) acc 90.6250 (90.6250) lr 1.9409e-03 eta 0:11:23
epoch [24/200] batch [2/3] time 1.029 (1.159) data 0.000 (0.133) loss 0.7031 (0.5175) acc 78.1250 (84.3750) lr 1.9409e-03 eta 0:10:13
epoch [24/200] batch [3/3] time 1.033 (1.117) data 0.000 (0.088) loss 0.9185 (0.6511) acc 71.8750 (80.2083) lr 1.9354e-03 eta 0:09:49
epoch [25/200] batch [1/3] time 1.304 (1.304) data 0.279 (0.279) loss 0.6265 (0.6265) acc 84.3750 (84.3750) lr 1.9354e-03 eta 0:11:27
epoch [25/200] batch [2/3] time 1.029 (1.166) data 0.000 (0.139) loss 0.3198 (0.4731) acc 90.6250 (87.5000) lr 1.9354e-03 eta 0:10:13
epoch [25/200] batch [3/3] time 1.026 (1.120) data 0.000 (0.093) loss 0.8672 (0.6045) acc 71.8750 (82.2917) lr 1.9298e-03 eta 0:09:47
epoch [26/200] batch [1/3] time 1.301 (1.301) data 0.275 (0.275) loss 0.7012 (0.7012) acc 81.2500 (81.2500) lr 1.9298e-03 eta 0:11:21
epoch [26/200] batch [2/3] time 1.032 (1.167) data 0.000 (0.138) loss 0.7095 (0.7053) acc 84.3750 (82.8125) lr 1.9298e-03 eta 0:10:10
epoch [26/200] batch [3/3] time 1.029 (1.121) data 0.000 (0.092) loss 0.9453 (0.7853) acc 78.1250 (81.2500) lr 1.9239e-03 eta 0:09:44
epoch [27/200] batch [1/3] time 1.310 (1.310) data 0.280 (0.280) loss 0.3562 (0.3562) acc 90.6250 (90.6250) lr 1.9239e-03 eta 0:11:22
epoch [27/200] batch [2/3] time 1.028 (1.169) data 0.000 (0.140) loss 0.9067 (0.6315) acc 68.7500 (79.6875) lr 1.9239e-03 eta 0:10:07
epoch [27/200] batch [3/3] time 1.024 (1.120) data 0.000 (0.093) loss 0.5649 (0.6093) acc 84.3750 (81.2500) lr 1.9178e-03 eta 0:09:41
epoch [28/200] batch [1/3] time 1.299 (1.299) data 0.271 (0.271) loss 0.3645 (0.3645) acc 93.7500 (93.7500) lr 1.9178e-03 eta 0:11:12
epoch [28/200] batch [2/3] time 1.027 (1.163) data 0.000 (0.135) loss 0.7778 (0.5712) acc 78.1250 (85.9375) lr 1.9178e-03 eta 0:10:01
epoch [28/200] batch [3/3] time 1.023 (1.116) data 0.000 (0.090) loss 0.5396 (0.5606) acc 84.3750 (85.4167) lr 1.9114e-03 eta 0:09:36
epoch [29/200] batch [1/3] time 1.301 (1.301) data 0.272 (0.272) loss 0.4885 (0.4885) acc 84.3750 (84.3750) lr 1.9114e-03 eta 0:11:10
epoch [29/200] batch [2/3] time 1.034 (1.167) data 0.000 (0.136) loss 0.6348 (0.5616) acc 84.3750 (84.3750) lr 1.9114e-03 eta 0:10:00
epoch [29/200] batch [3/3] time 1.032 (1.122) data 0.000 (0.091) loss 0.5713 (0.5649) acc 87.5000 (85.4167) lr 1.9048e-03 eta 0:09:35
epoch [30/200] batch [1/3] time 1.302 (1.302) data 0.273 (0.273) loss 0.5020 (0.5020) acc 84.3750 (84.3750) lr 1.9048e-03 eta 0:11:06
epoch [30/200] batch [2/3] time 1.026 (1.164) data 0.000 (0.137) loss 0.5366 (0.5193) acc 84.3750 (84.3750) lr 1.9048e-03 eta 0:09:54
epoch [30/200] batch [3/3] time 1.028 (1.119) data 0.000 (0.091) loss 0.5229 (0.5205) acc 87.5000 (85.4167) lr 1.8980e-03 eta 0:09:30
epoch [31/200] batch [1/3] time 1.298 (1.298) data 0.278 (0.278) loss 0.9253 (0.9253) acc 81.2500 (81.2500) lr 1.8980e-03 eta 0:11:00
epoch [31/200] batch [2/3] time 1.022 (1.160) data 0.000 (0.139) loss 0.5049 (0.7151) acc 84.3750 (82.8125) lr 1.8980e-03 eta 0:09:49
epoch [31/200] batch [3/3] time 1.027 (1.116) data 0.000 (0.093) loss 0.2281 (0.5528) acc 90.6250 (85.4167) lr 1.8910e-03 eta 0:09:25
epoch [32/200] batch [1/3] time 1.306 (1.306) data 0.274 (0.274) loss 0.4075 (0.4075) acc 84.3750 (84.3750) lr 1.8910e-03 eta 0:11:00
epoch [32/200] batch [2/3] time 1.028 (1.167) data 0.000 (0.137) loss 0.4211 (0.4143) acc 93.7500 (89.0625) lr 1.8910e-03 eta 0:09:49
epoch [32/200] batch [3/3] time 1.026 (1.120) data 0.000 (0.092) loss 0.9375 (0.5887) acc 75.0000 (84.3750) lr 1.8838e-03 eta 0:09:24
epoch [33/200] batch [1/3] time 1.291 (1.291) data 0.268 (0.268) loss 0.7715 (0.7715) acc 84.3750 (84.3750) lr 1.8838e-03 eta 0:10:49
epoch [33/200] batch [2/3] time 1.034 (1.162) data 0.000 (0.134) loss 0.3381 (0.5548) acc 84.3750 (84.3750) lr 1.8838e-03 eta 0:09:43
epoch [33/200] batch [3/3] time 1.028 (1.118) data 0.000 (0.089) loss 0.6387 (0.5828) acc 87.5000 (85.4167) lr 1.8763e-03 eta 0:09:19
epoch [34/200] batch [1/3] time 1.302 (1.302) data 0.273 (0.273) loss 0.5977 (0.5977) acc 81.2500 (81.2500) lr 1.8763e-03 eta 0:10:50
epoch [34/200] batch [2/3] time 1.029 (1.166) data 0.000 (0.136) loss 0.3496 (0.4736) acc 90.6250 (85.9375) lr 1.8763e-03 eta 0:09:41
epoch [34/200] batch [3/3] time 1.025 (1.119) data 0.000 (0.091) loss 0.6016 (0.5163) acc 84.3750 (85.4167) lr 1.8686e-03 eta 0:09:17
epoch [35/200] batch [1/3] time 1.301 (1.301) data 0.276 (0.276) loss 0.5186 (0.5186) acc 87.5000 (87.5000) lr 1.8686e-03 eta 0:10:46
epoch [35/200] batch [2/3] time 1.028 (1.165) data 0.000 (0.138) loss 0.7227 (0.6206) acc 81.2500 (84.3750) lr 1.8686e-03 eta 0:09:37
epoch [35/200] batch [3/3] time 1.033 (1.121) data 0.000 (0.092) loss 0.4365 (0.5592) acc 87.5000 (85.4167) lr 1.8607e-03 eta 0:09:14
epoch [36/200] batch [1/3] time 1.305 (1.305) data 0.278 (0.278) loss 1.0820 (1.0820) acc 65.6250 (65.6250) lr 1.8607e-03 eta 0:10:44
epoch [36/200] batch [2/3] time 1.025 (1.165) data 0.000 (0.139) loss 0.5034 (0.7927) acc 87.5000 (76.5625) lr 1.8607e-03 eta 0:09:34
epoch [36/200] batch [3/3] time 1.029 (1.120) data 0.000 (0.093) loss 0.6602 (0.7485) acc 78.1250 (77.0833) lr 1.8526e-03 eta 0:09:10
epoch [37/200] batch [1/3] time 1.303 (1.303) data 0.278 (0.278) loss 0.5405 (0.5405) acc 87.5000 (87.5000) lr 1.8526e-03 eta 0:10:39
epoch [37/200] batch [2/3] time 1.026 (1.164) data 0.000 (0.139) loss 0.6440 (0.5923) acc 81.2500 (84.3750) lr 1.8526e-03 eta 0:09:30
epoch [37/200] batch [3/3] time 1.019 (1.116) data 0.000 (0.093) loss 1.0811 (0.7552) acc 75.0000 (81.2500) lr 1.8443e-03 eta 0:09:05
epoch [38/200] batch [1/3] time 1.291 (1.291) data 0.267 (0.267) loss 0.5439 (0.5439) acc 81.2500 (81.2500) lr 1.8443e-03 eta 0:10:30
epoch [38/200] batch [2/3] time 1.024 (1.158) data 0.000 (0.133) loss 0.8271 (0.6855) acc 84.3750 (82.8125) lr 1.8443e-03 eta 0:09:23
epoch [38/200] batch [3/3] time 1.030 (1.115) data 0.000 (0.089) loss 0.4895 (0.6202) acc 81.2500 (82.2917) lr 1.8358e-03 eta 0:09:01
epoch [39/200] batch [1/3] time 1.297 (1.297) data 0.271 (0.271) loss 0.5132 (0.5132) acc 90.6250 (90.6250) lr 1.8358e-03 eta 0:10:29
epoch [39/200] batch [2/3] time 1.027 (1.162) data 0.000 (0.136) loss 0.6445 (0.5789) acc 75.0000 (82.8125) lr 1.8358e-03 eta 0:09:22
epoch [39/200] batch [3/3] time 1.032 (1.119) data 0.000 (0.090) loss 0.5776 (0.5785) acc 84.3750 (83.3333) lr 1.8271e-03 eta 0:09:00
epoch [40/200] batch [1/3] time 1.310 (1.310) data 0.281 (0.281) loss 0.5215 (0.5215) acc 87.5000 (87.5000) lr 1.8271e-03 eta 0:10:31
epoch [40/200] batch [2/3] time 1.026 (1.168) data 0.000 (0.140) loss 0.8335 (0.6775) acc 84.3750 (85.9375) lr 1.8271e-03 eta 0:09:21
epoch [40/200] batch [3/3] time 1.030 (1.122) data 0.000 (0.094) loss 0.6724 (0.6758) acc 81.2500 (84.3750) lr 1.8181e-03 eta 0:08:58
epoch [41/200] batch [1/3] time 1.300 (1.300) data 0.275 (0.275) loss 0.4224 (0.4224) acc 84.3750 (84.3750) lr 1.8181e-03 eta 0:10:22
epoch [41/200] batch [2/3] time 1.032 (1.166) data 0.000 (0.137) loss 0.4229 (0.4226) acc 87.5000 (85.9375) lr 1.8181e-03 eta 0:09:17
epoch [41/200] batch [3/3] time 1.029 (1.120) data 0.000 (0.092) loss 0.4604 (0.4352) acc 87.5000 (86.4583) lr 1.8090e-03 eta 0:08:54
epoch [42/200] batch [1/3] time 1.287 (1.287) data 0.263 (0.263) loss 0.3782 (0.3782) acc 87.5000 (87.5000) lr 1.8090e-03 eta 0:10:12
epoch [42/200] batch [2/3] time 1.026 (1.157) data 0.000 (0.132) loss 0.4656 (0.4219) acc 87.5000 (87.5000) lr 1.8090e-03 eta 0:09:09
epoch [42/200] batch [3/3] time 1.033 (1.115) data 0.000 (0.088) loss 0.6328 (0.4922) acc 87.5000 (87.5000) lr 1.7997e-03 eta 0:08:48
epoch [43/200] batch [1/3] time 1.310 (1.310) data 0.283 (0.283) loss 0.8169 (0.8169) acc 75.0000 (75.0000) lr 1.7997e-03 eta 0:10:19
epoch [43/200] batch [2/3] time 1.022 (1.166) data 0.000 (0.141) loss 0.4670 (0.6420) acc 90.6250 (82.8125) lr 1.7997e-03 eta 0:09:10
epoch [43/200] batch [3/3] time 1.031 (1.121) data 0.000 (0.094) loss 0.4753 (0.5864) acc 81.2500 (82.2917) lr 1.7902e-03 eta 0:08:48
epoch [44/200] batch [1/3] time 1.296 (1.296) data 0.276 (0.276) loss 0.1907 (0.1907) acc 96.8750 (96.8750) lr 1.7902e-03 eta 0:10:08
epoch [44/200] batch [2/3] time 1.029 (1.162) data 0.000 (0.138) loss 0.4497 (0.3202) acc 84.3750 (90.6250) lr 1.7902e-03 eta 0:09:05
epoch [44/200] batch [3/3] time 1.024 (1.116) data 0.000 (0.092) loss 0.7109 (0.4504) acc 78.1250 (86.4583) lr 1.7804e-03 eta 0:08:42
epoch [45/200] batch [1/3] time 1.289 (1.289) data 0.263 (0.263) loss 0.7607 (0.7607) acc 75.0000 (75.0000) lr 1.7804e-03 eta 0:10:01
epoch [45/200] batch [2/3] time 1.029 (1.159) data 0.000 (0.132) loss 0.6387 (0.6997) acc 87.5000 (81.2500) lr 1.7804e-03 eta 0:09:00
epoch [45/200] batch [3/3] time 1.028 (1.115) data 0.000 (0.088) loss 0.6060 (0.6685) acc 87.5000 (83.3333) lr 1.7705e-03 eta 0:08:38
epoch [46/200] batch [1/3] time 1.308 (1.308) data 0.277 (0.277) loss 0.6870 (0.6870) acc 81.2500 (81.2500) lr 1.7705e-03 eta 0:10:06
epoch [46/200] batch [2/3] time 1.027 (1.167) data 0.000 (0.138) loss 0.2954 (0.4912) acc 93.7500 (87.5000) lr 1.7705e-03 eta 0:09:00
epoch [46/200] batch [3/3] time 1.022 (1.119) data 0.000 (0.092) loss 0.2292 (0.4039) acc 93.7500 (89.5833) lr 1.7604e-03 eta 0:08:36
epoch [47/200] batch [1/3] time 1.301 (1.301) data 0.280 (0.280) loss 0.3203 (0.3203) acc 96.8750 (96.8750) lr 1.7604e-03 eta 0:09:59
epoch [47/200] batch [2/3] time 1.034 (1.168) data 0.000 (0.140) loss 0.6172 (0.4688) acc 81.2500 (89.0625) lr 1.7604e-03 eta 0:08:57
epoch [47/200] batch [3/3] time 1.027 (1.121) data 0.000 (0.093) loss 0.3032 (0.4136) acc 90.6250 (89.5833) lr 1.7501e-03 eta 0:08:34
epoch [48/200] batch [1/3] time 1.315 (1.315) data 0.285 (0.285) loss 0.4390 (0.4390) acc 84.3750 (84.3750) lr 1.7501e-03 eta 0:10:02
epoch [48/200] batch [2/3] time 1.026 (1.171) data 0.000 (0.143) loss 0.2610 (0.3500) acc 90.6250 (87.5000) lr 1.7501e-03 eta 0:08:54
epoch [48/200] batch [3/3] time 1.025 (1.122) data 0.000 (0.095) loss 0.5088 (0.4029) acc 84.3750 (86.4583) lr 1.7396e-03 eta 0:08:31
epoch [49/200] batch [1/3] time 1.308 (1.308) data 0.281 (0.281) loss 0.8052 (0.8052) acc 81.2500 (81.2500) lr 1.7396e-03 eta 0:09:55
epoch [49/200] batch [2/3] time 1.030 (1.169) data 0.000 (0.140) loss 0.2708 (0.5380) acc 96.8750 (89.0625) lr 1.7396e-03 eta 0:08:50
epoch [49/200] batch [3/3] time 1.027 (1.122) data 0.000 (0.094) loss 0.6538 (0.5766) acc 81.2500 (86.4583) lr 1.7290e-03 eta 0:08:28
epoch [50/200] batch [1/3] time 1.306 (1.306) data 0.280 (0.280) loss 0.4753 (0.4753) acc 87.5000 (87.5000) lr 1.7290e-03 eta 0:09:50
epoch [50/200] batch [2/3] time 1.021 (1.164) data 0.000 (0.140) loss 0.4604 (0.4679) acc 90.6250 (89.0625) lr 1.7290e-03 eta 0:08:44
epoch [50/200] batch [3/3] time 1.023 (1.117) data 0.000 (0.093) loss 0.5605 (0.4988) acc 87.5000 (88.5417) lr 1.7181e-03 eta 0:08:22
epoch [51/200] batch [1/3] time 1.300 (1.300) data 0.272 (0.272) loss 0.1952 (0.1952) acc 96.8750 (96.8750) lr 1.7181e-03 eta 0:09:43
epoch [51/200] batch [2/3] time 1.033 (1.166) data 0.000 (0.136) loss 0.4341 (0.3146) acc 87.5000 (92.1875) lr 1.7181e-03 eta 0:08:42
epoch [51/200] batch [3/3] time 1.024 (1.119) data 0.000 (0.091) loss 0.3948 (0.3413) acc 90.6250 (91.6667) lr 1.7071e-03 eta 0:08:20
epoch [52/200] batch [1/3] time 1.294 (1.294) data 0.264 (0.264) loss 0.3596 (0.3596) acc 93.7500 (93.7500) lr 1.7071e-03 eta 0:09:37
epoch [52/200] batch [2/3] time 1.031 (1.162) data 0.000 (0.132) loss 0.4597 (0.4097) acc 84.3750 (89.0625) lr 1.7071e-03 eta 0:08:37
epoch [52/200] batch [3/3] time 1.027 (1.117) data 0.000 (0.088) loss 0.2128 (0.3440) acc 93.7500 (90.6250) lr 1.6959e-03 eta 0:08:16
epoch [53/200] batch [1/3] time 1.296 (1.296) data 0.269 (0.269) loss 0.5132 (0.5132) acc 93.7500 (93.7500) lr 1.6959e-03 eta 0:09:34
epoch [53/200] batch [2/3] time 1.027 (1.162) data 0.000 (0.135) loss 0.7798 (0.6465) acc 75.0000 (84.3750) lr 1.6959e-03 eta 0:08:33
epoch [53/200] batch [3/3] time 1.028 (1.117) data 0.000 (0.090) loss 0.3667 (0.5532) acc 90.6250 (86.4583) lr 1.6845e-03 eta 0:08:12
epoch [54/200] batch [1/3] time 1.310 (1.310) data 0.282 (0.282) loss 0.5215 (0.5215) acc 84.3750 (84.3750) lr 1.6845e-03 eta 0:09:36
epoch [54/200] batch [2/3] time 1.023 (1.166) data 0.000 (0.141) loss 0.5913 (0.5564) acc 87.5000 (85.9375) lr 1.6845e-03 eta 0:08:32
epoch [54/200] batch [3/3] time 1.024 (1.119) data 0.000 (0.094) loss 0.4807 (0.5312) acc 87.5000 (86.4583) lr 1.6730e-03 eta 0:08:10
epoch [55/200] batch [1/3] time 1.298 (1.298) data 0.269 (0.269) loss 0.2810 (0.2810) acc 96.8750 (96.8750) lr 1.6730e-03 eta 0:09:27
epoch [55/200] batch [2/3] time 1.029 (1.163) data 0.000 (0.134) loss 0.1831 (0.2321) acc 96.8750 (96.8750) lr 1.6730e-03 eta 0:08:27
epoch [55/200] batch [3/3] time 1.022 (1.116) data 0.000 (0.090) loss 0.4368 (0.3003) acc 90.6250 (94.7917) lr 1.6613e-03 eta 0:08:05
epoch [56/200] batch [1/3] time 1.304 (1.304) data 0.268 (0.268) loss 0.5376 (0.5376) acc 84.3750 (84.3750) lr 1.6613e-03 eta 0:09:25
epoch [56/200] batch [2/3] time 1.028 (1.166) data 0.000 (0.134) loss 0.3660 (0.4518) acc 93.7500 (89.0625) lr 1.6613e-03 eta 0:08:24
epoch [56/200] batch [3/3] time 1.034 (1.122) data 0.000 (0.089) loss 0.5273 (0.4770) acc 81.2500 (86.4583) lr 1.6494e-03 eta 0:08:04
epoch [57/200] batch [1/3] time 1.295 (1.295) data 0.271 (0.271) loss 0.3386 (0.3386) acc 93.7500 (93.7500) lr 1.6494e-03 eta 0:09:18
epoch [57/200] batch [2/3] time 1.029 (1.162) data 0.000 (0.136) loss 0.6216 (0.4801) acc 84.3750 (89.0625) lr 1.6494e-03 eta 0:08:19
epoch [57/200] batch [3/3] time 1.021 (1.115) data 0.000 (0.090) loss 0.2930 (0.4177) acc 90.6250 (89.5833) lr 1.6374e-03 eta 0:07:58
epoch [58/200] batch [1/3] time 1.302 (1.302) data 0.279 (0.279) loss 0.5864 (0.5864) acc 81.2500 (81.2500) lr 1.6374e-03 eta 0:09:17
epoch [58/200] batch [2/3] time 1.025 (1.164) data 0.000 (0.139) loss 0.7202 (0.6533) acc 84.3750 (82.8125) lr 1.6374e-03 eta 0:08:16
epoch [58/200] batch [3/3] time 1.029 (1.119) data 0.000 (0.093) loss 0.3491 (0.5519) acc 87.5000 (84.3750) lr 1.6252e-03 eta 0:07:56
epoch [59/200] batch [1/3] time 1.307 (1.307) data 0.285 (0.285) loss 0.5264 (0.5264) acc 87.5000 (87.5000) lr 1.6252e-03 eta 0:09:15
epoch [59/200] batch [2/3] time 1.025 (1.166) data 0.000 (0.142) loss 0.2664 (0.3964) acc 93.7500 (90.6250) lr 1.6252e-03 eta 0:08:14
epoch [59/200] batch [3/3] time 1.025 (1.119) data 0.000 (0.095) loss 0.3655 (0.3861) acc 84.3750 (88.5417) lr 1.6129e-03 eta 0:07:53
epoch [60/200] batch [1/3] time 1.297 (1.297) data 0.268 (0.268) loss 0.2384 (0.2384) acc 93.7500 (93.7500) lr 1.6129e-03 eta 0:09:07
epoch [60/200] batch [2/3] time 1.031 (1.164) data 0.000 (0.134) loss 0.7163 (0.4774) acc 78.1250 (85.9375) lr 1.6129e-03 eta 0:08:10
epoch [60/200] batch [3/3] time 1.027 (1.118) data 0.000 (0.089) loss 0.2448 (0.3998) acc 90.6250 (87.5000) lr 1.6004e-03 eta 0:07:49
epoch [61/200] batch [1/3] time 1.300 (1.300) data 0.271 (0.271) loss 0.4448 (0.4448) acc 90.6250 (90.6250) lr 1.6004e-03 eta 0:09:04
epoch [61/200] batch [2/3] time 1.030 (1.165) data 0.000 (0.135) loss 0.3445 (0.3947) acc 90.6250 (90.6250) lr 1.6004e-03 eta 0:08:06
epoch [61/200] batch [3/3] time 1.023 (1.117) data 0.000 (0.090) loss 0.8252 (0.5382) acc 78.1250 (86.4583) lr 1.5878e-03 eta 0:07:45
epoch [62/200] batch [1/3] time 1.300 (1.300) data 0.270 (0.270) loss 0.6885 (0.6885) acc 90.6250 (90.6250) lr 1.5878e-03 eta 0:09:00
epoch [62/200] batch [2/3] time 1.024 (1.162) data 0.000 (0.135) loss 0.5269 (0.6077) acc 87.5000 (89.0625) lr 1.5878e-03 eta 0:08:02
epoch [62/200] batch [3/3] time 1.029 (1.117) data 0.000 (0.090) loss 0.6587 (0.6247) acc 84.3750 (87.5000) lr 1.5750e-03 eta 0:07:42
epoch [63/200] batch [1/3] time 1.296 (1.296) data 0.270 (0.270) loss 0.3313 (0.3313) acc 81.2500 (81.2500) lr 1.5750e-03 eta 0:08:55
epoch [63/200] batch [2/3] time 1.032 (1.164) data 0.000 (0.135) loss 0.5898 (0.4606) acc 84.3750 (82.8125) lr 1.5750e-03 eta 0:07:59
epoch [63/200] batch [3/3] time 1.032 (1.120) data 0.000 (0.090) loss 0.5620 (0.4944) acc 81.2500 (82.2917) lr 1.5621e-03 eta 0:07:40
epoch [64/200] batch [1/3] time 1.296 (1.296) data 0.276 (0.276) loss 0.2007 (0.2007) acc 96.8750 (96.8750) lr 1.5621e-03 eta 0:08:51
epoch [64/200] batch [2/3] time 1.030 (1.163) data 0.000 (0.138) loss 0.3618 (0.2812) acc 93.7500 (95.3125) lr 1.5621e-03 eta 0:07:55
epoch [64/200] batch [3/3] time 1.025 (1.117) data 0.000 (0.092) loss 0.3108 (0.2911) acc 100.0000 (96.8750) lr 1.5490e-03 eta 0:07:35
epoch [65/200] batch [1/3] time 1.301 (1.301) data 0.271 (0.271) loss 0.4001 (0.4001) acc 93.7500 (93.7500) lr 1.5490e-03 eta 0:08:49
epoch [65/200] batch [2/3] time 1.025 (1.163) data 0.000 (0.136) loss 0.7964 (0.5983) acc 84.3750 (89.0625) lr 1.5490e-03 eta 0:07:52
epoch [65/200] batch [3/3] time 1.025 (1.117) data 0.000 (0.090) loss 0.3926 (0.5297) acc 93.7500 (90.6250) lr 1.5358e-03 eta 0:07:32
epoch [66/200] batch [1/3] time 1.290 (1.290) data 0.266 (0.266) loss 0.5767 (0.5767) acc 81.2500 (81.2500) lr 1.5358e-03 eta 0:08:41
epoch [66/200] batch [2/3] time 1.033 (1.161) data 0.000 (0.133) loss 0.5474 (0.5620) acc 87.5000 (84.3750) lr 1.5358e-03 eta 0:07:48
epoch [66/200] batch [3/3] time 1.025 (1.116) data 0.000 (0.089) loss 0.6797 (0.6012) acc 90.6250 (86.4583) lr 1.5225e-03 eta 0:07:28
epoch [67/200] batch [1/3] time 1.305 (1.305) data 0.278 (0.278) loss 0.2937 (0.2937) acc 96.8750 (96.8750) lr 1.5225e-03 eta 0:08:43
epoch [67/200] batch [2/3] time 1.026 (1.166) data 0.000 (0.139) loss 0.3875 (0.3406) acc 87.5000 (92.1875) lr 1.5225e-03 eta 0:07:46
epoch [67/200] batch [3/3] time 1.025 (1.119) data 0.000 (0.093) loss 0.3655 (0.3489) acc 93.7500 (92.7083) lr 1.5090e-03 eta 0:07:26
epoch [68/200] batch [1/3] time 1.298 (1.298) data 0.271 (0.271) loss 0.4287 (0.4287) acc 87.5000 (87.5000) lr 1.5090e-03 eta 0:08:36
epoch [68/200] batch [2/3] time 1.025 (1.162) data 0.000 (0.136) loss 0.1637 (0.2962) acc 93.7500 (90.6250) lr 1.5090e-03 eta 0:07:41
epoch [68/200] batch [3/3] time 1.029 (1.117) data 0.000 (0.090) loss 0.7393 (0.4439) acc 81.2500 (87.5000) lr 1.4955e-03 eta 0:07:22
epoch [69/200] batch [1/3] time 1.305 (1.305) data 0.279 (0.279) loss 0.7598 (0.7598) acc 81.2500 (81.2500) lr 1.4955e-03 eta 0:08:35
epoch [69/200] batch [2/3] time 1.027 (1.166) data 0.000 (0.139) loss 0.4275 (0.5936) acc 90.6250 (85.9375) lr 1.4955e-03 eta 0:07:39
epoch [69/200] batch [3/3] time 1.027 (1.120) data 0.000 (0.093) loss 0.5283 (0.5719) acc 84.3750 (85.4167) lr 1.4818e-03 eta 0:07:20
epoch [70/200] batch [1/3] time 1.309 (1.309) data 0.278 (0.278) loss 0.5537 (0.5537) acc 84.3750 (84.3750) lr 1.4818e-03 eta 0:08:32
epoch [70/200] batch [2/3] time 1.028 (1.168) data 0.000 (0.139) loss 0.5322 (0.5430) acc 84.3750 (84.3750) lr 1.4818e-03 eta 0:07:36
epoch [70/200] batch [3/3] time 1.022 (1.120) data 0.000 (0.093) loss 0.3752 (0.4871) acc 90.6250 (86.4583) lr 1.4679e-03 eta 0:07:16
epoch [71/200] batch [1/3] time 1.298 (1.298) data 0.272 (0.272) loss 0.1602 (0.1602) acc 96.8750 (96.8750) lr 1.4679e-03 eta 0:08:24
epoch [71/200] batch [2/3] time 1.029 (1.163) data 0.000 (0.136) loss 0.4119 (0.2860) acc 84.3750 (90.6250) lr 1.4679e-03 eta 0:07:31
epoch [71/200] batch [3/3] time 1.023 (1.117) data 0.000 (0.091) loss 0.3083 (0.2935) acc 90.6250 (90.6250) lr 1.4540e-03 eta 0:07:12
epoch [72/200] batch [1/3] time 1.289 (1.289) data 0.263 (0.263) loss 0.5273 (0.5273) acc 81.2500 (81.2500) lr 1.4540e-03 eta 0:08:17
epoch [72/200] batch [2/3] time 1.026 (1.157) data 0.000 (0.132) loss 0.3303 (0.4288) acc 90.6250 (85.9375) lr 1.4540e-03 eta 0:07:25
epoch [72/200] batch [3/3] time 1.026 (1.114) data 0.000 (0.088) loss 0.3711 (0.4096) acc 93.7500 (88.5417) lr 1.4399e-03 eta 0:07:07
epoch [73/200] batch [1/3] time 1.304 (1.304) data 0.274 (0.274) loss 0.1613 (0.1613) acc 100.0000 (100.0000) lr 1.4399e-03 eta 0:08:19
epoch [73/200] batch [2/3] time 1.033 (1.168) data 0.000 (0.137) loss 0.4470 (0.3041) acc 87.5000 (93.7500) lr 1.4399e-03 eta 0:07:26
epoch [73/200] batch [3/3] time 1.030 (1.122) data 0.000 (0.091) loss 0.2529 (0.2871) acc 96.8750 (94.7917) lr 1.4258e-03 eta 0:07:07
epoch [74/200] batch [1/3] time 1.309 (1.309) data 0.283 (0.283) loss 0.4570 (0.4570) acc 87.5000 (87.5000) lr 1.4258e-03 eta 0:08:17
epoch [74/200] batch [2/3] time 1.028 (1.169) data 0.000 (0.142) loss 0.5771 (0.5171) acc 90.6250 (89.0625) lr 1.4258e-03 eta 0:07:22
epoch [74/200] batch [3/3] time 1.026 (1.121) data 0.000 (0.094) loss 0.4458 (0.4933) acc 84.3750 (87.5000) lr 1.4115e-03 eta 0:07:03
epoch [75/200] batch [1/3] time 1.310 (1.310) data 0.283 (0.283) loss 0.2595 (0.2595) acc 93.7500 (93.7500) lr 1.4115e-03 eta 0:08:13
epoch [75/200] batch [2/3] time 1.025 (1.168) data 0.000 (0.141) loss 0.2181 (0.2388) acc 93.7500 (93.7500) lr 1.4115e-03 eta 0:07:19
epoch [75/200] batch [3/3] time 1.027 (1.121) data 0.000 (0.094) loss 0.7231 (0.4003) acc 81.2500 (89.5833) lr 1.3971e-03 eta 0:07:00
epoch [76/200] batch [1/3] time 1.306 (1.306) data 0.280 (0.280) loss 0.1998 (0.1998) acc 93.7500 (93.7500) lr 1.3971e-03 eta 0:08:08
epoch [76/200] batch [2/3] time 1.031 (1.168) data 0.000 (0.140) loss 0.3914 (0.2956) acc 90.6250 (92.1875) lr 1.3971e-03 eta 0:07:15
epoch [76/200] batch [3/3] time 1.030 (1.122) data 0.000 (0.093) loss 0.3665 (0.3192) acc 93.7500 (92.7083) lr 1.3827e-03 eta 0:06:57
epoch [77/200] batch [1/3] time 1.287 (1.287) data 0.263 (0.263) loss 0.3066 (0.3066) acc 90.6250 (90.6250) lr 1.3827e-03 eta 0:07:57
epoch [77/200] batch [2/3] time 1.027 (1.157) data 0.000 (0.132) loss 0.4905 (0.3986) acc 87.5000 (89.0625) lr 1.3827e-03 eta 0:07:08
epoch [77/200] batch [3/3] time 1.017 (1.110) data 0.000 (0.088) loss 0.3367 (0.3779) acc 90.6250 (89.5833) lr 1.3681e-03 eta 0:06:49
epoch [78/200] batch [1/3] time 1.312 (1.312) data 0.280 (0.280) loss 0.2527 (0.2527) acc 96.8750 (96.8750) lr 1.3681e-03 eta 0:08:02
epoch [78/200] batch [2/3] time 1.030 (1.171) data 0.000 (0.140) loss 0.3042 (0.2784) acc 93.7500 (95.3125) lr 1.3681e-03 eta 0:07:09
epoch [78/200] batch [3/3] time 1.021 (1.121) data 0.000 (0.093) loss 0.3110 (0.2893) acc 90.6250 (93.7500) lr 1.3535e-03 eta 0:06:50
epoch [79/200] batch [1/3] time 1.282 (1.282) data 0.261 (0.261) loss 0.5322 (0.5322) acc 87.5000 (87.5000) lr 1.3535e-03 eta 0:07:47
epoch [79/200] batch [2/3] time 1.027 (1.154) data 0.000 (0.130) loss 0.5112 (0.5217) acc 87.5000 (87.5000) lr 1.3535e-03 eta 0:07:00
epoch [79/200] batch [3/3] time 1.022 (1.110) data 0.000 (0.087) loss 0.2365 (0.4266) acc 96.8750 (90.6250) lr 1.3387e-03 eta 0:06:43
epoch [80/200] batch [1/3] time 1.291 (1.291) data 0.262 (0.262) loss 0.5122 (0.5122) acc 87.5000 (87.5000) lr 1.3387e-03 eta 0:07:47
epoch [80/200] batch [2/3] time 1.035 (1.163) data 0.000 (0.131) loss 0.2954 (0.4038) acc 93.7500 (90.6250) lr 1.3387e-03 eta 0:06:59
epoch [80/200] batch [3/3] time 1.027 (1.118) data 0.000 (0.087) loss 0.4468 (0.4181) acc 84.3750 (88.5417) lr 1.3239e-03 eta 0:06:42
epoch [81/200] batch [1/3] time 1.301 (1.301) data 0.279 (0.279) loss 0.3242 (0.3242) acc 93.7500 (93.7500) lr 1.3239e-03 eta 0:07:46
epoch [81/200] batch [2/3] time 1.028 (1.164) data 0.000 (0.140) loss 0.6602 (0.4922) acc 81.2500 (87.5000) lr 1.3239e-03 eta 0:06:56
epoch [81/200] batch [3/3] time 1.024 (1.118) data 0.000 (0.093) loss 0.4417 (0.4753) acc 90.6250 (88.5417) lr 1.3090e-03 eta 0:06:39
epoch [82/200] batch [1/3] time 1.291 (1.291) data 0.262 (0.262) loss 0.3938 (0.3938) acc 87.5000 (87.5000) lr 1.3090e-03 eta 0:07:39
epoch [82/200] batch [2/3] time 1.025 (1.158) data 0.000 (0.131) loss 0.2966 (0.3452) acc 90.6250 (89.0625) lr 1.3090e-03 eta 0:06:51
epoch [82/200] batch [3/3] time 1.027 (1.114) data 0.000 (0.087) loss 0.4102 (0.3669) acc 90.6250 (89.5833) lr 1.2940e-03 eta 0:06:34
epoch [83/200] batch [1/3] time 1.308 (1.308) data 0.284 (0.284) loss 0.2827 (0.2827) acc 90.6250 (90.6250) lr 1.2940e-03 eta 0:07:41
epoch [83/200] batch [2/3] time 1.025 (1.166) data 0.000 (0.142) loss 0.1946 (0.2386) acc 96.8750 (93.7500) lr 1.2940e-03 eta 0:06:50
epoch [83/200] batch [3/3] time 1.027 (1.120) data 0.000 (0.095) loss 0.5298 (0.3357) acc 90.6250 (92.7083) lr 1.2790e-03 eta 0:06:33
epoch [84/200] batch [1/3] time 1.296 (1.296) data 0.270 (0.270) loss 0.5127 (0.5127) acc 87.5000 (87.5000) lr 1.2790e-03 eta 0:07:33
epoch [84/200] batch [2/3] time 1.027 (1.162) data 0.000 (0.135) loss 0.4714 (0.4921) acc 87.5000 (87.5000) lr 1.2790e-03 eta 0:06:45
epoch [84/200] batch [3/3] time 1.029 (1.117) data 0.000 (0.090) loss 0.1349 (0.3730) acc 100.0000 (91.6667) lr 1.2639e-03 eta 0:06:28
epoch [85/200] batch [1/3] time 1.288 (1.288) data 0.266 (0.266) loss 0.5791 (0.5791) acc 84.3750 (84.3750) lr 1.2639e-03 eta 0:07:26
epoch [85/200] batch [2/3] time 1.025 (1.156) data 0.000 (0.133) loss 0.5571 (0.5681) acc 87.5000 (85.9375) lr 1.2639e-03 eta 0:06:40
epoch [85/200] batch [3/3] time 1.026 (1.113) data 0.000 (0.089) loss 0.4421 (0.5261) acc 84.3750 (85.4167) lr 1.2487e-03 eta 0:06:23
epoch [86/200] batch [1/3] time 1.303 (1.303) data 0.274 (0.274) loss 0.3145 (0.3145) acc 90.6250 (90.6250) lr 1.2487e-03 eta 0:07:28
epoch [86/200] batch [2/3] time 1.030 (1.166) data 0.000 (0.137) loss 0.4644 (0.3894) acc 81.2500 (85.9375) lr 1.2487e-03 eta 0:06:40
epoch [86/200] batch [3/3] time 1.021 (1.118) data 0.000 (0.091) loss 0.5752 (0.4513) acc 87.5000 (86.4583) lr 1.2334e-03 eta 0:06:22
epoch [87/200] batch [1/3] time 1.302 (1.302) data 0.271 (0.271) loss 0.6694 (0.6694) acc 81.2500 (81.2500) lr 1.2334e-03 eta 0:07:24
epoch [87/200] batch [2/3] time 1.031 (1.167) data 0.000 (0.136) loss 0.5825 (0.6260) acc 90.6250 (85.9375) lr 1.2334e-03 eta 0:06:36
epoch [87/200] batch [3/3] time 1.027 (1.120) data 0.000 (0.090) loss 0.1780 (0.4766) acc 96.8750 (89.5833) lr 1.2181e-03 eta 0:06:19
epoch [88/200] batch [1/3] time 1.306 (1.306) data 0.277 (0.277) loss 0.5059 (0.5059) acc 87.5000 (87.5000) lr 1.2181e-03 eta 0:07:21
epoch [88/200] batch [2/3] time 1.027 (1.166) data 0.000 (0.138) loss 0.2491 (0.3775) acc 96.8750 (92.1875) lr 1.2181e-03 eta 0:06:32
epoch [88/200] batch [3/3] time 1.016 (1.116) data 0.000 (0.092) loss 0.2551 (0.3367) acc 93.7500 (92.7083) lr 1.2028e-03 eta 0:06:15
epoch [89/200] batch [1/3] time 1.295 (1.295) data 0.264 (0.264) loss 0.2189 (0.2189) acc 96.8750 (96.8750) lr 1.2028e-03 eta 0:07:13
epoch [89/200] batch [2/3] time 1.025 (1.160) data 0.000 (0.132) loss 0.1438 (0.1813) acc 96.8750 (96.8750) lr 1.2028e-03 eta 0:06:27
epoch [89/200] batch [3/3] time 1.020 (1.113) data 0.000 (0.088) loss 0.1592 (0.1740) acc 100.0000 (97.9167) lr 1.1874e-03 eta 0:06:10
epoch [90/200] batch [1/3] time 1.305 (1.305) data 0.274 (0.274) loss 0.4482 (0.4482) acc 84.3750 (84.3750) lr 1.1874e-03 eta 0:07:13
epoch [90/200] batch [2/3] time 1.028 (1.167) data 0.000 (0.137) loss 0.1947 (0.3215) acc 96.8750 (90.6250) lr 1.1874e-03 eta 0:06:26
epoch [90/200] batch [3/3] time 1.033 (1.122) data 0.000 (0.092) loss 0.2842 (0.3090) acc 87.5000 (89.5833) lr 1.1719e-03 eta 0:06:10
epoch [91/200] batch [1/3] time 1.285 (1.285) data 0.264 (0.264) loss 0.3062 (0.3062) acc 87.5000 (87.5000) lr 1.1719e-03 eta 0:07:02
epoch [91/200] batch [2/3] time 1.029 (1.157) data 0.000 (0.132) loss 0.3269 (0.3165) acc 93.7500 (90.6250) lr 1.1719e-03 eta 0:06:19
epoch [91/200] batch [3/3] time 1.026 (1.113) data 0.000 (0.088) loss 0.3091 (0.3140) acc 93.7500 (91.6667) lr 1.1564e-03 eta 0:06:04
epoch [92/200] batch [1/3] time 1.285 (1.285) data 0.269 (0.269) loss 0.4453 (0.4453) acc 84.3750 (84.3750) lr 1.1564e-03 eta 0:06:58
epoch [92/200] batch [2/3] time 1.023 (1.154) data 0.000 (0.135) loss 0.7236 (0.5845) acc 78.1250 (81.2500) lr 1.1564e-03 eta 0:06:15
epoch [92/200] batch [3/3] time 1.028 (1.112) data 0.000 (0.090) loss 0.3555 (0.5081) acc 87.5000 (83.3333) lr 1.1409e-03 eta 0:06:00
epoch [93/200] batch [1/3] time 1.312 (1.312) data 0.282 (0.282) loss 0.5269 (0.5269) acc 84.3750 (84.3750) lr 1.1409e-03 eta 0:07:03
epoch [93/200] batch [2/3] time 1.033 (1.173) data 0.000 (0.141) loss 0.3542 (0.4406) acc 93.7500 (89.0625) lr 1.1409e-03 eta 0:06:17
epoch [93/200] batch [3/3] time 1.022 (1.122) data 0.000 (0.094) loss 0.1818 (0.3543) acc 96.8750 (91.6667) lr 1.1253e-03 eta 0:06:00
epoch [94/200] batch [1/3] time 1.292 (1.292) data 0.267 (0.267) loss 0.3259 (0.3259) acc 96.8750 (96.8750) lr 1.1253e-03 eta 0:06:53
epoch [94/200] batch [2/3] time 1.027 (1.160) data 0.000 (0.134) loss 0.5903 (0.4581) acc 81.2500 (89.0625) lr 1.1253e-03 eta 0:06:09
epoch [94/200] batch [3/3] time 1.032 (1.117) data 0.000 (0.089) loss 0.3660 (0.4274) acc 93.7500 (90.6250) lr 1.1097e-03 eta 0:05:55
epoch [95/200] batch [1/3] time 1.305 (1.305) data 0.278 (0.278) loss 0.6733 (0.6733) acc 87.5000 (87.5000) lr 1.1097e-03 eta 0:06:53
epoch [95/200] batch [2/3] time 1.026 (1.165) data 0.000 (0.139) loss 0.1763 (0.4248) acc 93.7500 (90.6250) lr 1.1097e-03 eta 0:06:08
epoch [95/200] batch [3/3] time 1.026 (1.119) data 0.000 (0.093) loss 0.1047 (0.3181) acc 100.0000 (93.7500) lr 1.0941e-03 eta 0:05:52
epoch [96/200] batch [1/3] time 1.297 (1.297) data 0.273 (0.273) loss 0.4548 (0.4548) acc 84.3750 (84.3750) lr 1.0941e-03 eta 0:06:47
epoch [96/200] batch [2/3] time 1.029 (1.163) data 0.000 (0.137) loss 0.1299 (0.2924) acc 100.0000 (92.1875) lr 1.0941e-03 eta 0:06:04
epoch [96/200] batch [3/3] time 1.029 (1.118) data 0.000 (0.091) loss 0.2181 (0.2676) acc 96.8750 (93.7500) lr 1.0785e-03 eta 0:05:48
epoch [97/200] batch [1/3] time 1.306 (1.306) data 0.280 (0.280) loss 0.1159 (0.1159) acc 96.8750 (96.8750) lr 1.0785e-03 eta 0:06:46
epoch [97/200] batch [2/3] time 1.028 (1.167) data 0.000 (0.140) loss 0.2837 (0.1998) acc 93.7500 (95.3125) lr 1.0785e-03 eta 0:06:01
epoch [97/200] batch [3/3] time 1.024 (1.119) data 0.000 (0.093) loss 0.3806 (0.2601) acc 93.7500 (94.7917) lr 1.0628e-03 eta 0:05:45
epoch [98/200] batch [1/3] time 1.293 (1.293) data 0.261 (0.261) loss 0.5107 (0.5107) acc 87.5000 (87.5000) lr 1.0628e-03 eta 0:06:38
epoch [98/200] batch [2/3] time 1.024 (1.158) data 0.000 (0.131) loss 0.7319 (0.6213) acc 81.2500 (84.3750) lr 1.0628e-03 eta 0:05:55
epoch [98/200] batch [3/3] time 1.027 (1.115) data 0.000 (0.087) loss 0.3975 (0.5467) acc 90.6250 (86.4583) lr 1.0471e-03 eta 0:05:41
epoch [99/200] batch [1/3] time 1.297 (1.297) data 0.272 (0.272) loss 0.3042 (0.3042) acc 93.7500 (93.7500) lr 1.0471e-03 eta 0:06:35
epoch [99/200] batch [2/3] time 1.025 (1.161) data 0.000 (0.136) loss 0.2766 (0.2904) acc 93.7500 (93.7500) lr 1.0471e-03 eta 0:05:52
epoch [99/200] batch [3/3] time 1.030 (1.117) data 0.000 (0.091) loss 0.3225 (0.3011) acc 90.6250 (92.7083) lr 1.0314e-03 eta 0:05:38
epoch [100/200] batch [1/3] time 1.290 (1.290) data 0.262 (0.262) loss 0.4075 (0.4075) acc 93.7500 (93.7500) lr 1.0314e-03 eta 0:06:29
epoch [100/200] batch [2/3] time 1.029 (1.159) data 0.000 (0.131) loss 0.1108 (0.2591) acc 96.8750 (95.3125) lr 1.0314e-03 eta 0:05:48
epoch [100/200] batch [3/3] time 1.026 (1.115) data 0.000 (0.088) loss 0.1656 (0.2280) acc 96.8750 (95.8333) lr 1.0157e-03 eta 0:05:34
epoch [101/200] batch [1/3] time 1.289 (1.289) data 0.266 (0.266) loss 0.2642 (0.2642) acc 90.6250 (90.6250) lr 1.0157e-03 eta 0:06:25
epoch [101/200] batch [2/3] time 1.025 (1.157) data 0.000 (0.133) loss 0.1486 (0.2064) acc 96.8750 (93.7500) lr 1.0157e-03 eta 0:05:44
epoch [101/200] batch [3/3] time 1.026 (1.113) data 0.000 (0.089) loss 0.4768 (0.2965) acc 90.6250 (92.7083) lr 1.0000e-03 eta 0:05:30
epoch [102/200] batch [1/3] time 1.298 (1.298) data 0.270 (0.270) loss 0.4060 (0.4060) acc 90.6250 (90.6250) lr 1.0000e-03 eta 0:06:24
epoch [102/200] batch [2/3] time 1.026 (1.162) data 0.000 (0.135) loss 0.3721 (0.3890) acc 90.6250 (90.6250) lr 1.0000e-03 eta 0:05:42
epoch [102/200] batch [3/3] time 1.025 (1.116) data 0.000 (0.090) loss 0.4177 (0.3986) acc 90.6250 (90.6250) lr 9.8429e-04 eta 0:05:28
epoch [103/200] batch [1/3] time 1.292 (1.292) data 0.267 (0.267) loss 0.5029 (0.5029) acc 81.2500 (81.2500) lr 9.8429e-04 eta 0:06:18
epoch [103/200] batch [2/3] time 1.025 (1.158) data 0.000 (0.134) loss 0.3909 (0.4469) acc 90.6250 (85.9375) lr 9.8429e-04 eta 0:05:38
epoch [103/200] batch [3/3] time 1.022 (1.113) data 0.000 (0.089) loss 0.2050 (0.3663) acc 93.7500 (88.5417) lr 9.6859e-04 eta 0:05:23
epoch [104/200] batch [1/3] time 1.295 (1.295) data 0.271 (0.271) loss 0.1116 (0.1116) acc 100.0000 (100.0000) lr 9.6859e-04 eta 0:06:15
epoch [104/200] batch [2/3] time 1.028 (1.162) data 0.000 (0.135) loss 0.1136 (0.1126) acc 100.0000 (100.0000) lr 9.6859e-04 eta 0:05:35
epoch [104/200] batch [3/3] time 1.024 (1.116) data 0.000 (0.090) loss 0.3135 (0.1796) acc 90.6250 (96.8750) lr 9.5289e-04 eta 0:05:21
epoch [105/200] batch [1/3] time 1.304 (1.304) data 0.273 (0.273) loss 0.0648 (0.0648) acc 100.0000 (100.0000) lr 9.5289e-04 eta 0:06:14
epoch [105/200] batch [2/3] time 1.019 (1.162) data 0.000 (0.136) loss 0.4614 (0.2631) acc 90.6250 (95.3125) lr 9.5289e-04 eta 0:05:32
epoch [105/200] batch [3/3] time 1.024 (1.116) data 0.000 (0.091) loss 0.3059 (0.2774) acc 90.6250 (93.7500) lr 9.3721e-04 eta 0:05:18
epoch [106/200] batch [1/3] time 1.291 (1.291) data 0.265 (0.265) loss 0.1627 (0.1627) acc 96.8750 (96.8750) lr 9.3721e-04 eta 0:06:06
epoch [106/200] batch [2/3] time 1.024 (1.157) data 0.000 (0.133) loss 0.2329 (0.1978) acc 93.7500 (95.3125) lr 9.3721e-04 eta 0:05:27
epoch [106/200] batch [3/3] time 1.027 (1.114) data 0.000 (0.089) loss 0.2900 (0.2286) acc 93.7500 (94.7917) lr 9.2154e-04 eta 0:05:14
epoch [107/200] batch [1/3] time 1.287 (1.287) data 0.263 (0.263) loss 0.2944 (0.2944) acc 90.6250 (90.6250) lr 9.2154e-04 eta 0:06:01
epoch [107/200] batch [2/3] time 1.024 (1.156) data 0.000 (0.131) loss 0.1448 (0.2196) acc 96.8750 (93.7500) lr 9.2154e-04 eta 0:05:23
epoch [107/200] batch [3/3] time 1.029 (1.114) data 0.000 (0.088) loss 0.3184 (0.2525) acc 93.7500 (93.7500) lr 9.0589e-04 eta 0:05:10
epoch [108/200] batch [1/3] time 1.288 (1.288) data 0.262 (0.262) loss 0.2693 (0.2693) acc 93.7500 (93.7500) lr 9.0589e-04 eta 0:05:58
epoch [108/200] batch [2/3] time 1.029 (1.159) data 0.000 (0.131) loss 0.2869 (0.2781) acc 96.8750 (95.3125) lr 9.0589e-04 eta 0:05:20
epoch [108/200] batch [3/3] time 1.027 (1.115) data 0.000 (0.087) loss 0.3455 (0.3005) acc 93.7500 (94.7917) lr 8.9027e-04 eta 0:05:07
epoch [109/200] batch [1/3] time 1.292 (1.292) data 0.262 (0.262) loss 0.3140 (0.3140) acc 90.6250 (90.6250) lr 8.9027e-04 eta 0:05:55
epoch [109/200] batch [2/3] time 1.030 (1.161) data 0.000 (0.131) loss 0.2878 (0.3009) acc 90.6250 (90.6250) lr 8.9027e-04 eta 0:05:18
epoch [109/200] batch [3/3] time 1.017 (1.113) data 0.000 (0.087) loss 0.2671 (0.2896) acc 90.6250 (90.6250) lr 8.7467e-04 eta 0:05:03
epoch [110/200] batch [1/3] time 1.304 (1.304) data 0.279 (0.279) loss 0.4131 (0.4131) acc 90.6250 (90.6250) lr 8.7467e-04 eta 0:05:54
epoch [110/200] batch [2/3] time 1.026 (1.165) data 0.000 (0.140) loss 0.1541 (0.2836) acc 96.8750 (93.7500) lr 8.7467e-04 eta 0:05:15
epoch [110/200] batch [3/3] time 1.026 (1.119) data 0.000 (0.093) loss 0.4333 (0.3335) acc 90.6250 (92.7083) lr 8.5910e-04 eta 0:05:02
epoch [111/200] batch [1/3] time 1.294 (1.294) data 0.271 (0.271) loss 0.1326 (0.1326) acc 100.0000 (100.0000) lr 8.5910e-04 eta 0:05:48
epoch [111/200] batch [2/3] time 1.028 (1.161) data 0.000 (0.136) loss 0.5430 (0.3378) acc 87.5000 (93.7500) lr 8.5910e-04 eta 0:05:11
epoch [111/200] batch [3/3] time 1.027 (1.116) data 0.000 (0.091) loss 0.3032 (0.3263) acc 90.6250 (92.7083) lr 8.4357e-04 eta 0:04:57
epoch [112/200] batch [1/3] time 1.300 (1.300) data 0.272 (0.272) loss 0.4009 (0.4009) acc 87.5000 (87.5000) lr 8.4357e-04 eta 0:05:45
epoch [112/200] batch [2/3] time 1.025 (1.163) data 0.000 (0.136) loss 0.2705 (0.3357) acc 93.7500 (90.6250) lr 8.4357e-04 eta 0:05:08
epoch [112/200] batch [3/3] time 1.024 (1.116) data 0.000 (0.091) loss 0.1227 (0.2647) acc 100.0000 (93.7500) lr 8.2807e-04 eta 0:04:54
epoch [113/200] batch [1/3] time 1.306 (1.306) data 0.281 (0.281) loss 0.1766 (0.1766) acc 96.8750 (96.8750) lr 8.2807e-04 eta 0:05:43
epoch [113/200] batch [2/3] time 1.023 (1.165) data 0.000 (0.140) loss 0.3105 (0.2436) acc 93.7500 (95.3125) lr 8.2807e-04 eta 0:05:05
epoch [113/200] batch [3/3] time 1.029 (1.120) data 0.000 (0.094) loss 0.4417 (0.3096) acc 90.6250 (93.7500) lr 8.1262e-04 eta 0:04:52
epoch [114/200] batch [1/3] time 1.294 (1.294) data 0.264 (0.264) loss 0.3774 (0.3774) acc 87.5000 (87.5000) lr 8.1262e-04 eta 0:05:36
epoch [114/200] batch [2/3] time 1.026 (1.160) data 0.000 (0.132) loss 0.6108 (0.4941) acc 78.1250 (82.8125) lr 8.1262e-04 eta 0:05:00
epoch [114/200] batch [3/3] time 1.025 (1.115) data 0.000 (0.088) loss 0.3779 (0.4554) acc 87.5000 (84.3750) lr 7.9721e-04 eta 0:04:47
epoch [115/200] batch [1/3] time 1.286 (1.286) data 0.262 (0.262) loss 0.4565 (0.4565) acc 87.5000 (87.5000) lr 7.9721e-04 eta 0:05:30
epoch [115/200] batch [2/3] time 1.033 (1.160) data 0.000 (0.131) loss 0.2233 (0.3399) acc 96.8750 (92.1875) lr 7.9721e-04 eta 0:04:56
epoch [115/200] batch [3/3] time 1.029 (1.116) data 0.000 (0.088) loss 0.2515 (0.3104) acc 93.7500 (92.7083) lr 7.8186e-04 eta 0:04:44
epoch [116/200] batch [1/3] time 1.289 (1.289) data 0.262 (0.262) loss 0.5024 (0.5024) acc 90.6250 (90.6250) lr 7.8186e-04 eta 0:05:27
epoch [116/200] batch [2/3] time 1.029 (1.159) data 0.000 (0.131) loss 0.2534 (0.3779) acc 90.6250 (90.6250) lr 7.8186e-04 eta 0:04:53
epoch [116/200] batch [3/3] time 1.029 (1.116) data 0.000 (0.087) loss 0.3027 (0.3529) acc 90.6250 (90.6250) lr 7.6655e-04 eta 0:04:41
epoch [117/200] batch [1/3] time 1.307 (1.307) data 0.279 (0.279) loss 0.3845 (0.3845) acc 87.5000 (87.5000) lr 7.6655e-04 eta 0:05:28
epoch [117/200] batch [2/3] time 1.028 (1.168) data 0.000 (0.139) loss 0.2479 (0.3162) acc 93.7500 (90.6250) lr 7.6655e-04 eta 0:04:51
epoch [117/200] batch [3/3] time 1.027 (1.121) data 0.000 (0.093) loss 0.3398 (0.3241) acc 93.7500 (91.6667) lr 7.5131e-04 eta 0:04:39
epoch [118/200] batch [1/3] time 1.309 (1.309) data 0.278 (0.278) loss 0.3479 (0.3479) acc 90.6250 (90.6250) lr 7.5131e-04 eta 0:05:24
epoch [118/200] batch [2/3] time 1.032 (1.170) data 0.000 (0.139) loss 0.3616 (0.3547) acc 93.7500 (92.1875) lr 7.5131e-04 eta 0:04:49
epoch [118/200] batch [3/3] time 1.028 (1.123) data 0.000 (0.093) loss 0.4421 (0.3839) acc 87.5000 (90.6250) lr 7.3613e-04 eta 0:04:36
epoch [119/200] batch [1/3] time 1.294 (1.294) data 0.264 (0.264) loss 0.3433 (0.3433) acc 87.5000 (87.5000) lr 7.3613e-04 eta 0:05:16
epoch [119/200] batch [2/3] time 1.032 (1.163) data 0.000 (0.132) loss 0.5791 (0.4612) acc 81.2500 (84.3750) lr 7.3613e-04 eta 0:04:43
epoch [119/200] batch [3/3] time 1.026 (1.117) data 0.000 (0.088) loss 0.2028 (0.3750) acc 93.7500 (87.5000) lr 7.2101e-04 eta 0:04:31
epoch [120/200] batch [1/3] time 1.303 (1.303) data 0.274 (0.274) loss 0.3513 (0.3513) acc 87.5000 (87.5000) lr 7.2101e-04 eta 0:05:15
epoch [120/200] batch [2/3] time 1.022 (1.162) data 0.000 (0.137) loss 0.2827 (0.3170) acc 93.7500 (90.6250) lr 7.2101e-04 eta 0:04:40
epoch [120/200] batch [3/3] time 1.030 (1.118) data 0.000 (0.091) loss 0.2659 (0.3000) acc 93.7500 (91.6667) lr 7.0596e-04 eta 0:04:28
epoch [121/200] batch [1/3] time 1.307 (1.307) data 0.279 (0.279) loss 0.1792 (0.1792) acc 96.8750 (96.8750) lr 7.0596e-04 eta 0:05:12
epoch [121/200] batch [2/3] time 1.030 (1.168) data 0.000 (0.140) loss 0.3540 (0.2666) acc 87.5000 (92.1875) lr 7.0596e-04 eta 0:04:38
epoch [121/200] batch [3/3] time 1.025 (1.121) data 0.000 (0.093) loss 0.3059 (0.2797) acc 90.6250 (91.6667) lr 6.9098e-04 eta 0:04:25
epoch [122/200] batch [1/3] time 1.297 (1.297) data 0.271 (0.271) loss 0.5103 (0.5103) acc 90.6250 (90.6250) lr 6.9098e-04 eta 0:05:06
epoch [122/200] batch [2/3] time 1.029 (1.163) data 0.000 (0.136) loss 0.5806 (0.5454) acc 84.3750 (87.5000) lr 6.9098e-04 eta 0:04:33
epoch [122/200] batch [3/3] time 1.031 (1.119) data 0.000 (0.090) loss 0.1741 (0.4216) acc 93.7500 (89.5833) lr 6.7608e-04 eta 0:04:21
epoch [123/200] batch [1/3] time 1.307 (1.307) data 0.278 (0.278) loss 0.4666 (0.4666) acc 87.5000 (87.5000) lr 6.7608e-04 eta 0:05:04
epoch [123/200] batch [2/3] time 1.028 (1.167) data 0.000 (0.139) loss 0.2556 (0.3611) acc 93.7500 (90.6250) lr 6.7608e-04 eta 0:04:30
epoch [123/200] batch [3/3] time 1.027 (1.121) data 0.000 (0.093) loss 0.5464 (0.4229) acc 87.5000 (89.5833) lr 6.6126e-04 eta 0:04:18
epoch [124/200] batch [1/3] time 1.294 (1.294) data 0.263 (0.263) loss 0.3645 (0.3645) acc 90.6250 (90.6250) lr 6.6126e-04 eta 0:04:57
epoch [124/200] batch [2/3] time 1.026 (1.160) data 0.000 (0.132) loss 0.3252 (0.3448) acc 93.7500 (92.1875) lr 6.6126e-04 eta 0:04:25
epoch [124/200] batch [3/3] time 1.027 (1.116) data 0.000 (0.088) loss 0.0312 (0.2403) acc 100.0000 (94.7917) lr 6.4653e-04 eta 0:04:14
epoch [125/200] batch [1/3] time 1.313 (1.313) data 0.281 (0.281) loss 0.3613 (0.3613) acc 84.3750 (84.3750) lr 6.4653e-04 eta 0:04:58
epoch [125/200] batch [2/3] time 1.024 (1.168) data 0.000 (0.141) loss 0.2393 (0.3003) acc 100.0000 (92.1875) lr 6.4653e-04 eta 0:04:24
epoch [125/200] batch [3/3] time 1.029 (1.122) data 0.000 (0.094) loss 0.3713 (0.3240) acc 90.6250 (91.6667) lr 6.3188e-04 eta 0:04:12
epoch [126/200] batch [1/3] time 1.303 (1.303) data 0.271 (0.271) loss 0.3130 (0.3130) acc 90.6250 (90.6250) lr 6.3188e-04 eta 0:04:51
epoch [126/200] batch [2/3] time 1.022 (1.162) data 0.000 (0.135) loss 0.2147 (0.2639) acc 100.0000 (95.3125) lr 6.3188e-04 eta 0:04:19
epoch [126/200] batch [3/3] time 1.030 (1.118) data 0.000 (0.090) loss 0.4661 (0.3313) acc 87.5000 (92.7083) lr 6.1732e-04 eta 0:04:08
epoch [127/200] batch [1/3] time 1.303 (1.303) data 0.272 (0.272) loss 0.2476 (0.2476) acc 90.6250 (90.6250) lr 6.1732e-04 eta 0:04:47
epoch [127/200] batch [2/3] time 1.032 (1.167) data 0.000 (0.136) loss 0.2683 (0.2579) acc 90.6250 (90.6250) lr 6.1732e-04 eta 0:04:16
epoch [127/200] batch [3/3] time 1.030 (1.122) data 0.000 (0.091) loss 0.3767 (0.2975) acc 87.5000 (89.5833) lr 6.0285e-04 eta 0:04:05
epoch [128/200] batch [1/3] time 1.302 (1.302) data 0.274 (0.274) loss 0.1294 (0.1294) acc 96.8750 (96.8750) lr 6.0285e-04 eta 0:04:43
epoch [128/200] batch [2/3] time 1.028 (1.165) data 0.000 (0.137) loss 0.2844 (0.2069) acc 90.6250 (93.7500) lr 6.0285e-04 eta 0:04:12
epoch [128/200] batch [3/3] time 1.027 (1.119) data 0.000 (0.091) loss 0.1461 (0.1866) acc 96.8750 (94.7917) lr 5.8849e-04 eta 0:04:01
epoch [129/200] batch [1/3] time 1.285 (1.285) data 0.261 (0.261) loss 0.3691 (0.3691) acc 93.7500 (93.7500) lr 5.8849e-04 eta 0:04:36
epoch [129/200] batch [2/3] time 1.029 (1.157) data 0.000 (0.131) loss 0.2263 (0.2977) acc 96.8750 (95.3125) lr 5.8849e-04 eta 0:04:07
epoch [129/200] batch [3/3] time 1.031 (1.115) data 0.000 (0.087) loss 0.3589 (0.3181) acc 93.7500 (94.7917) lr 5.7422e-04 eta 0:03:57
epoch [130/200] batch [1/3] time 1.303 (1.303) data 0.273 (0.273) loss 0.2505 (0.2505) acc 93.7500 (93.7500) lr 5.7422e-04 eta 0:04:36
epoch [130/200] batch [2/3] time 1.026 (1.164) data 0.000 (0.137) loss 0.3286 (0.2896) acc 90.6250 (92.1875) lr 5.7422e-04 eta 0:04:05
epoch [130/200] batch [3/3] time 1.022 (1.117) data 0.000 (0.091) loss 0.3950 (0.3247) acc 87.5000 (90.6250) lr 5.6006e-04 eta 0:03:54
epoch [131/200] batch [1/3] time 1.298 (1.298) data 0.271 (0.271) loss 0.3218 (0.3218) acc 90.6250 (90.6250) lr 5.6006e-04 eta 0:04:31
epoch [131/200] batch [2/3] time 1.027 (1.163) data 0.000 (0.136) loss 0.2896 (0.3057) acc 93.7500 (92.1875) lr 5.6006e-04 eta 0:04:01
epoch [131/200] batch [3/3] time 1.022 (1.116) data 0.000 (0.090) loss 0.5205 (0.3773) acc 93.7500 (92.7083) lr 5.4601e-04 eta 0:03:50
epoch [132/200] batch [1/3] time 1.301 (1.301) data 0.277 (0.277) loss 0.3513 (0.3513) acc 93.7500 (93.7500) lr 5.4601e-04 eta 0:04:28
epoch [132/200] batch [2/3] time 1.033 (1.167) data 0.000 (0.139) loss 0.4863 (0.4188) acc 87.5000 (90.6250) lr 5.4601e-04 eta 0:03:59
epoch [132/200] batch [3/3] time 1.032 (1.122) data 0.000 (0.092) loss 0.2952 (0.3776) acc 93.7500 (91.6667) lr 5.3207e-04 eta 0:03:48
epoch [133/200] batch [1/3] time 1.285 (1.285) data 0.263 (0.263) loss 0.4436 (0.4436) acc 87.5000 (87.5000) lr 5.3207e-04 eta 0:04:20
epoch [133/200] batch [2/3] time 1.031 (1.158) data 0.000 (0.132) loss 0.1823 (0.3129) acc 93.7500 (90.6250) lr 5.3207e-04 eta 0:03:53
epoch [133/200] batch [3/3] time 1.027 (1.114) data 0.000 (0.088) loss 0.3865 (0.3374) acc 90.6250 (90.6250) lr 5.1825e-04 eta 0:03:44
epoch [134/200] batch [1/3] time 1.301 (1.301) data 0.278 (0.278) loss 0.3860 (0.3860) acc 90.6250 (90.6250) lr 5.1825e-04 eta 0:04:20
epoch [134/200] batch [2/3] time 1.028 (1.165) data 0.000 (0.139) loss 0.0956 (0.2408) acc 100.0000 (95.3125) lr 5.1825e-04 eta 0:03:51
epoch [134/200] batch [3/3] time 1.023 (1.117) data 0.000 (0.093) loss 0.4082 (0.2966) acc 84.3750 (91.6667) lr 5.0454e-04 eta 0:03:41
epoch [135/200] batch [1/3] time 1.286 (1.286) data 0.260 (0.260) loss 0.1499 (0.1499) acc 96.8750 (96.8750) lr 5.0454e-04 eta 0:04:13
epoch [135/200] batch [2/3] time 1.020 (1.153) data 0.000 (0.130) loss 0.3865 (0.2682) acc 96.8750 (96.8750) lr 5.0454e-04 eta 0:03:46
epoch [135/200] batch [3/3] time 1.026 (1.111) data 0.000 (0.087) loss 0.3904 (0.3089) acc 87.5000 (93.7500) lr 4.9096e-04 eta 0:03:36
epoch [136/200] batch [1/3] time 1.296 (1.296) data 0.266 (0.266) loss 0.1785 (0.1785) acc 96.8750 (96.8750) lr 4.9096e-04 eta 0:04:11
epoch [136/200] batch [2/3] time 1.026 (1.161) data 0.000 (0.133) loss 0.4900 (0.3342) acc 90.6250 (93.7500) lr 4.9096e-04 eta 0:03:44
epoch [136/200] batch [3/3] time 1.026 (1.116) data 0.000 (0.089) loss 0.4006 (0.3564) acc 90.6250 (92.7083) lr 4.7750e-04 eta 0:03:34
epoch [137/200] batch [1/3] time 1.300 (1.300) data 0.279 (0.279) loss 0.1920 (0.1920) acc 93.7500 (93.7500) lr 4.7750e-04 eta 0:04:08
epoch [137/200] batch [2/3] time 1.024 (1.162) data 0.000 (0.139) loss 0.2971 (0.2446) acc 96.8750 (95.3125) lr 4.7750e-04 eta 0:03:40
epoch [137/200] batch [3/3] time 1.028 (1.117) data 0.000 (0.093) loss 0.1348 (0.2080) acc 96.8750 (95.8333) lr 4.6417e-04 eta 0:03:31
epoch [138/200] batch [1/3] time 1.299 (1.299) data 0.271 (0.271) loss 0.2094 (0.2094) acc 93.7500 (93.7500) lr 4.6417e-04 eta 0:04:04
epoch [138/200] batch [2/3] time 1.025 (1.162) data 0.000 (0.136) loss 0.5586 (0.3840) acc 84.3750 (89.0625) lr 4.6417e-04 eta 0:03:37
epoch [138/200] batch [3/3] time 1.030 (1.118) data 0.000 (0.090) loss 0.3879 (0.3853) acc 90.6250 (89.5833) lr 4.5098e-04 eta 0:03:27
epoch [139/200] batch [1/3] time 1.306 (1.306) data 0.280 (0.280) loss 0.1508 (0.1508) acc 96.8750 (96.8750) lr 4.5098e-04 eta 0:04:01
epoch [139/200] batch [2/3] time 1.031 (1.169) data 0.000 (0.140) loss 0.3271 (0.2390) acc 93.7500 (95.3125) lr 4.5098e-04 eta 0:03:35
epoch [139/200] batch [3/3] time 1.028 (1.122) data 0.000 (0.094) loss 0.2969 (0.2583) acc 93.7500 (94.7917) lr 4.3792e-04 eta 0:03:25
epoch [140/200] batch [1/3] time 1.280 (1.280) data 0.260 (0.260) loss 0.1868 (0.1868) acc 96.8750 (96.8750) lr 4.3792e-04 eta 0:03:53
epoch [140/200] batch [2/3] time 1.027 (1.154) data 0.000 (0.130) loss 0.0793 (0.1330) acc 96.8750 (96.8750) lr 4.3792e-04 eta 0:03:28
epoch [140/200] batch [3/3] time 1.031 (1.113) data 0.000 (0.087) loss 0.2502 (0.1721) acc 90.6250 (94.7917) lr 4.2499e-04 eta 0:03:20
epoch [141/200] batch [1/3] time 1.305 (1.305) data 0.279 (0.279) loss 0.7036 (0.7036) acc 81.2500 (81.2500) lr 4.2499e-04 eta 0:03:53
epoch [141/200] batch [2/3] time 1.024 (1.164) data 0.000 (0.140) loss 0.2057 (0.4547) acc 93.7500 (87.5000) lr 4.2499e-04 eta 0:03:27
epoch [141/200] batch [3/3] time 1.026 (1.118) data 0.000 (0.093) loss 0.0956 (0.3350) acc 96.8750 (90.6250) lr 4.1221e-04 eta 0:03:17
epoch [142/200] batch [1/3] time 1.305 (1.305) data 0.281 (0.281) loss 0.0933 (0.0933) acc 100.0000 (100.0000) lr 4.1221e-04 eta 0:03:49
epoch [142/200] batch [2/3] time 1.020 (1.162) data 0.000 (0.141) loss 0.2045 (0.1489) acc 93.7500 (96.8750) lr 4.1221e-04 eta 0:03:23
epoch [142/200] batch [3/3] time 1.025 (1.116) data 0.000 (0.094) loss 0.3054 (0.2010) acc 93.7500 (95.8333) lr 3.9958e-04 eta 0:03:14
epoch [143/200] batch [1/3] time 1.297 (1.297) data 0.272 (0.272) loss 0.2299 (0.2299) acc 90.6250 (90.6250) lr 3.9958e-04 eta 0:03:44
epoch [143/200] batch [2/3] time 1.033 (1.165) data 0.000 (0.136) loss 0.2141 (0.2220) acc 96.8750 (93.7500) lr 3.9958e-04 eta 0:03:20
epoch [143/200] batch [3/3] time 1.028 (1.119) data 0.000 (0.091) loss 0.3796 (0.2745) acc 87.5000 (91.6667) lr 3.8709e-04 eta 0:03:11
epoch [144/200] batch [1/3] time 1.311 (1.311) data 0.280 (0.280) loss 0.1727 (0.1727) acc 93.7500 (93.7500) lr 3.8709e-04 eta 0:03:42
epoch [144/200] batch [2/3] time 1.029 (1.170) data 0.000 (0.140) loss 0.2081 (0.1904) acc 93.7500 (93.7500) lr 3.8709e-04 eta 0:03:17
epoch [144/200] batch [3/3] time 1.030 (1.123) data 0.000 (0.093) loss 0.2930 (0.2246) acc 90.6250 (92.7083) lr 3.7476e-04 eta 0:03:08
epoch [145/200] batch [1/3] time 1.292 (1.292) data 0.268 (0.268) loss 0.1781 (0.1781) acc 93.7500 (93.7500) lr 3.7476e-04 eta 0:03:35
epoch [145/200] batch [2/3] time 1.027 (1.159) data 0.000 (0.134) loss 0.4294 (0.3038) acc 87.5000 (90.6250) lr 3.7476e-04 eta 0:03:12
epoch [145/200] batch [3/3] time 1.027 (1.115) data 0.000 (0.089) loss 0.3784 (0.3287) acc 90.6250 (90.6250) lr 3.6258e-04 eta 0:03:03
epoch [146/200] batch [1/3] time 1.302 (1.302) data 0.273 (0.273) loss 0.2479 (0.2479) acc 90.6250 (90.6250) lr 3.6258e-04 eta 0:03:33
epoch [146/200] batch [2/3] time 1.031 (1.167) data 0.000 (0.136) loss 0.2847 (0.2663) acc 93.7500 (92.1875) lr 3.6258e-04 eta 0:03:10
epoch [146/200] batch [3/3] time 1.021 (1.118) data 0.000 (0.091) loss 0.5205 (0.3510) acc 87.5000 (90.6250) lr 3.5055e-04 eta 0:03:01
epoch [147/200] batch [1/3] time 1.289 (1.289) data 0.262 (0.262) loss 0.4204 (0.4204) acc 90.6250 (90.6250) lr 3.5055e-04 eta 0:03:27
epoch [147/200] batch [2/3] time 1.028 (1.159) data 0.000 (0.131) loss 0.0961 (0.2582) acc 96.8750 (93.7500) lr 3.5055e-04 eta 0:03:05
epoch [147/200] batch [3/3] time 1.025 (1.114) data 0.000 (0.088) loss 0.2610 (0.2592) acc 96.8750 (94.7917) lr 3.3869e-04 eta 0:02:57
epoch [148/200] batch [1/3] time 1.286 (1.286) data 0.262 (0.262) loss 0.2571 (0.2571) acc 96.8750 (96.8750) lr 3.3869e-04 eta 0:03:23
epoch [148/200] batch [2/3] time 1.026 (1.156) data 0.000 (0.131) loss 0.4351 (0.3461) acc 90.6250 (93.7500) lr 3.3869e-04 eta 0:03:01
epoch [148/200] batch [3/3] time 1.036 (1.116) data 0.000 (0.088) loss 0.3372 (0.3431) acc 90.6250 (92.7083) lr 3.2699e-04 eta 0:02:54
epoch [149/200] batch [1/3] time 1.307 (1.307) data 0.279 (0.279) loss 0.0709 (0.0709) acc 100.0000 (100.0000) lr 3.2699e-04 eta 0:03:22
epoch [149/200] batch [2/3] time 1.028 (1.168) data 0.000 (0.140) loss 0.5151 (0.2930) acc 81.2500 (90.6250) lr 3.2699e-04 eta 0:02:59
epoch [149/200] batch [3/3] time 1.026 (1.120) data 0.000 (0.093) loss 0.4526 (0.3462) acc 93.7500 (91.6667) lr 3.1545e-04 eta 0:02:51
epoch [150/200] batch [1/3] time 1.286 (1.286) data 0.266 (0.266) loss 0.3291 (0.3291) acc 93.7500 (93.7500) lr 3.1545e-04 eta 0:03:15
epoch [150/200] batch [2/3] time 1.026 (1.156) data 0.000 (0.133) loss 0.1699 (0.2495) acc 93.7500 (93.7500) lr 3.1545e-04 eta 0:02:54
epoch [150/200] batch [3/3] time 1.030 (1.114) data 0.000 (0.089) loss 0.1897 (0.2296) acc 96.8750 (94.7917) lr 3.0409e-04 eta 0:02:47
epoch [151/200] batch [1/3] time 1.300 (1.300) data 0.269 (0.269) loss 0.2515 (0.2515) acc 93.7500 (93.7500) lr 3.0409e-04 eta 0:03:13
epoch [151/200] batch [2/3] time 1.028 (1.164) data 0.000 (0.135) loss 0.0848 (0.1682) acc 100.0000 (96.8750) lr 3.0409e-04 eta 0:02:52
epoch [151/200] batch [3/3] time 1.022 (1.117) data 0.000 (0.090) loss 0.6465 (0.3276) acc 81.2500 (91.6667) lr 2.9289e-04 eta 0:02:44
epoch [152/200] batch [1/3] time 1.290 (1.290) data 0.262 (0.262) loss 0.5146 (0.5146) acc 87.5000 (87.5000) lr 2.9289e-04 eta 0:03:08
epoch [152/200] batch [2/3] time 1.025 (1.158) data 0.000 (0.131) loss 0.2439 (0.3793) acc 96.8750 (92.1875) lr 2.9289e-04 eta 0:02:47
epoch [152/200] batch [3/3] time 1.028 (1.114) data 0.000 (0.087) loss 0.3616 (0.3734) acc 90.6250 (91.6667) lr 2.8187e-04 eta 0:02:40
epoch [153/200] batch [1/3] time 1.295 (1.295) data 0.265 (0.265) loss 0.1617 (0.1617) acc 96.8750 (96.8750) lr 2.8187e-04 eta 0:03:05
epoch [153/200] batch [2/3] time 1.022 (1.158) data 0.000 (0.133) loss 0.3547 (0.2582) acc 93.7500 (95.3125) lr 2.8187e-04 eta 0:02:44
epoch [153/200] batch [3/3] time 1.031 (1.116) data 0.000 (0.088) loss 0.2839 (0.2668) acc 90.6250 (93.7500) lr 2.7103e-04 eta 0:02:37
epoch [154/200] batch [1/3] time 1.310 (1.310) data 0.279 (0.279) loss 0.2018 (0.2018) acc 96.8750 (96.8750) lr 2.7103e-04 eta 0:03:03
epoch [154/200] batch [2/3] time 1.029 (1.169) data 0.000 (0.140) loss 0.4680 (0.3349) acc 87.5000 (92.1875) lr 2.7103e-04 eta 0:02:42
epoch [154/200] batch [3/3] time 1.024 (1.121) data 0.000 (0.093) loss 0.1550 (0.2749) acc 96.8750 (93.7500) lr 2.6037e-04 eta 0:02:34
epoch [155/200] batch [1/3] time 1.287 (1.287) data 0.262 (0.262) loss 0.1409 (0.1409) acc 100.0000 (100.0000) lr 2.6037e-04 eta 0:02:56
epoch [155/200] batch [2/3] time 1.028 (1.158) data 0.000 (0.131) loss 0.2043 (0.1726) acc 93.7500 (96.8750) lr 2.6037e-04 eta 0:02:37
epoch [155/200] batch [3/3] time 1.028 (1.114) data 0.000 (0.087) loss 0.1472 (0.1641) acc 96.8750 (96.8750) lr 2.4989e-04 eta 0:02:30
epoch [156/200] batch [1/3] time 1.294 (1.294) data 0.270 (0.270) loss 0.1643 (0.1643) acc 96.8750 (96.8750) lr 2.4989e-04 eta 0:02:53
epoch [156/200] batch [2/3] time 1.024 (1.159) data 0.000 (0.135) loss 0.2683 (0.2163) acc 93.7500 (95.3125) lr 2.4989e-04 eta 0:02:34
epoch [156/200] batch [3/3] time 1.025 (1.114) data 0.000 (0.090) loss 0.5293 (0.3206) acc 84.3750 (91.6667) lr 2.3959e-04 eta 0:02:27
epoch [157/200] batch [1/3] time 1.303 (1.303) data 0.275 (0.275) loss 0.2230 (0.2230) acc 93.7500 (93.7500) lr 2.3959e-04 eta 0:02:50
epoch [157/200] batch [2/3] time 1.023 (1.163) data 0.000 (0.137) loss 0.4167 (0.3199) acc 90.6250 (92.1875) lr 2.3959e-04 eta 0:02:31
epoch [157/200] batch [3/3] time 1.028 (1.118) data 0.000 (0.092) loss 0.2179 (0.2859) acc 100.0000 (94.7917) lr 2.2949e-04 eta 0:02:24
epoch [158/200] batch [1/3] time 1.303 (1.303) data 0.273 (0.273) loss 0.2097 (0.2097) acc 93.7500 (93.7500) lr 2.2949e-04 eta 0:02:46
epoch [158/200] batch [2/3] time 1.026 (1.164) data 0.000 (0.137) loss 0.1143 (0.1620) acc 96.8750 (95.3125) lr 2.2949e-04 eta 0:02:27
epoch [158/200] batch [3/3] time 1.026 (1.118) data 0.000 (0.091) loss 0.1670 (0.1637) acc 96.8750 (95.8333) lr 2.1957e-04 eta 0:02:20
epoch [159/200] batch [1/3] time 1.307 (1.307) data 0.279 (0.279) loss 0.3523 (0.3523) acc 93.7500 (93.7500) lr 2.1957e-04 eta 0:02:43
epoch [159/200] batch [2/3] time 1.031 (1.169) data 0.000 (0.140) loss 0.3682 (0.3602) acc 90.6250 (92.1875) lr 2.1957e-04 eta 0:02:24
epoch [159/200] batch [3/3] time 1.030 (1.123) data 0.000 (0.093) loss 0.4917 (0.4041) acc 87.5000 (90.6250) lr 2.0984e-04 eta 0:02:18
epoch [160/200] batch [1/3] time 1.294 (1.294) data 0.270 (0.270) loss 0.4280 (0.4280) acc 90.6250 (90.6250) lr 2.0984e-04 eta 0:02:37
epoch [160/200] batch [2/3] time 1.028 (1.161) data 0.000 (0.135) loss 0.4128 (0.4204) acc 84.3750 (87.5000) lr 2.0984e-04 eta 0:02:20
epoch [160/200] batch [3/3] time 1.028 (1.117) data 0.000 (0.090) loss 0.2561 (0.3656) acc 93.7500 (89.5833) lr 2.0032e-04 eta 0:02:13
epoch [161/200] batch [1/3] time 1.301 (1.301) data 0.273 (0.273) loss 0.1471 (0.1471) acc 93.7500 (93.7500) lr 2.0032e-04 eta 0:02:34
epoch [161/200] batch [2/3] time 1.025 (1.163) data 0.000 (0.137) loss 0.3579 (0.2525) acc 90.6250 (92.1875) lr 2.0032e-04 eta 0:02:17
epoch [161/200] batch [3/3] time 1.029 (1.118) data 0.000 (0.091) loss 0.1700 (0.2250) acc 96.8750 (93.7500) lr 1.9098e-04 eta 0:02:10
epoch [162/200] batch [1/3] time 1.312 (1.312) data 0.282 (0.282) loss 0.2332 (0.2332) acc 96.8750 (96.8750) lr 1.9098e-04 eta 0:02:32
epoch [162/200] batch [2/3] time 1.029 (1.171) data 0.000 (0.141) loss 0.2710 (0.2521) acc 90.6250 (93.7500) lr 1.9098e-04 eta 0:02:14
epoch [162/200] batch [3/3] time 1.024 (1.122) data 0.000 (0.094) loss 0.0698 (0.1913) acc 100.0000 (95.8333) lr 1.8185e-04 eta 0:02:07
epoch [163/200] batch [1/3] time 1.292 (1.292) data 0.263 (0.263) loss 0.4841 (0.4841) acc 84.3750 (84.3750) lr 1.8185e-04 eta 0:02:25
epoch [163/200] batch [2/3] time 1.025 (1.158) data 0.000 (0.132) loss 0.2922 (0.3882) acc 90.6250 (87.5000) lr 1.8185e-04 eta 0:02:09
epoch [163/200] batch [3/3] time 1.024 (1.114) data 0.000 (0.088) loss 0.2335 (0.3366) acc 96.8750 (90.6250) lr 1.7292e-04 eta 0:02:03
epoch [164/200] batch [1/3] time 1.296 (1.296) data 0.265 (0.265) loss 0.2162 (0.2162) acc 96.8750 (96.8750) lr 1.7292e-04 eta 0:02:22
epoch [164/200] batch [2/3] time 1.027 (1.162) data 0.000 (0.132) loss 0.5015 (0.3588) acc 90.6250 (93.7500) lr 1.7292e-04 eta 0:02:06
epoch [164/200] batch [3/3] time 1.026 (1.117) data 0.000 (0.088) loss 0.3540 (0.3572) acc 93.7500 (93.7500) lr 1.6419e-04 eta 0:02:00
epoch [165/200] batch [1/3] time 1.283 (1.283) data 0.260 (0.260) loss 0.2092 (0.2092) acc 96.8750 (96.8750) lr 1.6419e-04 eta 0:02:17
epoch [165/200] batch [2/3] time 1.025 (1.154) data 0.000 (0.130) loss 0.4512 (0.3302) acc 90.6250 (93.7500) lr 1.6419e-04 eta 0:02:02
epoch [165/200] batch [3/3] time 1.028 (1.112) data 0.000 (0.087) loss 0.3374 (0.3326) acc 93.7500 (93.7500) lr 1.5567e-04 eta 0:01:56
epoch [166/200] batch [1/3] time 1.308 (1.308) data 0.273 (0.273) loss 0.2280 (0.2280) acc 96.8750 (96.8750) lr 1.5567e-04 eta 0:02:16
epoch [166/200] batch [2/3] time 1.029 (1.168) data 0.000 (0.136) loss 0.4253 (0.3267) acc 90.6250 (93.7500) lr 1.5567e-04 eta 0:02:00
epoch [166/200] batch [3/3] time 1.025 (1.120) data 0.000 (0.091) loss 0.2654 (0.3062) acc 90.6250 (92.7083) lr 1.4736e-04 eta 0:01:54
epoch [167/200] batch [1/3] time 1.292 (1.292) data 0.264 (0.264) loss 0.1244 (0.1244) acc 100.0000 (100.0000) lr 1.4736e-04 eta 0:02:10
epoch [167/200] batch [2/3] time 1.027 (1.159) data 0.000 (0.132) loss 0.3108 (0.2176) acc 93.7500 (96.8750) lr 1.4736e-04 eta 0:01:55
epoch [167/200] batch [3/3] time 1.023 (1.114) data 0.000 (0.088) loss 0.2532 (0.2295) acc 96.8750 (96.8750) lr 1.3926e-04 eta 0:01:50
epoch [168/200] batch [1/3] time 1.295 (1.295) data 0.264 (0.264) loss 0.3457 (0.3457) acc 90.6250 (90.6250) lr 1.3926e-04 eta 0:02:06
epoch [168/200] batch [2/3] time 1.027 (1.161) data 0.000 (0.132) loss 0.3948 (0.3702) acc 90.6250 (90.6250) lr 1.3926e-04 eta 0:01:52
epoch [168/200] batch [3/3] time 1.027 (1.116) data 0.000 (0.088) loss 0.2942 (0.3449) acc 90.6250 (90.6250) lr 1.3137e-04 eta 0:01:47
epoch [169/200] batch [1/3] time 1.298 (1.298) data 0.271 (0.271) loss 0.0610 (0.0610) acc 100.0000 (100.0000) lr 1.3137e-04 eta 0:02:03
epoch [169/200] batch [2/3] time 1.024 (1.161) data 0.000 (0.136) loss 0.6226 (0.3418) acc 84.3750 (92.1875) lr 1.3137e-04 eta 0:01:49
epoch [169/200] batch [3/3] time 1.027 (1.116) data 0.000 (0.091) loss 0.3660 (0.3498) acc 90.6250 (91.6667) lr 1.2369e-04 eta 0:01:43
epoch [170/200] batch [1/3] time 1.299 (1.299) data 0.271 (0.271) loss 0.2129 (0.2129) acc 96.8750 (96.8750) lr 1.2369e-04 eta 0:01:59
epoch [170/200] batch [2/3] time 1.024 (1.162) data 0.000 (0.135) loss 0.1359 (0.1744) acc 96.8750 (96.8750) lr 1.2369e-04 eta 0:01:45
epoch [170/200] batch [3/3] time 1.025 (1.116) data 0.000 (0.090) loss 0.0729 (0.1405) acc 100.0000 (97.9167) lr 1.1623e-04 eta 0:01:40
epoch [171/200] batch [1/3] time 1.294 (1.294) data 0.270 (0.270) loss 0.2330 (0.2330) acc 96.8750 (96.8750) lr 1.1623e-04 eta 0:01:55
epoch [171/200] batch [2/3] time 1.027 (1.160) data 0.000 (0.135) loss 0.2389 (0.2360) acc 93.7500 (95.3125) lr 1.1623e-04 eta 0:01:42
epoch [171/200] batch [3/3] time 1.028 (1.116) data 0.000 (0.090) loss 0.1907 (0.2209) acc 96.8750 (95.8333) lr 1.0899e-04 eta 0:01:37
epoch [172/200] batch [1/3] time 1.281 (1.281) data 0.265 (0.265) loss 0.1405 (0.1405) acc 96.8750 (96.8750) lr 1.0899e-04 eta 0:01:50
epoch [172/200] batch [2/3] time 1.028 (1.154) data 0.000 (0.133) loss 0.1831 (0.1618) acc 96.8750 (96.8750) lr 1.0899e-04 eta 0:01:38
epoch [172/200] batch [3/3] time 1.026 (1.111) data 0.000 (0.088) loss 0.1390 (0.1542) acc 96.8750 (96.8750) lr 1.0197e-04 eta 0:01:33
epoch [173/200] batch [1/3] time 1.301 (1.301) data 0.272 (0.272) loss 0.2881 (0.2881) acc 87.5000 (87.5000) lr 1.0197e-04 eta 0:01:48
epoch [173/200] batch [2/3] time 1.031 (1.166) data 0.000 (0.136) loss 0.2939 (0.2910) acc 90.6250 (89.0625) lr 1.0197e-04 eta 0:01:35
epoch [173/200] batch [3/3] time 1.028 (1.120) data 0.000 (0.091) loss 0.2727 (0.2849) acc 96.8750 (91.6667) lr 9.5173e-05 eta 0:01:30
epoch [174/200] batch [1/3] time 1.307 (1.307) data 0.279 (0.279) loss 0.2073 (0.2073) acc 93.7500 (93.7500) lr 9.5173e-05 eta 0:01:44
epoch [174/200] batch [2/3] time 1.035 (1.171) data 0.000 (0.139) loss 0.5786 (0.3929) acc 84.3750 (89.0625) lr 9.5173e-05 eta 0:01:32
epoch [174/200] batch [3/3] time 1.039 (1.127) data 0.000 (0.093) loss 0.1285 (0.3048) acc 100.0000 (92.7083) lr 8.8597e-05 eta 0:01:27
epoch [175/200] batch [1/3] time 1.338 (1.338) data 0.279 (0.279) loss 0.3301 (0.3301) acc 90.6250 (90.6250) lr 8.8597e-05 eta 0:01:43
epoch [175/200] batch [2/3] time 1.039 (1.189) data 0.000 (0.140) loss 0.2454 (0.2877) acc 93.7500 (92.1875) lr 8.8597e-05 eta 0:01:30
epoch [175/200] batch [3/3] time 1.036 (1.138) data 0.000 (0.093) loss 0.4304 (0.3353) acc 93.7500 (92.7083) lr 8.2245e-05 eta 0:01:25
epoch [176/200] batch [1/3] time 1.298 (1.298) data 0.264 (0.264) loss 0.2971 (0.2971) acc 93.7500 (93.7500) lr 8.2245e-05 eta 0:01:36
epoch [176/200] batch [2/3] time 1.029 (1.164) data 0.000 (0.132) loss 0.3064 (0.3018) acc 96.8750 (95.3125) lr 8.2245e-05 eta 0:01:24
epoch [176/200] batch [3/3] time 1.032 (1.120) data 0.000 (0.088) loss 0.3167 (0.3067) acc 90.6250 (93.7500) lr 7.6120e-05 eta 0:01:20
epoch [177/200] batch [1/3] time 1.296 (1.296) data 0.271 (0.271) loss 0.2170 (0.2170) acc 93.7500 (93.7500) lr 7.6120e-05 eta 0:01:31
epoch [177/200] batch [2/3] time 1.028 (1.162) data 0.000 (0.135) loss 0.0911 (0.1541) acc 100.0000 (96.8750) lr 7.6120e-05 eta 0:01:21
epoch [177/200] batch [3/3] time 1.030 (1.118) data 0.000 (0.090) loss 0.5229 (0.2770) acc 87.5000 (93.7500) lr 7.0224e-05 eta 0:01:17
epoch [178/200] batch [1/3] time 1.309 (1.309) data 0.278 (0.278) loss 0.3127 (0.3127) acc 90.6250 (90.6250) lr 7.0224e-05 eta 0:01:29
epoch [178/200] batch [2/3] time 1.032 (1.170) data 0.000 (0.139) loss 0.1720 (0.2424) acc 100.0000 (95.3125) lr 7.0224e-05 eta 0:01:18
epoch [178/200] batch [3/3] time 1.036 (1.126) data 0.000 (0.093) loss 0.4172 (0.3007) acc 90.6250 (93.7500) lr 6.4556e-05 eta 0:01:14
epoch [179/200] batch [1/3] time 1.309 (1.309) data 0.278 (0.278) loss 0.2267 (0.2267) acc 93.7500 (93.7500) lr 6.4556e-05 eta 0:01:25
epoch [179/200] batch [2/3] time 1.022 (1.165) data 0.000 (0.139) loss 0.3008 (0.2637) acc 90.6250 (92.1875) lr 6.4556e-05 eta 0:01:14
epoch [179/200] batch [3/3] time 1.025 (1.119) data 0.000 (0.093) loss 0.2712 (0.2662) acc 93.7500 (92.7083) lr 5.9119e-05 eta 0:01:10
epoch [180/200] batch [1/3] time 1.302 (1.302) data 0.272 (0.272) loss 0.1576 (0.1576) acc 96.8750 (96.8750) lr 5.9119e-05 eta 0:01:20
epoch [180/200] batch [2/3] time 1.026 (1.164) data 0.000 (0.136) loss 0.2141 (0.1859) acc 96.8750 (96.8750) lr 5.9119e-05 eta 0:01:11
epoch [180/200] batch [3/3] time 1.030 (1.119) data 0.000 (0.091) loss 0.3992 (0.2570) acc 84.3750 (92.7083) lr 5.3915e-05 eta 0:01:07
epoch [181/200] batch [1/3] time 1.309 (1.309) data 0.279 (0.279) loss 0.1014 (0.1014) acc 100.0000 (100.0000) lr 5.3915e-05 eta 0:01:17
epoch [181/200] batch [2/3] time 1.023 (1.166) data 0.000 (0.140) loss 0.5171 (0.3093) acc 84.3750 (92.1875) lr 5.3915e-05 eta 0:01:07
epoch [181/200] batch [3/3] time 1.025 (1.119) data 0.000 (0.093) loss 0.3210 (0.3132) acc 93.7500 (92.7083) lr 4.8943e-05 eta 0:01:03
epoch [182/200] batch [1/3] time 1.290 (1.290) data 0.262 (0.262) loss 0.1586 (0.1586) acc 96.8750 (96.8750) lr 4.8943e-05 eta 0:01:12
epoch [182/200] batch [2/3] time 1.030 (1.160) data 0.000 (0.131) loss 0.3335 (0.2460) acc 93.7500 (95.3125) lr 4.8943e-05 eta 0:01:03
epoch [182/200] batch [3/3] time 1.025 (1.115) data 0.000 (0.087) loss 0.4395 (0.3105) acc 90.6250 (93.7500) lr 4.4207e-05 eta 0:01:00
epoch [183/200] batch [1/3] time 1.303 (1.303) data 0.278 (0.278) loss 0.2620 (0.2620) acc 93.7500 (93.7500) lr 4.4207e-05 eta 0:01:09
epoch [183/200] batch [2/3] time 1.030 (1.167) data 0.000 (0.139) loss 0.4355 (0.3488) acc 93.7500 (93.7500) lr 4.4207e-05 eta 0:01:00
epoch [183/200] batch [3/3] time 1.033 (1.122) data 0.000 (0.093) loss 0.0964 (0.2646) acc 100.0000 (95.8333) lr 3.9706e-05 eta 0:00:57
epoch [184/200] batch [1/3] time 1.296 (1.296) data 0.270 (0.270) loss 0.4348 (0.4348) acc 87.5000 (87.5000) lr 3.9706e-05 eta 0:01:04
epoch [184/200] batch [2/3] time 1.024 (1.160) data 0.000 (0.135) loss 0.1063 (0.2706) acc 96.8750 (92.1875) lr 3.9706e-05 eta 0:00:56
epoch [184/200] batch [3/3] time 1.023 (1.114) data 0.000 (0.090) loss 0.2262 (0.2558) acc 93.7500 (92.7083) lr 3.5443e-05 eta 0:00:53
epoch [185/200] batch [1/3] time 1.287 (1.287) data 0.262 (0.262) loss 0.2194 (0.2194) acc 93.7500 (93.7500) lr 3.5443e-05 eta 0:01:00
epoch [185/200] batch [2/3] time 1.032 (1.160) data 0.000 (0.131) loss 0.1962 (0.2078) acc 93.7500 (93.7500) lr 3.5443e-05 eta 0:00:53
epoch [185/200] batch [3/3] time 1.035 (1.118) data 0.000 (0.087) loss 0.2539 (0.2231) acc 93.7500 (93.7500) lr 3.1417e-05 eta 0:00:50
epoch [186/200] batch [1/3] time 1.290 (1.290) data 0.263 (0.263) loss 0.1979 (0.1979) acc 96.8750 (96.8750) lr 3.1417e-05 eta 0:00:56
epoch [186/200] batch [2/3] time 1.024 (1.157) data 0.000 (0.132) loss 0.4531 (0.3255) acc 90.6250 (93.7500) lr 3.1417e-05 eta 0:00:49
epoch [186/200] batch [3/3] time 1.037 (1.117) data 0.000 (0.088) loss 0.3057 (0.3189) acc 93.7500 (93.7500) lr 2.7630e-05 eta 0:00:46
epoch [187/200] batch [1/3] time 1.300 (1.300) data 0.271 (0.271) loss 0.2688 (0.2688) acc 87.5000 (87.5000) lr 2.7630e-05 eta 0:00:53
epoch [187/200] batch [2/3] time 1.026 (1.163) data 0.000 (0.135) loss 0.1727 (0.2208) acc 96.8750 (92.1875) lr 2.7630e-05 eta 0:00:46
epoch [187/200] batch [3/3] time 1.029 (1.118) data 0.000 (0.090) loss 0.0938 (0.1784) acc 100.0000 (94.7917) lr 2.4083e-05 eta 0:00:43
epoch [188/200] batch [1/3] time 1.297 (1.297) data 0.273 (0.273) loss 0.0971 (0.0971) acc 100.0000 (100.0000) lr 2.4083e-05 eta 0:00:49
epoch [188/200] batch [2/3] time 1.028 (1.163) data 0.000 (0.137) loss 0.1569 (0.1270) acc 96.8750 (98.4375) lr 2.4083e-05 eta 0:00:43
epoch [188/200] batch [3/3] time 1.027 (1.117) data 0.000 (0.091) loss 0.3210 (0.1917) acc 87.5000 (94.7917) lr 2.0777e-05 eta 0:00:40
epoch [189/200] batch [1/3] time 1.303 (1.303) data 0.279 (0.279) loss 0.3921 (0.3921) acc 96.8750 (96.8750) lr 2.0777e-05 eta 0:00:45
epoch [189/200] batch [2/3] time 1.029 (1.166) data 0.000 (0.139) loss 0.1686 (0.2803) acc 96.8750 (96.8750) lr 2.0777e-05 eta 0:00:39
epoch [189/200] batch [3/3] time 1.030 (1.121) data 0.000 (0.093) loss 0.4597 (0.3401) acc 90.6250 (94.7917) lr 1.7713e-05 eta 0:00:36
epoch [190/200] batch [1/3] time 1.294 (1.294) data 0.264 (0.264) loss 0.3818 (0.3818) acc 87.5000 (87.5000) lr 1.7713e-05 eta 0:00:41
epoch [190/200] batch [2/3] time 1.026 (1.160) data 0.000 (0.132) loss 0.2080 (0.2949) acc 93.7500 (90.6250) lr 1.7713e-05 eta 0:00:35
epoch [190/200] batch [3/3] time 1.024 (1.115) data 0.000 (0.088) loss 0.3130 (0.3009) acc 96.8750 (92.7083) lr 1.4891e-05 eta 0:00:33
epoch [191/200] batch [1/3] time 1.301 (1.301) data 0.279 (0.279) loss 0.2184 (0.2184) acc 90.6250 (90.6250) lr 1.4891e-05 eta 0:00:37
epoch [191/200] batch [2/3] time 1.029 (1.165) data 0.000 (0.139) loss 0.3159 (0.2672) acc 93.7500 (92.1875) lr 1.4891e-05 eta 0:00:32
epoch [191/200] batch [3/3] time 1.031 (1.120) data 0.000 (0.093) loss 0.3130 (0.2824) acc 90.6250 (91.6667) lr 1.2312e-05 eta 0:00:30
epoch [192/200] batch [1/3] time 1.299 (1.299) data 0.269 (0.269) loss 0.1924 (0.1924) acc 96.8750 (96.8750) lr 1.2312e-05 eta 0:00:33
epoch [192/200] batch [2/3] time 1.024 (1.161) data 0.000 (0.135) loss 0.4644 (0.3284) acc 87.5000 (92.1875) lr 1.2312e-05 eta 0:00:29
epoch [192/200] batch [3/3] time 1.022 (1.115) data 0.000 (0.090) loss 0.4480 (0.3682) acc 90.6250 (91.6667) lr 9.9763e-06 eta 0:00:26
epoch [193/200] batch [1/3] time 1.308 (1.308) data 0.279 (0.279) loss 0.1566 (0.1566) acc 96.8750 (96.8750) lr 9.9763e-06 eta 0:00:30
epoch [193/200] batch [2/3] time 1.025 (1.167) data 0.000 (0.140) loss 0.1317 (0.1442) acc 96.8750 (96.8750) lr 9.9763e-06 eta 0:00:25
epoch [193/200] batch [3/3] time 1.029 (1.121) data 0.000 (0.093) loss 0.4944 (0.2609) acc 90.6250 (94.7917) lr 7.8853e-06 eta 0:00:23
epoch [194/200] batch [1/3] time 1.290 (1.290) data 0.263 (0.263) loss 0.3738 (0.3738) acc 87.5000 (87.5000) lr 7.8853e-06 eta 0:00:25
epoch [194/200] batch [2/3] time 1.028 (1.159) data 0.000 (0.132) loss 0.4329 (0.4033) acc 90.6250 (89.0625) lr 7.8853e-06 eta 0:00:22
epoch [194/200] batch [3/3] time 1.025 (1.115) data 0.000 (0.088) loss 0.2241 (0.3436) acc 93.7500 (90.6250) lr 6.0390e-06 eta 0:00:20
epoch [195/200] batch [1/3] time 1.295 (1.295) data 0.270 (0.270) loss 0.5205 (0.5205) acc 90.6250 (90.6250) lr 6.0390e-06 eta 0:00:22
epoch [195/200] batch [2/3] time 1.024 (1.160) data 0.000 (0.135) loss 0.1187 (0.3196) acc 100.0000 (95.3125) lr 6.0390e-06 eta 0:00:18
epoch [195/200] batch [3/3] time 1.028 (1.116) data 0.000 (0.090) loss 0.4460 (0.3617) acc 84.3750 (91.6667) lr 4.4380e-06 eta 0:00:16
epoch [196/200] batch [1/3] time 1.290 (1.290) data 0.263 (0.263) loss 0.4368 (0.4368) acc 87.5000 (87.5000) lr 4.4380e-06 eta 0:00:18
epoch [196/200] batch [2/3] time 1.026 (1.158) data 0.000 (0.131) loss 0.2419 (0.3394) acc 90.6250 (89.0625) lr 4.4380e-06 eta 0:00:15
epoch [196/200] batch [3/3] time 1.029 (1.115) data 0.000 (0.088) loss 0.3774 (0.3521) acc 93.7500 (90.6250) lr 3.0827e-06 eta 0:00:13
epoch [197/200] batch [1/3] time 1.310 (1.310) data 0.279 (0.279) loss 0.2915 (0.2915) acc 93.7500 (93.7500) lr 3.0827e-06 eta 0:00:14
epoch [197/200] batch [2/3] time 1.026 (1.168) data 0.000 (0.140) loss 0.5127 (0.4021) acc 87.5000 (90.6250) lr 3.0827e-06 eta 0:00:11
epoch [197/200] batch [3/3] time 1.027 (1.121) data 0.000 (0.093) loss 0.6650 (0.4897) acc 81.2500 (87.5000) lr 1.9733e-06 eta 0:00:10
epoch [198/200] batch [1/3] time 1.297 (1.297) data 0.270 (0.270) loss 0.1040 (0.1040) acc 96.8750 (96.8750) lr 1.9733e-06 eta 0:00:10
epoch [198/200] batch [2/3] time 1.026 (1.161) data 0.000 (0.135) loss 0.4229 (0.2634) acc 87.5000 (92.1875) lr 1.9733e-06 eta 0:00:08
epoch [198/200] batch [3/3] time 1.028 (1.117) data 0.000 (0.090) loss 0.3801 (0.3023) acc 93.7500 (92.7083) lr 1.1101e-06 eta 0:00:06
epoch [199/200] batch [1/3] time 1.299 (1.299) data 0.273 (0.273) loss 0.2659 (0.2659) acc 96.8750 (96.8750) lr 1.1101e-06 eta 0:00:06
epoch [199/200] batch [2/3] time 1.027 (1.163) data 0.000 (0.136) loss 0.1163 (0.1911) acc 96.8750 (96.8750) lr 1.1101e-06 eta 0:00:04
epoch [199/200] batch [3/3] time 1.029 (1.118) data 0.000 (0.091) loss 0.1335 (0.1719) acc 100.0000 (97.9167) lr 4.9344e-07 eta 0:00:03
epoch [200/200] batch [1/3] time 1.296 (1.296) data 0.273 (0.273) loss 0.1796 (0.1796) acc 96.8750 (96.8750) lr 4.9344e-07 eta 0:00:02
epoch [200/200] batch [2/3] time 1.020 (1.158) data 0.000 (0.137) loss 0.2296 (0.2046) acc 87.5000 (92.1875) lr 4.9344e-07 eta 0:00:01
epoch [200/200] batch [3/3] time 1.031 (1.116) data 0.000 (0.091) loss 0.4468 (0.2853) acc 87.5000 (90.6250) lr 1.2337e-07 eta 0:00:00
Checkpoint saved to output/Caltech/1/2/3/prompt_learner/model.pth.tar-200
Finish training
Deploy the last-epoch model
Evaluate on the *test* set
=> result
* total: 2,465
* correct: 2,178
* accuracy: 88.4%
* error: 11.6%
* macro_f1: 83.3%
Elapsed: 0:11:49
